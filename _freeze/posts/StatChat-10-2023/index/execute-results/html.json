{
  "hash": "ae6ed1b14f83be34651e813f86dd8ede",
  "result": {
    "markdown": "---\ntitle: \"A Model Statistics Course\"\nauthor: \"Daniel Kaplan\"\ndate: \"2024-01-31\"\ncategories: [presentation, overview]\nformat:\n  html:\n    toc: true\n---\n\n\n> *These are presentation notes for the October 2023 StatChat meeting. For more than 15 years, statistical educators in the Twin Cities region of Minnesota have been gathering a half-dozen times a year at StatChat to share comradeship and teaching insights. Among the schools regularly represented are the University of Minnesota, Macalester College, St. Olaf College, Hamline University, Augsburg University, Carleton College, St. Cloud State University, and Minnesota State University Mankato. The slides were slightly revised for a 2024-01-31 presentation \"StatsChat\" for high-school teachers in Wisconsin.*\n\n**Abstract**: \"Mere Renovation is Too Little Too Late: We Need to Rethink Our Undergraduate Curriculum from the Ground Up\" is the title [2015 paper](https://nhorton.people.amherst.edu/mererenovation/) by George Cobb. Honoring George's challenge, I have been rethinking and re-designing the introductory statistics course, replacing traditional foundations using modern materials and reconfiguring the living and working spaces to suit today's applied statistical needs and projects. In the spirit of a \"model house\" used to demonstrate housing innovations, I'll take you on a tour of my \"model course,\" whose materials are available free, open, and online. Among the features you'll see: an accessible handling of causal reasoning, a unification of the course structure around modeling, a highly streamlined yet professional-quality computational platform, and an honest presentation of Hypothesis Testing that puts it in the framework of Bayesian reasoning.\n\n\n\n\n\n## Introductions\n\n1. What is the most important take-away from your stats course?\n\n2. What subjects could be dropped without loss?\n\n3. Are there course topics that are misleading?\n\n4. Are there course topics that are out of date?\n\n\n## Motivation\n\nThe \"consensus\" Stat 101 is 50 years out of date:\n\na. fails to engage issues of causation, covariation, and adjustment\nb. too much emphasis on p-values \nc. entirely ignores Bayes\nd. no substantial coverage of risk, risk factors, ... \ne. uses a confusing over-variety of graphic modes (many of which are out-of-date)\nf. doesn't make contact with data science, machine learning, and AI/GPT\n\nI'm happy to discuss the above points anytime, but that's where I aimed this talk.\n\n\nMy objectives: \n\na. Demonstrate the extent to which it's possible to overcome these deficiencies with a complete, practicable, no-prerequisite course.\n\nb. Provide a complete course framework, avoiding topic bloat, to which other people can add their own exercises, topics, and examples. \n\nTo this end, there is now a completed draft textbook: [*Lessons in Statistical Thinking*](https://dtkaplan.github.io/Lessons-in-statistical-thinking) that is free, online. \n\n\n::: {.callout-note}\n## Jeff Witmer's approach\n\nJeff proposes 15 changes*, dividing them into amount-of-effort categories:\n\na. Changes You Could Make with Little Effort or Planning. (e.g. \"significant\" -> discernible)\nb. Changes to a Course That You Could Implement after Investing a Day or so of Planning. (e.g. emphasize power, not $\\alpha$)\nc. Changes to a Course That Would Require Quite a Bit of Planning but That Are Worth Considering Nonetheless (e.g. emphasize effect size)\n\nJeff Witmer (2023) \"What Should We Do Differently in STAT 101?\" *Journal of Statistics and Data Science Education* [link](https://www.tandfonline.com/doi/full/10.1080/26939169.2023.2205905)\n\n*Almost all of which are engaged in* [Lessons in Statistical Thinking](https://dtkaplan.github.io/Lessons-in-statistical-thinking)\n:::\n\n## Style\n\na. Demonstrate and describe statistical phenomena by causal simulation\n    - Examples:\n        - sampling variation\n        - confounders, covariates, colliders, adjustment\n    - Stat theory from simulation/wrangling rather than probability/algebra\n#. Informal inference from the very beginning, *gradually* formalizing it over the semester\n#. Single, standard format for graphics: the *annotated point plot*.\n    - Annotations for (i) distribution, and (ii) models, including multivariable models.\n#. Keep the software powerful, but simple. \n    i. Example: a point plot of Francis Galton's data on children's heights versus their parents.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGalton |> point_plot(height ~ mother + sex)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/index-i9Xy1G-1.png){width=672}\n:::\n:::\n\nii. Example: The most frequently encountered command has this structure:\n    \n\n::: {.cell}\n\n```{.r .cell-code}\n    Galton |> \n      model_train(height ~ mother + sex) |>\n      conf_interval()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) 37.1   41.4   45.8  \n2 mother       0.286  0.353  0.421\n3 sexM         4.87   5.18   5.49 \n```\n:::\n:::\n\n    \n## Course overview\n\n- Part 1: Handling data\n    - Data frames\n    - Graphics (data and models)\n    - Wrangling\n\n- Part 2: Describing relationships\n    - Regression (incl. categorical and multiple explanatory variables)\n    - Adjustment\n\n- Part 3: Randomness and the unexplained\n    - Signal and noise\n    - Simulation and DAGs\n    - Probability models (optional)\n    - Sampling variation and confidence intervals/bands \n    - Likelihood (optional, prep. for Part 5)\n    - Measuring and accumulating risk\n\n- Part 4: Causal reasoning\n    - Effect size\n    - Directed Acyclic Graphs (DAGs), or \"influence diagrams\"\n    - Causality/Confounding/Adjustment\n    - Experiment\n\n- Part 5: Hypothetical thinking\n    - Basic Bayes: competing two hypotheses\n    - Hypothesis testing \n\n## How can we fit more in an already crowded course?\n\nStreamline!\n\n1. Reduce ~~drag~~ cognitive load. \n    a. Repeated use a small number of standard forms\n        - one basic graphical pattern: annotated point plot\n        - one basic computational pattern: noun |> verb `|>` verb `|>` ...\n    b. Avoid nomenclature conflicts with everyday words, e.g.\n        - \"table\" -> data frame\n        - \"case\" -> specimen\n        - \"assignment\" -> storage\n2. Unify t, p into regression modeling, conf. interval/bands in both graphics and models\n3. Keep number of types of objects small: data frame, model, graphic, simulation\n    a. Very small computational footprint, a dozen stat/graphics/wrangling functions.\n4. Remove square roots whenever that's easy\n    - focus on variance rather than standard deviation, R^2 rather than r\n\n--------\n\n## Part I: Handling Data  (6-7 class hrs)\n\n\n### Lesson 1. Data frames\n\nData is *always* in data frames.\n\n- Columns: Variables\n\n- Rows: \"Specimens\" / Unit of observation\n\nComputing concepts:\n\n- name of data frame, e.g. `Galton` or `Nats`\n- pipe\n- function()\n\nUsually start with a named data frame, piping it to a function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNats |> names()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"country\" \"year\"    \"GDP\"     \"pop\"    \n```\n:::\n:::\n\n\n### Lesson 2. Graphics\n\nBoth the horizontal and vertical axes are mapped to variables.\n\nJust one command: `point_plot()` produces point plot with automatic jittering as needed.\n\nTilde expression specifies which variable is mapped to y and x (and, optionally, color and faceting).\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nGalton |> point_plot(height ~ sex)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/index-fyLGcH-1.png){width=672}\n:::\n\n```{.r .cell-code}\nGalton |> point_plot(height ~ mother + father + sex)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/index-fyLGcH-2.png){width=672}\n:::\n:::\n\n\n### Lesson 3. Empirical distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGalton |> point_plot(height ~ sex, annot = \"violin\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/index-TRHxeO-1.png){width=672}\n:::\n:::\n\n\n### Lesson 4. Models as graphical annotation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGalton |> sample_n(size=100) |> \n  point_plot(height ~ sex, annot = \"model\", \n             point_ink = 0.1, model_ink=0.75)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/index-a3uFv5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nGalton |> point_plot(height ~ mother + father + sex, \n                     annot = \"model\", \n                     point_ink = 0.1, model_ink=0.75)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/index-a3uFv5-2.png){width=672}\n:::\n:::\n\n\n## Lesson 5: Wrangling\n\n[Perhaps use two class days]\n\nFive basic operations: `mutate()`, `filter()`, `summarize()`, `select()`, `arrange()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNats\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 4\n  country  year   GDP   pop\n  <chr>   <dbl> <dbl> <dbl>\n1 Korea    2020   874    32\n2 Cuba     2020    80     7\n3 France   2020  1203    55\n4 India    2020  1100  1300\n5 Korea    1950   100    32\n6 Cuba     1950    60     8\n7 France   1950   250    40\n8 India    1950   300   700\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nNats |> filter(year == 2020)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  country  year   GDP   pop\n  <chr>   <dbl> <dbl> <dbl>\n1 Korea    2020   874    32\n2 Cuba     2020    80     7\n3 France   2020  1203    55\n4 India    2020  1100  1300\n```\n:::\n\n```{.r .cell-code}\nNats |> summarize(totalpop = sum(pop), .by=year)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n   year totalpop\n  <dbl>    <dbl>\n1  2020     1394\n2  1950      780\n```\n:::\n:::\n\n\n## Lesson 6: Computing recap\n\n[Perhaps merged into a two-day wrangling unit with Lesson 5]\n\nPipes, functions, parentheses, arguments, ...\n\n## Lesson 7: Databases\n\n[Entirely optional]\n\n- Joins \n- Why we put related data into separate tables with different units of observation.\n\n\n## Part II: Describing Relationships\n\nConsistently use explanatory/response modeling paradigm. Introduce models with two or three explanatory variables early in the course. \n\nUse variance as measure of **varia**tion of a **varia**ble. (Ask me about the simple explanation of variance that doesn't involve calculating a mean.)\n\nUse data wrangling to introduce model values, residuals, ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars |> \n  mutate(mpg_mod = model_values(mpg ~ hp + wt)) |> \n  select(hp, wt, mpg_mod) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   hp    wt  mpg_mod\nMazda RX4         110 2.620 23.57233\nMazda RX4 Wag     110 2.875 22.58348\nDatsun 710         93 2.320 25.27582\nHornet 4 Drive    110 3.215 21.26502\nHornet Sportabout 175 3.440 18.32727\nValiant           105 3.460 20.47382\n```\n:::\n:::\n\n\nThen transition to model coefficients. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars |>\n  model_train(mpg ~ hp + wt) |>\n  conf_interval()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) 34.0    37.2    40.5   \n2 hp          -0.0502 -0.0318 -0.0133\n3 wt          -5.17   -3.88   -2.58  \n```\n:::\n:::\n\n\nCoefficients are always shown in the context of a confidence interval, even if they don't yet know the mechanism for generating such intervals. \n\n\nDemonstrate mechanism of \"adjustment\": Evaluate model holding covariates constant. \n\n## Part III: Randomness and noise\n\n6-11 class hours, depending on how much spent with named probability distributions. (USAFA engineers want some practice with named distributions: normal, exponential, poisson, ...)\n\n### Signal and noise\n\n### Simulations\n\nStudents construct simple simulations, using them to generate data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmysim <- datasim_make(\n  x <- rnorm(n),\n  y <- 2 + 3*x + rnorm(n, sd=0.5)\n)\nmysim |> sample(size=4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 2\n       x       y\n   <dbl>   <dbl>\n1  0.552  3.97  \n2 -0.675 -0.0812\n3  0.214  3.10  \n4  0.311  2.82  \n5  1.17   5.79  \n```\n:::\n:::\n\n\n### Probability models\n\nMostly using simulations.\n\n### Likelihood\n\nEarly introduction of the concept of **likelihood**: probability of data given hypothesis/model.\n\n- main point: distinguish between p(model | data) and p(data | model)\n- we'll use likelihood in last part of course.\n\n### R^2^\n\n### Prediction\n\na. The proper form for a prediction: a relative probability assigned to each possible outcome.\n\nb. The prediction-interval shorthand for (a).\n\n### Sampling variation and confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRuns <- mysim |> \n  sample(n = 5) |>\n  model_train(y ~ x) |> \n  trials(10) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `tidy()` method for objects of class `model_object` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n```\n:::\n\n```{.r .cell-code}\nRuns |> select(.trial, term, estimate )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   .trial        term estimate\n1       1 (Intercept)    1.659\n2       1           x    3.164\n3       2 (Intercept)    2.204\n4       2           x    2.950\n5       3 (Intercept)    2.055\n6       3           x    3.318\n7       4 (Intercept)    1.771\n8       4           x    3.141\n9       5 (Intercept)    2.156\n10      5           x    2.920\n11      6 (Intercept)    2.157\n12      6           x    3.324\n13      7 (Intercept)    2.091\n14      7           x    2.833\n15      8 (Intercept)    2.446\n16      8           x    2.794\n17      9 (Intercept)    1.892\n18      9           x    2.399\n19     10 (Intercept)    2.053\n20     10           x    2.694\n```\n:::\n\n```{.r .cell-code}\nRuns |> summarize(var(estimate), .by = term)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         term var(estimate)\n1 (Intercept)       0.05117\n2           x       0.08504\n```\n:::\n:::\n\n\nDemonstrate that variance scales as 1/n.  \n\n### Risk\n\n(2 or 3 day unit)\n\nDefinition of risk, risk factors, baseline risk, risk ratios, absolute change in risk. \n\nUse absolute change for decision making, but use risk ratios and odds ratios for calculations.\n\nRegression when response is a zero-one variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWhickham |> point_plot(outcome ~ smoker, annot=\"model\", \n                       model_ink = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/index-oS96Bi-1.png){width=672}\n:::\n\n```{.r .cell-code}\nWhickham |> point_plot(outcome ~ age + smoker, annot=\"model\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/index-oS96Bi-2.png){width=672}\n:::\n:::\n\n\n\n## Part IV: Causal modeling\n\n### Effect size\n\nRatio of (change in output) to (change in input).\n\nPhysical units important.\n\n### DAGs\n\nReading \"influence diagrams\"\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Causality\n\nconfounding, covariates, and adjustment\n\nChoosing covariates based on a DAG\n\n### Experiment\n\nexperiment interpreted as re-wiring of DAGs: requires *intervention*\n\n## Part V: Hypothetical thinking\n\n### Strong emphasis on the idea of *hypothesis*. \n\n- Your turn: Please define hypothesis.\n\nWhat we want: p(hypothesis | data)\n\nWhat we have: hypothesis |> simulation |> data summarized as a Likelihood.\n\nQuestion: How do we calculate what we want.\n\n### Bayes framework\n\n- Setting: medical screening. Test result + or -. \n\n- We put two hypotheses into competition based on the test result.\n\n- **Likelihoods** we can measure from data:\n\n    - p(+ | Sick) aka \"sensitivity\"\n    - p(+ | Healthy) aka \"false-positive rate\" translated to \"specificity\"\n    \n- Setting: prevalence(Sick)\n\n- Calculation\n\n    - Graphically\n    - Algebra from the graph\n    - Formula in Likelihood-ratio form \n    \n$$odds(Sick|+) = \\frac{p(+ | Sick)}{p(+ | Healthy)} \\ odds(prevalence)$$\n    \n### Null hypothesis testing\n\nNull is \"Healthy.\" \n\nWe have no claim about $prevalence$.\n\nIf we have no claim about $p(+ | Sick)$, we are more inclined to conclude $Sick$ if $p(+ | Healthy)$ is small.  \n\n- $p(+ | Healthy)$ is p-value.\n\n### Neyman-Pearson\n\nNull is \"Healthy.\" Alternative is \"Sick\".\n\nWe have no claim about $prevalence$, but we have the ability to estimate $p(+ | Sick)$. \n\nInclined to conclude $Sick$ if $p(+ | Healthy)$ is small (like HNT) *and* $p(+ | Sick)$ is small. $p(+ | Sick)$ is the **power** of the test.\n\n### Gotcha's in HT\n\n- Without power, can't say what constitutes a big p-value.\n- Estimation of p suffers greatly from sampling variation. No good reason to think that p < 0.01 is any different from p < 0.05. \n- Sensible decisions require knowledge of prevalence.\n- Effect size, not p-value, is needed to interpret practical importance of results.\n\n\n\n\n\n\n\n\n\n\n\n## Objects and operations\n\na. Data frame\n#. Data graphics (as distinct from \"infographics\")\n#. Statistical model\n#. Simulation\n\nOperations for all students\n\n1. Data wrangling (simplified)\n2. Annotated point plot of variables from a data frame\n3. Model training\n4. Model summarization\n\nOperations used in demonstrations (and suited to some students)\n\n5. Simulation (in demonstrations)\n6. Iteration and accumulation (in demonstrations)\n\nComputations on variables are always **inside** the arguments of a function taking a data frame as an input.\n\nTilde expressions for models and graphics.\n\n## Resources\n\n1. Textbook: [**Statistical Inference via Data Science**](https://moderndive.com) by Chester Ismay and Albert Y. Kim\n2. Textbook: [**Lessons in Statistical Thinking**](https://dtkaplan.github.io/Lessons-in-statistical-thinking) by Danny Kaplan\n    - [Associated R package: `{LST}` ](https://dtkaplan.github.io/LST/)\n3. [USAFA Math 300Z](https://dtkaplan.github.io/Math-300Z/) course outline, instructor notes, etc.\n4. Jeff Witmer (2023) \"What Should We Do Differently in STAT 101?\" *Journal of Statistics and Data Science Education* [link](https://www.tandfonline.com/doi/full/10.1080/26939169.2023.2205905)\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}