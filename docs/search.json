[
  {
    "objectID": "posts/Takeaways-22/index.html",
    "href": "posts/Takeaways-22/index.html",
    "title": "Lesson 22 take-aways",
    "section": "",
    "text": "A DAG (directed acyclic graph) is a way of representing a hypothesis or belief or possibility to be considered of the causal connections among variables.\n\n\n\n\n\n\n\nFigure 1: A drawing of dag06, to use as an example.\n\n\n\n- DAGs consist of **nodes**---`a`, `b`, `c`, and `d` above, each representing a variable.\n- Any pair of nodes can be connected (or not!) by a **directed edge**. The edge means that the two variables are causally connected. The one-way flow of influence from one variable to the other is indicated by the direction.\n- The **A** in D**A**G stands for \"acyclic,\" meaning that there are no loops (cycles) in the flow.\n\nWe will use DAGs for three purposes:\n\nTo encapsulate hypotheses about causal connections in a variety of real-world settings.\nAs a means to reason about which covariates should and should not be included in a model. (Lesson 30)\nTo generate data from a source whose mechanism is exactly known. This will allow us to learn how and how much we can learn from data. We can then carry these lessons to real situations where the mechanism is only hypothetically known.\n\nThe software for our use of DAGs is built in to the {math300} package. This includes:\n\nAbout a dozen different DAGs providing examples of different sorts of causal connection. This includes those named dag01 through dag12, which are meant to be schematic (abstract). Other built-in DAGs encode hypotheses about causal connections in real-world systems.\nsample(dag01, size=10) generates data.\ndag_draw(dag01) draws a picture of the graph.\nprint(dag01) shows the formulas used in the generation of simulated data.\nIn Lesson 32 (about experimentation) you will see how to change a DAG using dag_intervene().\ndag_make(..formulas..) creates a DAG, but you won’t have much occasion to use it."
  },
  {
    "objectID": "posts/Takeaways-22/index.html#lesson-22",
    "href": "posts/Takeaways-22/index.html#lesson-22",
    "title": "Lesson 22 take-aways",
    "section": "Lesson 22",
    "text": "Lesson 22\n\nOften, the data we have at hand is a sample of specimens from a much larger population of objects (called a “population”). We store the sample’s observations in a data frame.\nEven when the data are a census, we often analyze them as if they are a sample from a population.\nA statistical point of view is that the data in our data frame are merely one sample collected at random from the population. Consequently, we imagine for the purpose of constructing many statistical methods, that there are infinitely many other samples that are equivalent to the one we have but which just happened not to be selected.\nSample statistics are numbers that we calculate from our sample. For us in Math 300Z, such statistics will typically be coefficients from a model (but we will also use some other sample statistics).\nOwing to the randomness involved in collecting our sample, we regard any sample statistic as a random draw from a population of sample statistics that could have been computed on other samples (as in (3)). That is, every sample statistic is a combination of “signal” and “noise.”\nSince our sample statistic includes noise, it is appropriate to quantify how much noise there is. Knowing this can, for example, enable us to decide whether two different samples come from different populations or not."
  },
  {
    "objectID": "posts/Takeaways-25/index.html",
    "href": "posts/Takeaways-25/index.html",
    "title": "Lesson 25 take-aways",
    "section": "",
    "text": "We contrasted the very different tasks of ….\n\nEstimation, which is mainly what we have been talking about until today. Estimation has to do with coefficients and effect sizes, understanding the relationships among values. Confidence intervals are a very important part of estimation methodology. Estimation focuses on “average” or “typical” or “central” patterns.\nPrediction which focuses on outcomes for individuals, and doesn’t benefit much from averaging. Graphically, a reasonable job drawing a prediction “interval” can be done from a plot of the data: look at the range of outcomes for the points near the given input levels.\n\nThe proper form for a prediction is to list all the possible outcomes, then assign a probability to each possible outcome.\n\nWhen the outcomes are numeric over a continuous range, then “probability” should be interpreted as “probability density” (a technical term from calculus) or, in more everyday language, a “relative probability.”\nA violin plot gives a reasonable representation of the relative probability of the different outcomes. But this is suitable only when the explanatory variables are categorical. For continuous, numerical explanatory variables, we will need another technique.\n\nEstimation of an effect size or a coefficient is accompanied by a confidence interval, which has a lower and an upper bound (the “confidence bounds”). The specific interval depends on the “confidence level,” but you won’t be mislead if you always use a 95% level, which is the convention.\n\nHistorical aside: The term “confidence” rather than “probability” was used to step around philosophical debates about the nature of probability. The confidence interval is not intended to be translated into a probability. Such a translation would look like this, “There is a 95% probability that the true value falls into the range covered by the interval.” This is what almost everybody does, even though it is not exactly legitimate. The “mathematically correct” formulation for translating a confidence interval into a probability is more subtle and not satisfying. (It is, “If I build confidence intervals according to the rules, then I can expect that in 95% of the situations being studied the true value will be within the confidence interval. But I can’t know for any one situation whether this is the case.”)\n\nPredictions are often formatted into an interval, so it’s tempting to think that the same principles (e.g., use a 95% level) are applicable. But useful predictions often have to do with extreme events. So levels like 80% are often appropriate. Strictly speaking, what’s presented as an interval ought really to be presented as a probability distribution (like a violin plot). Experts learn how to reverse engineer the probability distribution from the interval.\nNotwithstanding (4), the proper form for a prediction is to assign a probability to each possible outcome, as in (2). It is this form that is useful for decision making.\nWe looked at a case study about SAT scores"
  },
  {
    "objectID": "posts/Takeaways-24/index.html",
    "href": "posts/Takeaways-24/index.html",
    "title": "Lesson 24 take-aways",
    "section": "",
    "text": "The lm() model-training function is an entirely automatic machine for turning two inputs into model coefficients (which are stored in a “model object”).\n\nInput 1: A model specification in the form of a tilde-expression. Example: height ~ mother + sex\nInput 2: A data frame holding the variables used in the tilde espression. Example: Galton\n\n\n\nthe_model <- lm(height ~ mother + sex, data = Galton)\ncoefficients(the_model)\n\n(Intercept)      mother        sexM \n 41.4495235   0.3531371   5.1766949 \n\n\n\nModel coefficients are a convenient and historically important way to present a model. For fundamental, however, is the idea of a model function that takes as inputs the explanatory variables and returns a corresponding output to be interpreted as a value of the response variable.\nThere are other ways to represent model functions. The field of “machine learning” is largely about the variety of ways of representing model functions. At an elementary level, when there are few explanatory variables, a graph will do:\n\n\nmodel_plot(the_model, interval=\"confidence\")\n\n\n\n\nIn the above graph, a confidence interval has been added to indicate the precision that can justifiably be claimed for the model function. Any line that fits within the shaded region is a reasonable claim.\n\nOften, the interest is in measuring the size of the connection between an explanatory variable and the response variable. This is called the effect size. For instance, in the above graph there is obviously a connection between mother’s height and her children’s height. The size of the connection is the amount by which the child’s height would change if the mother’s height were magically altered. Here, the effect size is about 0.35. A two-inch gain in mother’s height would lead to a 0.70 inch gain in child’s height.\nEffect size is always “with respect to” a single, selected explanatory variable. Each explanatory variable has its own effect size. An effect size always means to change the selected variable while holding every other component of the system contant. In mathematical language, an effect size is a partial derivative (if the selected explanatory variable is quantitative) or a partial change (if the selected explanatory variable is categorical).\nWhen we talk about effect size, we are not necessarily implying any causal connection in the real world. Obviously, it’s absurd to think that changing a mother’s height (and nothing else!) would lead to any change in her children’s heights. The effect size describes how the model function output will change when the input is changed. The model function may or may not be faithful to the causal mechanisms in the world. For our mother/child height example, the model height ~ mother + sex does not capture the real-world genetics/environment determinants of child’s height.\nAlmost all the models we will construct in Math 300Z have only “linear” terms, so in every case the effect size with respect to a variable will be exactly the same as the coefficient on that variable. This is just to keep the accounting simple for us. (In class we showed a couple of models that have nonlinear terms, the most common of which are called “interactions,” but which also include curvy (rather than straight-line) functions. You won’t be responsible for this material.)"
  },
  {
    "objectID": "posts/Takeaways-23/index.html",
    "href": "posts/Takeaways-23/index.html",
    "title": "Lesson 23 take-aways",
    "section": "",
    "text": "lm() creates a model, which we can summarize in several ways. These numerical summaries—for instance, the coefficients reported by lm()—are called sample statistics. Mathematically, the sample statistics are exact, that is, the arithmetic is done correctly and everyone will get the same sample statistics when building the same model on the same data.\nStatistically, we take another point of view. We see the sample that we are working with as just one of the many samples that might have been collected. Imagine calculating a sample statistic on each of the many samples. The sample statistic would vary from one hypothetical sample to another. We call this sampling variation: note the “ing” ending on “sampling.”\nA confidence interval indicates the amount of sampling variation. It always consists of two numbers, the lower and the upper limits of the interval. Compute them from a model using conf_interval().\nThe width of a confidence interval is proportional to \\(1/\\sqrt{n}\\); the more data you have, the narrower will be the confidence interval.\nPrecision and accuracy are two different concepts. Accuracy refers to whether the measurement is “on target” or “close to reality.” Confidence intervals have nothing at all to say about accuracy. To get an accurate measurement of a coefficient, we need to choose the model that represents reality. Usually, we have no way to do this for sure. (With DAG simulations, we can read reality from the formulas, letting us match the model to the formula.)\nPrecision refers to the reliability or repeatability of the measurement. Confidence intervals are a good way to describe the precision of your measurements."
  },
  {
    "objectID": "posts/ChatGPT-and-R/index.html",
    "href": "posts/ChatGPT-and-R/index.html",
    "title": "Can ChatGPT help in interpreting or correcting R commands?",
    "section": "",
    "text": "The results are pretty good, to judge from a couple of initial trials. But, clearly, you cannot be completely oblivious about how R works to make sense of the results.\nChatGPT: This R code runs a linear regression model on the mtcars dataset, which is a built-in dataset in R containing information about 32 different cars. The model aims to predict the miles per gallon (mpg) of a car based on its horsepower (hp) and number of cylinders (cyl).\nHere’s a breakdown of what each part of the code means:\nTogether, the code lm(mpg ~ hp + cyl, data=mtcars) fits a linear regression model with mpg as the response variable and hp and cyl as the predictors, using the mtcars dataset. The output of the lm() function will include information about the coefficients of the model, as well as measures of model fit and statistical significance.\nChatGPT: This R code does not work because there is a syntax error in the formula provided to the lm() function. Specifically, there is an extra comma (,) after the cy variable, which causes R to interpret the formula incorrectly.*\nThe correct syntax for the lm() function requires that the formula is separated from the data argument using the data= parameter. Here’s the corrected code:\nThis code specifies the formula mpg ~ hp + cyl as before, and the data= parameter specifies that the data for the analysis should be taken from the mtcars dataset."
  },
  {
    "objectID": "posts/ChatGPT-and-R/index.html#comments",
    "href": "posts/ChatGPT-and-R/index.html#comments",
    "title": "Can ChatGPT help in interpreting or correcting R commands?",
    "section": "Comments",
    "text": "Comments\n\nChatGPT has a preferred set of words for describing computation. All the ones I see here are legitimate, but they don’t reflect the usage in the Math 300 textbook. Examples:\n\n“parameter”—We tend to use “argument” or “input,” as in “The data= input to lm().”\n“formula”—We use the phrase “tilde expression.” That’s because our course is mathematical in nature. In mathematics, “formula” refers to an arithmetical combination of quantities, as in \\(mx + b\\). If you want to construct a linear regression model along the lines of \\(mx + b\\), your tilde expression will look like y ~ 1 + x, assuming that y is the name of your response variable and x is the name of the explanatory variable.\n\nIn the 2nd example, ChatGPT didn’t mention that cy is not a variable in the mtcars data frame. It did, however, correct it to cyl in the example code."
  },
  {
    "objectID": "posts/Prediction-vs-estimation/index.html",
    "href": "posts/Prediction-vs-estimation/index.html",
    "title": "Case study: Prediction vs estimation",
    "section": "",
    "text": "This case study is motivated by the common misconception that “estimation” is more appropriate than “prediction.” The setting is gender preconceptions, specifically the claim that “boys are better at math than girls.” The data we looked at are math SAT scores for males and for females.\n\n\n\nAlthough published studies use actual individual-level data about SAT scores—for instance, a data frame with 1.5 million rows and the variables sex and score—those data are not available to us. Instead, I’ve reconstructed 20,000 rows of such raw data from the summary information provided by the College Board. The data frame is called Math_scores. (It is not available from R packages.)\nHere is an estimation: What is the difference in mean SAT scores for females and males?\n\nMath_scores |>\n  lm(score ~ sex, data=_) |>\n  conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    493.26869 \n    495.44183 \n    497.61497 \n  \n  \n    sexmale \n    19.33271 \n    22.40599 \n    25.47927 \n  \n\n\n\n\nThis model summary points to males having a higher average math SAT score than females, by about 20 points. The confidence interval has a width of about 6 points. In the actual data, which had 1.5 million rather than 20 thousand rows, the confidence interval would be much narrower, about 1 point.\nThere are two major problems with interpreting such results as meaning “boys are better at math than girls.”\n\nThe calculations for the confidence interval on the sexmale effect size are factually correct, but they are incomplete and misleading. The model represents an oversimplified DAG: \\(\\mathtt{sex} \\rightarrow \\mathtt{score}\\). In reality there are many other factors at work. One that we talked about in class is that a higher proportion of women take the SAT than of men. We know from other sources (and common sense) that the smaller the fraction of a group who take the SAT, the higher the average scores will be. We can estimate the effect size of SAT with respect to “fraction taking the test,” and it accounts for the observed difference in mean scores between males and females. To summarize, female test scores are on average lower than males because a more inclusive group of women take the test than of men. This is ironic: because women are more likely to go to college than men (a sign of academic success for women), the women’s scores are on average worse.\n\nMore germane to today’s topic of prediction …\n\nA statement like “boys are better at math than girls” tends to get translated to valuations of the skills of individuals. But if it is individuals we are thinking of, we should be framing the data analysis in terms of prediction rather than estimation. In this case, since sex is a categorical explanatory variable, the predictive probability distribution is well indicated by a violin plot, as in Figure 1.\n\n\ngf_jitter(score ~ sex, data = Math_scores, size=0.2, width = 0.15, height = 0, alpha = 0.3) %>% \n  gf_violin(color = NA, fill = \"blue\", alpha = 0.5) %>%\n  gf_labs(y = \"SAT math score\", x = \"\")\n\n\n\n\nFigure 1: The violin plot shows the probability distribution associated with each sex. They are almost identical.\n\n\n\n\nA reasonable interpretation of this graph: Knowing the sex of a person tells nothing about the person’s SAT score.\nAnother way to look at the problem is whether knowing the test score gives us insight into the sex of the person. We will study the modeling technique used to answer such a question in Lesson 33. But for now, all you need to know is that it’s possible to model the probability that a person is female given the test score.\n\nMath_scores <- Math_scores |> mutate(female = zero_one(sex, one=\"female\"))\nmodel <- glm(female ~ splines::ns(score,4), data = Math_scores, family=\"binomial\")\nmodel_plot(model, interval=\"none\", show_data=FALSE)\n\n\n\n\nExcept for scores above 650, the difference between the model values and 50% probability would be statistically undetectable unless averaging over groups of 400 or larger. At the very highest scores (around 750-800), there are about 2 males for every female. Even that hardly justifies applying the claim that “boys have high math scores than girls” to individuals."
  },
  {
    "objectID": "posts/Stat-and-non-stat-questions/index.html",
    "href": "posts/Stat-and-non-stat-questions/index.html",
    "title": "Statistical or not?",
    "section": "",
    "text": "Some questions and answers are statistical in nature, and others not. How can you know which is which?\nI’m thinking of starting my statistics class today by asking my students their ages. Next, a mathematical question: Is everyone in the class of the same age? That’s an easy question to answer, of course. The question is mathematical in the sense that it involves mathematical stuff—numbers—and there is a single, correct answer.\nI expect that not everyone will be the same age. That is, age varies among the members of the class. Now the question is: Why does age vary? When a statistical answer will be in the form, “Age varies because this other quantity or characteristic varies among the group.” This style of answer attempts to account for the variation in age by the variation in other things. For instance, the students don’t all have the same major, and different majors may place the stat course at different points in the students’ studies. Or, students entered the academy at different points in time. Presumably there would be less variation in age if we adjusted for each student’s age at matriculation.\nAnother question. Why are the statistics students typically about 20 years old? The answer that occurs to me is that almost all of the students are from the US and in the US students enter first grade around age 6 and continue through 12 years of primary and secondary education. Then they go to college and take stats in their second or third years. Few students have a gap between high-school graduation and college matriculation.\nThis answer is not statistical in nature. A statistical answers explains variation in one characteristic by appealing to the simultaneous variation in other characteristics. May statisticians may quarrel with my using words like “why” and “because.” But that is not central to the matter. What’s central to a question and answer being statistical in nature is that the question is about variation in a characteristic and the answer pairs that variation with variation in other characteristics.\n\nExample:\n\nNot a statistical question or answer: “Why is the sky blue?” Because blue light from the sun is scattered by small particles in the atmosphere.\nA statistical question and answer: “Why does the color of the sky change from time to time?” Because the weather is sometimes cloudy or foggy, and because the sun is sometimes high in the sky, sometimes rising or setting, and sometimes not visible at all."
  },
  {
    "objectID": "posts/Takeaways-26/index.html",
    "href": "posts/Takeaways-26/index.html",
    "title": "Lesson 26 take-aways",
    "section": "",
    "text": "In Lesson 25 we pointed out that the proper form for a prediction is a list of the potential outcomes, each matched to a probability of that outcome occurring. The probabilities across all outcomes (the “probability distribution”) should add to 1. (If the outcomes are quantitative and continuous, a probability density is used and “adding” is replaced with integration. But this is not important to us here.)\nWe are not going to be spending much time on the topic of probability distributions. This is a technical matter and we don’t have enough time to do it justice on a technical level. (We’ll come back to it, however, in Lessons 34 and 35, in a simple setting.)\nThere are some things you ought to learn, even if you don’t develop a mastery of probability.\n\nThere is a small set of mathematically defined probability distributions that are often used as models to organize prediction.\nThere is a probability logic called “Bayesian updating” that provides the means to update a probability distribution as new data come in. The relevant terms here are:\n\nprior distribution: our predictions before we see the new data.\nlikelihood: a mathematical model of the plausibility of each possible value for the new data observation.\nposterior distribution: the updated prediction upon seeing the new data.\nThe relationship among these three terms is mathematically simple: posterior is proportional to likelihood times prior. A demonstration, using self-driving cars as an example, is in this blog post.\n\n\nJust for general background, we looked at two of the distributions in the small set (3.i) of probability distributions:\n\nThe normal or bell-shaped distribution (available in R as dnorm()). This is the distribution that underlies the shorthand of using prediction intervals rather than probability distributions. Experts know exactly how to translate the prediction interval into the corresponding bell-shaped distribution.\nThe binomial distribution (available in R as dbinom()). The example we used for this had to do with hospital supplies. Suppose there are drugs or other medical material that is needed only rarely, for instance, having a 1% chance of use in any given hospital or clinic in the time before the material expires. If all hospitals and clinic keep it in stock, just in case, then roughly 99% will be wasted: an expensive proposition. However, if we can provide a central warehouse which can quickly send the material where and when it’s needed, we can dramatically reduce the amount needed to be in stock. For instance, if there are 500 hospitals and clinics, each of which has a 1% probability of needing a given drug, then we can virtually guarantee the availability of the drug with a small amount in stock. The binomial distribution provides the prediction of the amount of drug that will be called for across all hospitals and clinics. For the 500-hospital, 1% use case, the warehouse need only keep 15 doses in stock."
  },
  {
    "objectID": "posts/Takeaways-19/index.html",
    "href": "posts/Takeaways-19/index.html",
    "title": "Lesson 19 take-aways",
    "section": "",
    "text": "Measure variation using the “variance.” In R,\n\n\nGalton %>% summarize(vh = var(height))\n\n\nResponse variable will always be quantitative/numerical in a regression model.\nExpect graphics in our section to always have the response variable on the vertical axis."
  },
  {
    "objectID": "posts/Takeaways-19/index.html#administrative",
    "href": "posts/Takeaways-19/index.html#administrative",
    "title": "Lesson 19 take-aways",
    "section": "Administrative",
    "text": "Administrative\n\nDO THE DAILY WORKSHEET! You’ll fall behind if you don’t.\nBookmark this class site in your browser. That way you don’t have to detour through Teams.\nUse the Z-section project on posit.cloud. Bring in the worksheet using\n\n\nmath300::get_lesson_worksheet( LESSON NUMBER )"
  },
  {
    "objectID": "posts/Takeaways-21/index.html",
    "href": "posts/Takeaways-21/index.html",
    "title": "Lesson 21 take-aways",
    "section": "",
    "text": "Figure 1: A drawing of dag06, to use as an example.\n\n\n\n\nDAGs (“directed acyclic graphs”) have three properties, all of which are essential for representing causality.\n\nThey are “graphs” in the technical vocabulary of mathematics. That is, they consist of nodes and edges. Each edge connects two (and only two) of the nodes. In Figure 1 there are four nodes, labeled a, b, c, and d. Coincidentally, there are also four edges, one of which is c\\(\\longrightarrow\\)d.\nEvery edge is directed, that is, it points from a source node to a target node. Think of the directed edge as if it were a pipe carrying causal “fluid” from the source to the target. All the pipes in a graph are one-way only. In Figure 1 each of the edges is an arrow. The edge a\\(\\longrightarrow\\)b means that causal “fluid” can flow from a to b, but not from b to a.\nThere are no loops in the fluid flow (that is, there are no “cycles” of flow). In the name DAG, this is the meaning of the A: “acyclic,” meaning “no cycles.”\n\n\nExercise: Figure 1 is a DAG. By reversing the direction of a particular edge, you can modify the graph so that it has a cycle of flow. Which edge will do the job.\nExercise: Figure 1 does not have an edge connecting c and a. Add in that edge, directing it so that the new graph is still acyclic.\n\nStarting in Lesson 30, we will get into the ways to use DAGs in order to select explanatory variables that produce a model that is a faithful representation of the causal flows.\nFor the next few lessons, we will use DAGs for another purpose: to simulate data and make it easy to conduct random trials.\nOther than for teaching purposes (as in (3)), the role of DAGs in statistics and data analysis is to encode hypotheses about how elements of a system might be connected. Usually, you work with DAGs that reflect your and your colleague’s beliefs about how things are connected in the real world. Of course, believing a hypothesis does not make it true. Think of a DAG as a piece of fiction. Sometimes fiction is close to real life, and sometimes not. Both situations have their purposes for story-telling."
  },
  {
    "objectID": "posts/Takeaways-21/index.html#lesson-21",
    "href": "posts/Takeaways-21/index.html#lesson-21",
    "title": "Lesson 21 take-aways",
    "section": "Lesson 21",
    "text": "Lesson 21\n\nIt is helpful to think of any response variable as a combination of “signal” and “noise.” The signal reflects how the explanatory variables are related to the response. The noise is the unexplained part of the response variable. More precisely, the “noise” is that part that we do not care to explain in terms of relationships to other variables.\nThe idea that there is always noise in the response variable allows us to train models that do not go through every (or even any) data point. This enables us to claim that simple shapes of models can be good representations of relationships among variables.\nIn Math 300Z, with few exceptions we will work with models that have one or two explanatory variables. A nicer feature of such models is that we can draw a graphic of the model using just two or three aesthetics.\n\naes(y= ), the vertical axis: always will be assigned to the response variable. (This is a Math 300Z convention, and a good one, but not universal.)\naes(x= ), the horizontal axis: the first explanatory variable will be assigned to this.\naes(color= ) if there is a second explanatory variable, it will be assigned to color.\n\nThe model_plot() function will take care of all this assignment of variables to aesthetics.\nSince explanatory variables can be either categorical or quantitative, there are only a handful of model shapes we need to deal with. (The response variable is always quantitative.) These are enumerated in the Instructor’s notes for Lesson 21.\nAn excellent type of exam question would show you the graph of a model and ask you to identify whether there is one or two explanatory variables, and what type(s) it (they) are: quantitative or categorical.\nThere is also a role in statistics for models that have zero explanatory variables. The tilde expression for such models (letting y be the response variable) is y ~ 1. We have not yet discussed what the use is of such models.\nThe data used to build a regression model is called the training data. It is a data frame containing both the response variable and any explanatory variables. Once a model is built, we often run the rows of the training data through the model function. Doing this divides the response variable values into two components:\n\nThe signal, which is the model function output for each row of the training data. We call this the “model value.”\nThe noise, which for each row of the training data is the difference between the value of the response variable (the “response value”) and the “model value.” This difference—one for each row of the training data—is called the “residual”.\n\nRemember this simple relationship:\n\nresponse value = model value + residual\n\nOften we will need to measure how big these three things are. We use the variance as the measure of “how big.” The variance is only one of many possible ways to quantify “how big.” But it has the great advantage that\n\nvar(response value) = var(model value) + var(residual)"
  },
  {
    "objectID": "posts/Takeaways-28/index.html",
    "href": "posts/Takeaways-28/index.html",
    "title": "Lesson 28 take-aways",
    "section": "",
    "text": "Recall that the fundamental method in the second half of Math 300Z is linear regression. That involves identifying a response variable and one or more explanatory variables.1\n\nWe are adding some more detail to the vocabulary of explanatory variables in order to be able to talk about a very common situation:\n\nThere is one explanatory variable that is of primary interest to us.\nThere are also other explanatory variables that we might choose to include in the model specification, even though they aren’t of direct interest to us.\n\nThe generic name covariates is used for the explanatory variables in (b). A covariate is an ordinary variable, but it is always one that is being considered for inclusion in a model specification. We have a choice in this matter since the covariate is not of direct interest. Using this vocabulary gives us a way to say concisely, “The variable may be important in an explanatory role, but it is not of direct interest to us.”\nAn important critical thinking skill for evaluating a claim is to ask, “What other factors might be involved?” This is exactly the role for covariates. The statistics of covariates lets us examine the consequences of incorporating such “other factors.”\nCovariates often show up implicitly in models. For instance, the model life_expectancy ~ GDP has little explanatory power. Instead, we likely want to consider “per capita GDP, which takes into account the size of the population. In our modeling framework, this is quite like adding a covariatepopulation, that islife_expectancy ~ GDP + population`.\n\n\n\n\n\n\n\nAside: Intensive, extensive, and logarithms\n\n\n\nIn physical chemistry it’s common to distinguish between intensive and extensive variables. An extensive variable refers to the size (or “extent”) of the system: e.g. mass, volume, or energy. An intensive variable is not a measure of the “extent” of the system, but of a property like temperature or density. GDP is an extensive variable, as is population or land area. GDP is one way of describing how “big” the country is. Life expectancy does not tell you about the extent of the country, it’s an intensive quantity that refers to individuals.\nAs a modeling rule of thumb, whenever you are working with an extensive quantity, think seriously about using the logarithm of that quantity.\nDoing this in the context of life-expectancy and GDP (with population size as a covariate) would involve the model specification life_expectancy ~ log(GDP) + log(population). This model incorporates the “per capita” adjustment, since $(GDP/population) = (GDP) - ln(population). In other words,life_expectancy ~ log(GDP) + log(population)` is a generalization of the per-capita adjustment.\n\n\n\nIn Lesson 30, we will consider reasons why or why not to include covariates in a model. But in this Lesson 28, we want to make a technical point about comparing the coefficients between two nested models. An example of a pair of nested models is BFat ~ Hips and BFat ~ Hips + DThigh (to use the Anthro_F example). The models are nested because they both have the same response variable and the larger model includes all the explanatory variables in the smaller models.\nComparing the Hips coefficient for the two models shows something that surprises many people. Adding the covariate PThigh leads to a change in the coefficient on the explanatory variable Hips.\n\n\nlm(BFat ~ Hips, data=Anthro_F) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -53.00 \n    -44.00 \n    -35.00 \n  \n  \n    Hips \n    0.58 \n    0.68 \n    0.78 \n  \n\n\n\nlm(BFat ~ Hips + PThigh, data=Anthro_F) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -50.000 \n    -40.00 \n    -31.00 \n  \n  \n    Hips \n    0.083 \n    0.31 \n    0.54 \n  \n  \n    PThigh \n    0.240 \n    0.56 \n    0.87 \n  \n\n\n\n\nNotice the the confidence interval on the `Hips` coefficient from the smaller model doesn't overlap at all with the confidence interval on `Hips` from the larger model.\n\nMany people fallaciously believe that a situation as in (5) indicates some kind of deficiency in statistical methods. Such a person might ask, “How can I take a coefficient or confidence interval seriously if adding in another factor changes things completely?” In fact, the situation in (5) is entirely a mathematical phenomenon that depends on how closely related the explanatory variable (Hips) is to the covariate (PThigh).\n\nWe can measure how closely two variables are related using R2\n\nlm(Hips ~ PThigh, data=Anthro_F) |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    180 \n    1 \n    0.83 \n    880 \n    0.83 \n    0 \n    1 \n    180 \n  \n\n\n\n\nWhen the covariate is not closely related to the explanatory variable, the dramatic shift in the confidence interval of the explanatory variable is not seen. To illustrate, suppose we use Calf as the covariate rather than PThigh.\n\nlm(Hips ~ Calf, data=Anthro_F) |> R2() # not so closely related\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    180 \n    1 \n    0.2 \n    47 \n    0.2 \n    0 \n    1 \n    180 \n  \n\n\n\n\nThe high R2 indicates that the two variables are closely related. But with Calf as the covariate, the confidence interval on Hips is almost identical to what’s seen in the smaller model (see above) BFat ~ Hips.\n\nlm(BFat ~ Hips + Calf, data=Anthro_F) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -55.000 \n    -45.000 \n    -35.00 \n  \n  \n    Hips \n    0.540 \n    0.650 \n    0.76 \n  \n  \n    Calf \n    -0.091 \n    0.095 \n    0.28 \n  \n\n\n\n\nThe decision to include a covariate should rest on one’s understanding of the system being models. We’ve been using DAGs to describe such understanding. Lesson 30 will show how a DAG can be transformed into a decision about whether to include covariates.\n\n\n\n\nFootnotes\n\n\nTo be pedantic, I should say “zero or more explanatory variables.” That’s because y ~ 1 is also a regression model, even though it has no explanatory variables.↩︎"
  },
  {
    "objectID": "posts/Takeaways-20/index.html",
    "href": "posts/Takeaways-20/index.html",
    "title": "Lesson 20 take-aways",
    "section": "",
    "text": "Measure variation using the “variance.” In R,\n\n\n\nGalton %>% summarize(vh = var(height))\n\n\n\n \n  \n    vh \n  \n \n\n  \n    12.8 \n  \n\n\n\n\n\nInterpretation: The heights of the people in the Galton data frame vary. The amount of this variability is the variance: 12.8 square-inches. In less strange units, the standard deviation is \\(\\sqrt{12.8\\ \\text{square-inches}} = 3.6\\) inches\n\n\nThe most common action for the rest of this course will be to create a model and summarize it.\nExample: lm(height ~ mother + father, data=Galton)\n\nheight ~ mother + father is a tilde expression that specifies the roles of variables in the model. height is the response variable. mother and father are the explanatory variables.\ndata = Galton tells lm() to use the Galton data frame to construct the model corresponding to the tilde expression.\n\nExample: Summarizing functions R2() and conf_interval()\n\n\n\nlm(height ~ mother + father, data=Galton) |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    898 \n    2 \n    0.109 \n    54.7 \n    0.107 \n    0 \n    2 \n    895 \n  \n\n\n\n\nInterpretation: mother and father jointly explain about 10% of the variance in the height of their adult children.\n\nlm(height ~ mother + father, data=Galton) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    13.900 \n    22.300 \n    30.80 \n  \n  \n    mother \n    0.187 \n    0.283 \n    0.38 \n  \n  \n    father \n    0.290 \n    0.380 \n    0.47 \n  \n\n\n\n\n\nInterpretation: The model equation for a person’s height in inches is, according to this model:\n\\[\\text{person's height} = 22.3 + 0.283\\ \\mathtt{mother} + 0.380\\ \\mathtt{father}\\]\n\n\nRegression models (such as those built by lm()) quantify how to account-for/explain the variation in the response variable in terms of the variation in the explanatory variables."
  },
  {
    "objectID": "posts/Takeaways-20/index.html#new-topics",
    "href": "posts/Takeaways-20/index.html#new-topics",
    "title": "Lesson 20 take-aways",
    "section": "New topics",
    "text": "New topics\n\nAccount-for/explain. Often when we use the word “explain” we mean to suggest a causal connection. For instance, this randomized clinical trial established that a particular blood-pressure drug leads to lower blood pressure, that is, it causes the blood pressure to go down.\nWhen we say that “A causes B,” we don’t necessarily mean that A is the complete and total explanation for B. More often, we mean that “A contributes in some way to the value of B.” For instance, “high blood pressure increases mortality” does not mean that high blood pressure is the sole determinant of mortality. Instead, it means that high blood pressure contributes to an increased risk of mortality.\nA DAG (Directed acyclic graph—unnecessarily intimidating name!) is a way of encoding a hypothesis of what causes what in a system. We discussed the system involving treating a battlefield casualty with a tourniquet. (Link to in-class activity.) The system—a “system” is a collection of components—involved USE of a tourniquet, SEVERITY of injury, staying alive long enough for ADMISSION to hospital, and post-hospital SURVIVAL. Common sense suggests some causal connections:\n\nSEVERITY \\(\\longrightarrow\\) USE, since tourniquets are used for severe injuries involving bleeding from a limb.\nSEVERITY \\(\\longrightarrow\\) ADMISSION. The more severe the injury, the less likely the soldier is to reach the hospital.\nADMISSION \\(\\longrightarrow\\) SURVIVAL. There would be little point to having hospitals if they didn’t contribute to survival!\nSEVERITY \\(\\longrightarrow\\) SURVIVAL. Even if reaching the hospital, the soldier’s survival is in part determined by the SEVERITY of the injury.\n\nOther link were more hypothetical:\n\nUSE \\(\\longrightarrow\\) ADMISSION\n\n\nUSE \\(\\longrightarrow\\) SURVIVAL\n\nA DAG describes the hypothesized causal links among all the system components.\n\n\n\n\n\n\n\n\n\n\n\n\nMore detail\n\n\n\nThe clinical study looking at the relationship between tourniquet use and survival included only those patients admitted to the hospital, not the ones who died before admission. Their data did not demonstration a relationship between USE and SURVIVAL. That is to say, the hypothesized DAG as a result of the study was this:\n\n\n\n\n\nIn this second DAG, there is no causal connection between USE and SURVIVAL.\nA critique of the clinical trial is that it didn’t consider ADMISSION, that is, both those soldiers who reached the hospital and those who didn’t. Consequently, even if there were no direct link between USE and SURVIVAL, a more complete DAG includes a possible indirect path, as in this third DAG:\n\n\n\n\n\nThe path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL could not be demonstrated by the (imperfect) clinical study, even though it might well exist. Any decision about the efficacy of tourniquets would need to consider the possible path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL.\nHere’s a paradox: Effective use of tourniquets increases hospital mortality rates. Explain why this says something positive about tourniquet use."
  },
  {
    "objectID": "posts/Takeaways-20/index.html#not-gotten-to-in-class",
    "href": "posts/Takeaways-20/index.html#not-gotten-to-in-class",
    "title": "Lesson 20 take-aways",
    "section": "NOT gotten to in class",
    "text": "NOT gotten to in class\n\nUse of sample() and dag_draw() with DAGs.\nWhat is a “random trial”\nHow (and why) to automate replication of random trials.\n\nYou can learn these things from the text and the worksheet for Lesson 20."
  },
  {
    "objectID": "posts/Takeaways-27/index.html",
    "href": "posts/Takeaways-27/index.html",
    "title": "Lesson 27 take-aways",
    "section": "",
    "text": "In our original plan for Math 300Z, Lesson 27 was to be the first of two exam days. That plan changed; we dropped the first exam, keeping only the second. Consequently, we passed directly from Lesson 26 to Lesson 28."
  },
  {
    "objectID": "posts/Self-driving-safety/index.html",
    "href": "posts/Self-driving-safety/index.html",
    "title": "Bayesian updating for self-driving cars",
    "section": "",
    "text": "Suppose ordinary new-ish cars have a mean distance between accidents of 200,000 miles. (This is roughly consistent with the Internet factoid that the probability of a car accident in 1000 miles is 1/366.)\nWhat might a skeptical regulator reasonably believe about newly released self-driving cars?\n“These things are crazy. Very likely to get in an accident.”\n“Perhaps a 1% chance that they are safer than regular cars.”\nIt takes some math to translate these views into a prediction about the actual mean time between accidents. We teach that in other courses. But here is a graph of such a probability function.\nThis is called a “prior” probability distribution: our starting point.\nNow data comes in. Each day we get a report from all the self-driving cars:\nBased on these data, we update our beliefs to produce a “posterior” probability distribution. The Bayesian updating rule is:\nPosterior(m) \\(\\propto\\) Likelihood(m, observations) \\(\\times\\) prior(m)\nSuppose the observation is: car crashed at 23,241 miles. A plausible **likelihood function* is based on simple probability, not the prior.\nThere is also a likelihood function for a car having driven 30,000 miles without an accident."
  },
  {
    "objectID": "posts/Self-driving-safety/index.html#now-for-the-data",
    "href": "posts/Self-driving-safety/index.html#now-for-the-data",
    "title": "Bayesian updating for self-driving cars",
    "section": "Now for the data",
    "text": "Now for the data\nThe New York Times report indicates 400 crashes out of 360,000 self-driving cars. Suppose we observe these data for Tesla\n95 cars have driven 20K miles without an accident; 5 cars had accidents respectively at 1K, 4K, 8K, 12K, 16K\n\n\nCode\nlog_likelihood_observed <- function(m) {\n  ( log10(nocrash_likelihood(m, 20000))*95) +\n    log10(crash_likelihood(m, 1000)) +\n    log10(crash_likelihood(m, 4000)) +\n    log10(crash_likelihood(m, 8000)) +\n    log10(crash_likelihood(m, 12000)) +\n    log10(crash_likelihood(m, 16000)) \n}\n\n\n\n\nCode\nlog_posterior <- function(m) log_likelihood_observed(m) + log10(belief(m))\nslice_plot(10^((log_posterior(m)+38)) ~ m, bounds(m=2000:500000), npts=1001) +\n  geom_vline(xintercept=200000, color=\"red\") +\n  ylab(\"Posterior: relative probability\") + xlab(\"Average miles per accident\")\n\n\n\n\n\nThe posterior indicates that data on 100 cars, 5 of which had accidents before 20,000 miles, places most belief that the self-driving cars are safer than regular cars: about twice as safe.\nThis is counter-intuitive, since we have no data on cars that drove farther than 20,000 miles. But would be hard to get 95 out of 100 cars to 20,000 without an accident if the mean distance betwee accidents were even 100,000 miles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300 Blog",
    "section": "",
    "text": "A Bayes interpretation of Hypothesis testing\n\n\n\n\n\n\n\nlikelihood\n\n\nprior\n\n\nposterior\n\n\nlikelihood ratio\n\n\nodds\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 36 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nlikelihood\n\n\nnull hypothesis\n\n\nalternative hypothesis\n\n\nprior\n\n\nposterior\n\n\np-value\n\n\nconfidence intervals\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nNamed tests from Stat 101\n\n\n\n\n\n\n\none- and two-sample tests\n\n\np-test\n\n\nt-test\n\n\nANOVA\n\n\nsimple regression\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nA bad graph for medical screening\n\n\n\n\n\n\n\nprevalence\n\n\nsensitivity\n\n\nspecificity\n\n\nprior\n\n\nposterior\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nCompeting two hypotheses\n\n\n\n\n\n\n\nlikelihood\n\n\nprior\n\n\nposterior\n\n\nlikelihood ratio\n\n\nodds\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 35 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nhypothesis (definition)\n\n\nsensitivity/specificity\n\n\nprevalence\n\n\nprior\n\n\nposterior\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 34 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nclassifier\n\n\nlogistic regression\n\n\nthreshold\n\n\nloss function\n\n\nfalse-positive/false-negative\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 32 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nexperiment\n\n\nblocking\n\n\nrandom assignment\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 28 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\ncovariate\n\n\nadjustment\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 27 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 26 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nprediction\n\n\nprobability distribution\n\n\nbayesian updating\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 25 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nprediction\n\n\nestimation\n\n\nintervals\n\n\nprobability distribution\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nCase study: Prediction vs estimation\n\n\n\n\n\n\n\nSAT\n\n\nprediction\n\n\nestimation\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nBayesian updating for self-driving cars\n\n\n\n\n\n\n\nself-driving cars\n\n\nprediction\n\n\nbelief\n\n\nBayes\n\n\nprobability distribution\n\n\nlikelihood\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 24 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\neffect size\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 23 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nconfidence intervals\n\n\nprecision\n\n\naccuracy\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 22 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nsignal and noise\n\n\nsampling variability\n\n\nsampling variance\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 21 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nsignal and noise\n\n\nlinear model shapes\n\n\nmodel function\n\n\nmodel value\n\n\nresidual\n\n\nresponse value\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 20 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nvariance\n\n\nDAGs\n\n\ncausality\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 19 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nvariance\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nStatistical or not?\n\n\n\n\n\n\n\nvariation\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nCan ChatGPT help in interpreting or correcting R commands?\n\n\n\n\n\n\n\nR\n\n\nChatGPT\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Notes, comments, tools, and suggestions relating to Math 300Z, the prototype run of the core statistics course at USAFA.\nThis blog contains Danny Kaplan’s personal opinions. Nothing on this blog necessarily reflects the views or policies of the US Air Force Academy, the Department of Defense, or the US government generally."
  },
  {
    "objectID": "posts/Takeaways-32/index.html",
    "href": "posts/Takeaways-32/index.html",
    "title": "Lesson 32 take-aways",
    "section": "",
    "text": "Statisticians divide data into two broad categories, depending on how it was collected.\n\nObservational data, where the mechanism behind the data operates in its natural way.\nExperimental data, where the investigator intervenes to change the mechanism in order to simplify the mechanism or reduce possible ambiguity in the data analysis.\n\nFigure 1 is an example of system in which treatment is the explanatory variable of interest and outcome is the response.\n\n\n\nCode\nNatural <- dag_make(\n  covar ~ exo(),\n  unknown ~ exo(),\n  treatment ~ covar + unknown,\n  outcome ~ treatment + covar + unknown\n)\nExpt <- dag_make(\n  investigator ~ exo(),\n  covar ~ exo(),\n  unknown ~ exo(),\n  treatment ~ investigator,\n  outcome ~ treatment + covar + unknown\n)\nset.seed(101); dag_draw(Natural, vertex.label.cex=1)\nset.seed(101); dag_draw(Expt, vertex.label.cex=1)\n\n\n\n\n\n\n\n\n(a) System as it operates naturally.\n\n\n\n\n\n\n\n(b) System with investigator intervention.\n\n\n\n\nFigure 1: The investigator is interested in studying the direct effect of treatment on outcome. A natural system compared to another system in which the investigator has intervened to simplify the system with respect to data analysis.\n\n\n\n\nThe data analysis techniques we have been using apply equally well to both observational and experimental data, although sometimes simpler techniques can be used in working with experimental data because the data-generation mechanism is simpler.\n\nIn the mechanism of Figure 1(a), studying the direct relationship treatment \\(\\rightarrow\\) outcome requires adjusting for the confounders covar and unknown which lie on back-door pathways between treatment and outcome. But without data on unknown, this is impossible.\nIn the mechanism of Figure 1(b), there are no backdoor pathways from treatment to outcome, so the direct link can be studied without adjustments for covariates, known or unknown. In other words, analysis is simple and unambiguous despite the unknown factors.\n\nThe trick to disconnecting treatment from the causal influences of covar or unknown is for the investigator to assign the values of treatment. Consider a medical investigation where treatment is whether a patient takes aspirin, which might be influenced by the overall health of the patient, some aspects of which might be measured (covar) and some not (unknown). In a medical experiment (often called a “clinical trial”), the patient would be assigned to either of two treatments: take aspirin or not. (In good experiments, “not” might mean taking a placebo, to be explained below.)\nThere’s a potential confusion introduced by using the word “treatment” for the experimental variable. In everyday speech, “treatment” refers to giving a drug or surgery or some other active intervention. In statistical vocabulary, a “treatment” can be “doing nothing.” Keep in mind that our data analysis techniques are always about relating the variation in an explanatory variable (and, as we choose, covariates) to the variation in the response. This means that the investigator must create variation in the treatment variable. This is done by giving different types of treatment to the different experimental units (e.g. patients).\nCare should be taken in the investigator’s choice of which treatment should be given to which experimental unit. The key consideration is to avoid re-creating a link covar \\(\\rightarrow\\) investigator or unknown \\(\\rightarrow\\) investigator. A simple and effective safeguard is to use random assignment, that is, the value of investigator is set at random. Experience demonstrates that even when an investigator thinks his choice of treatment is random, there is some influence from outside factors. Example: The investigator prefers to assign placebo to those patients who are thought to be more likely to have a bad reaction. Another example: It’s convenient for record-keeping purposes to assign the intervention to those in the first months of a study and reserve the placebo for those in the later months. However, overall conditions might be different over time, e.g. winter turns into spring, medical suppliers or personnel change.\nConsequently, it’s best for the investigator to let the random assignment be done by a computer random-number generator. This is an important part of experimental method and ought to be reported in any write-up of the experimental results.\nAnother situation that can create unintended backdoor pathways between treatment and outcome is when knowledge of the treatment can alter behavior by the experimental unit (patient) or the person who measures the outcome. For instance, in a vaccine trial, patients who know they have received a vaccine might be more willing to expose themselves, as with people returning to public venues after getting the COVID vaccine. Similarly, a doctor diagnosing a patient might be more inclined to look for other causes than COVID if the doctor knows that the patient was vaccinated.\nTwo methods are widely used to avoid such situations. The patients can be, in many circumstances, made unaware of whether they received the active treatment. A study that does this is called “blinded.” One way to accomplish this is to use a “placebo” or other sham treatment as the alternative to the active treatment. In a drug trial, for instance, all patients are given pills to take, but the patient doesn’t know whether the pill is an active treatment or not. In a vaccine trial, all patients are given the injection so the patient doesn’t know if they received the vaccine or an inert substance.\nOften it is possible to arrange things so that the investigator doesn’t know whether treatment or placebo was given. (This information is hidden from the investigator until all the data has been collected, and revealed only in the data analysis phase.) Studies which keep the investigator ignorant in this way are called “double blinded,” since usually the patient is also kept ignorant of the treatment type.\n\nGOING FURTHER. Sometimes it’s impossible to keep the treatment from being affected by other factors. For instance, patients might not comply with the instructions for taking a drug. Or, suppose treatment is assignment to a training program. Not all people so assigned might be active participants in the program. The non-compliance can, potentially, re-create the back-door pathways between treatment and outcome, as in Figure 2.\n\n\nCode\nIntent <- dag_make(\n  investigator ~ exo(),\n  covar ~ exo(),\n  unknown ~ exo(),\n  treatment ~ investigator + covar + unknown,\n  outcome ~ treatment + covar + unknown\n)\nset.seed(101); dag_draw(Intent, vertex.label.cex=1)\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nThere are two methods available to deal with such situations.\n\nIntent to treat involves a very easy change in the data analysis: instead of modeling outcome ~ treatment, model outcome ~ investigator. In other words, look toward the intent of the investigator rather than the actual value of treatment. Notice in Figure 2 that there are no back-door pathways between investigator and outcome. The model outcome ~ investigator therefore avoids confounding. This is not a panacea, since in the case of non-compliance, the effect size of investigator on outcome will tend to be weaker than the direct effect size of treatment on outcome.\nA more mathematically sophisticated analysis goes by the name instrumental variables, a technique especially esteemed in econometrics. Remarkably, the intrumental variable technique is used even in a non-experimental setting, where the investigator has not intervened in the system. In such cases, the technique relies on identifying a measurable variable, called an “instrument,” that takes the place of investigator. The theory of instrumental variables is too difficult to summarize here. The term “natural experiment” is often used to describe a study undertaken with instrumental variables. But such studies are not “experiments” in the sense used above, which involves an investigator intervening in the causal system."
  },
  {
    "objectID": "posts/Takeaways-35/index.html",
    "href": "posts/Takeaways-35/index.html",
    "title": "Lesson 35 take-aways",
    "section": "",
    "text": "Define a hypothesis (in a statistical sense) as “a statement that might or might not be true.” This is really just a wordy way of saying, “a statement,” but we want to emphasize that the statement doesn’t have to be true. Examples of types of hypotheses we’ve encountered in this course:\n\nModels\nDAGs\n\nBut here we are interested in statments about the world, such as “the patient has congestive heart failure,” or (in an utterly different setting) “the aircraft is a threat.”\nDefine an argument (in the sense of critical thinking) as “reasoning to demonstrate the truth/falsity/credibility of a hypothesis.” The conclusion from an argument is a statement about the truth/falsity or credibility of a hypothesis.\nIn science, there are two main forms of argument:\n\nDeductive reasoning where one starts with premises, that is, statements that are assumed to be true, and modify those statements using a mechanism that is generally accepted as correct. Example: Algebra where you start with an equation and then apply operations to both sides (e.g. multiply both sides by x) to create a new equation. Example: DAGs, where we learned the machinary to identify covariates that can block or open causal paths, and used this to decide which covariates ought be included in or excluded from a model.\nAlmost all of your math and science training has featured deductive reasoning, and you know how to check an argument to determine if you should believe the conclusion: a) check the premises for truth, b) check the mechanical steps of going from one statement to another.\nInductive reasoning where one starts with observations/data and moves to a conclusion about one or more hypotheses. (Inductive arguments generally contain premises such as models and DAGs and deductive reasoning.)\n\nThe setting for inductive reasoning we will work with involves two mutually exclusive hypotheses relevant to the question of interest, then using observations to reach a conclusion about the respective credibility of the hypotheses. Examples of mutually exclusive hypotheses: the patient has the disease versus the patient does not have the disease; the aircraft is a threat vs it is not a threat. (There are also important settings with multiple hypotheses, and the methods we’ll discuss are relevant to those settings, with more complicated book-keeping. For us, the two-hypothesis setting will suffice. Happily, many real-world situations correspond to the two-hypothesis deductive reasoning setting.)\nThe two-hypothesis setting is directly relevant to decision-making, insofar as heightened credibility in one of the hypotheses often corresponds to taking a particular action, for instance, medical treatment for a patient who has the disease.\n\nThe framework for deductive reasoning we consider first is exemplified by medical screening tests, such as testing for colon, breast, and prostate cancers. The two contending hypotheses will be labelled “D” and “H,” which you can think of as “disease” and “healthy” respectively.\n\nIn this framework, much preliminary work (see Lesson 34) has been done before the test is applied to any patient. For instance, researchers have developed a testing method which might involve measuring antigen levels in blood or reading X-rays. The testing method amounts to a machine which takes the measurement of antigen levels or the X-ray and translates it into one of two results, a positive test result or a negative test result. Such a machine is called a classifier.\n\nOnce the classifier has been developed, applying it to a patient is an essentially automatic procedure that takes the patient as “input” and produces the test result (“positive” or “negative”) as output.\nThe question for the inductive reasoner is how to decide whether any given classifier accomplishes what you want, how to frame “what you want,” and how to interpret the result of the test. For instance, studies have shown that physicians and patients are likely to read something into the test result that is not at all justified. For instance, the women who test positive on a mammogram, and their physicians, often conclude that this is a definitive diagnosis, which is not at all the case.\n\nIn Lesson 34, we considered the perspective of the researchers who are developing a test. Refer to those notes for a refresher. The upshot of that lesson is that the proper way to describe the quality of performance of the test is with two numbers, each of which is a probability. These numbers are the sensitivity and specificity of the test. It’s important to understand that it takes two numbers to describe the test performance. Many people wrongly assume that only one number is required, the test accuracy. In reality, accuracy is a naive and incomplete concept and should not be used.\nThe patient comes in, the test is performed, and a result is found: positive or negative. Now we need to translate that result into the credibility level to be assigned to each of the hypotheses D and H. This is done by using ancillary information to calculate a probability level for hypothesis D. The probability level for hypothesis H will be the complement of that for D. For instance, if D has a 75% probability, H has a 25% probability.\nWhat is the ancillary information needed to translate the positive-or-negative test result into a probability for D? In the setting of disease, this information is the prevalence of the disease. Prevalence is the fraction of the population who have the disease. (Which population is relevant to any given patient is a question of some importance, but for us, the prevalence is just a number between zero and 100%)\nThe three numbers needed to translate the positive/negative test result into a probability for the D hypotheses are:\n\nThe sensitivity\nThe specificity\nThe prevalence\n\n\nThis diagram shows how to put these three numbers together to find the probability of hypothesis D given a positive test using a sensitivity of 90%, specificity of 80%, and prevalence of 10%.\n\n\n\n\n\nThe sensitivity is the proportion of the left (D) bar where the test is +. The specificity is the proportion of the right (H) bar where the test is -.\nThe prevalence is the proportion of the whole that is in the left (D) bar.\nThe probability of D given a + test is 9/(17+9)\nThe probability of H given a - test is 1/(73+1)"
  },
  {
    "objectID": "posts/Takeaways-34/index.html",
    "href": "posts/Takeaways-34/index.html",
    "title": "Lesson 34 take-aways",
    "section": "",
    "text": "We talked about “classifiers,” a mechanism aimed to distinguish people with a given condition (e.g. prospective congestive heart failure, or colon cancer, …) from people without that conditions.\nThe classifier takes measurements from a person and returns either a positive (\\(\\mathbb P\\)) or a negative (\\(\\mathbb N\\)) result.\nThe two numbers that summarize the performance of the test are:\n\nSensitivity: The fraction of people with the condition who test \\(\\mathbb P\\), which is the correct result for such people.\nSpecificity: The fraction of people without the condition to test \\(\\mathbb N\\), which is the correct result for this other group of people.\n\nAn incorrect result is called “false,” a correct result is “true.”\n\nFor people who do not have the condition, a \\(\\mathbb P\\) result is false. Such people are called “false positives.” A good classifier keeps the rate of false positives low because such people may receive treatment which is not needed.\nFor people who have the condition, a \\(\\mathbb N\\) result is false. Such people are called “false negatives.” A good classifier keeps the rate of false negatives low because such people do not receive the treatment which is called for.\n\nBy changing a number called the “threshold,” the false negative rate can be improved, but at the cost of a higher false positive rate. And vice versa. That is, there is always a trade-off between false positives and false negatives.\nWe resolve the trade-off by looking at the overall loss due to the mistaken classifications. The loss for a false positive is generally different (often, less) than the loss for a false negative. To calculate the overall loss, multiply the false-positive rate and the false-negative rate by their respective losses, and add. Choose the threshold that minimizes the overall loss."
  },
  {
    "objectID": "posts/Takeaways-34/index.html#more-detail",
    "href": "posts/Takeaways-34/index.html#more-detail",
    "title": "Lesson 34 take-aways",
    "section": "More detail",
    "text": "More detail\n\nA classifier is a machine that, based on measurements of some sort assigns a categorical level to the object the measurements came from. We can make this less abstract by talking about classifiers in the context of medical screening. There, the “object” is a patient; the possible categorical levels are \\(\\mathbb{P}\\) and \\(\\mathbb{N}\\), meaning a “positive” test result or a “negative” test result.\nIn this Lesson, we demonstrated how to build a classifier. The classifier takes the form of a statistical model fitted to training data.\nSkipping, for the moment, to the use of a classifier that’s already been built, this is a matter of evaluating the statistical model at the inputs relevant to the patient, then applying a threshold to the model output to make the choice between \\(\\mathbb{P}\\) and \\(\\mathbb{N}\\). Interpreting the meaning of \\(\\mathbb{P}\\) and \\(\\mathbb{N}\\) requires that we know about:\n\nThe performance of the classifier, which is summarized by two numbers, the sensitivity and the specificity. Using these two numbers (along with a third number, called the prevalence) is the topic of Lesson 35.\nYou may hear people talk about the “accuracy” of a classifier. We’ll show how, in the process of building the classifier, to calculate the accuracy, specificity, and sensitivity. Note that the “accuracy” is inadequate for describing the classifier.\n\nThere are three steps to building a classifier:\n\nAssembling training data. This is a very difficult and drawn-out process, usually done in a clinical setting and involving hundreds or thousands of test subjects.\n\nIn our example, we used the math300::Framingham data frame, which is the product of a lot of work by many people.\nThe training data includes one or more measurements made on each of the test subjects. In Framingham such measurements included age, sex, smoking status, whether the subject was taking medicine for high blood pressure, BMI, and so on. In the well known screening tests (e.g. for breast cancer, prostate cancer, colon cancer) the measurement is often the concentration in blood or stool of a particular antigen.\nThe training data must include an outcome variable. With Framingham, the outcome is TenYearCHD which records whether or not the subject developed congestive heart failure (CHD) in the ten-year follow up period (that is, the ten years after the measurements in (b) were made. The outcome variable is a zero-one variable.\n\nBuilding a statistical model of the outcome variable as a function of one or more of the measurements in (b). The role of this model, once built, is to convert the measurements in (b) into a score. Often in such model building, care is taken to identify the measurements in (b) that give the best “performance.” Performance refers to producing a wide range of model output values that correlate well with the outcome variable.\nThe final step is to establish a threshold. This is a number, in the units of the score. When the score is above this threshold, the classifier produces a \\(\\mathbb P\\) result. A score below the threshold means the result is \\(\\mathbb N\\).\n\nIn class, we used logistic regression to build the statistical model (section 4.ii). Then we evaluated the model on the training data to produce a score for each subject in the training data. Each of these subjects either developed CHD (in the ten-year follow-up) or did not. The fraction of subjects who developed CHD is called the “training prevalence.” In the example constructed for the table below (in (5)), there are 1000 subjects altogether, of whom 50 +150=200 had CHD as the outcome. The training prevalence is the fraction of the whole who had the CHD outcome: 200/1000 = 20% in the example below.\nThe process of choosing a threshold (section 4.iii) is an important part of the decision-making guided by the test. It works like this:\n\nChoose a candidate threshold. Apply this to each of the scores in the model-evaluated training data to produce a new column which we can call the output of the candidate test. This will have entries that are \\(\\mathbb P\\) or \\(\\mathbb N\\) depending on whether the score is above the candidate threshold.\nTally up the table using the outcome (disease or not) and the test result (\\(\\mathbb P\\) or \\(\\mathbb N\\)). This will produce a table like this:\n\n\n\n\n\npatient outcome\ntest output\ncount\ndescription\n\n\n\n\nDisease.\n\\(\\mathbb P\\)\n150.\n“true positive”\n\n\nDisease.\n\\(\\mathbb N\\)\n50.\n“false negative”\n\n\nHealthy\n\\(\\mathbb P\\)\n75.\n“false positive”\n\n\nHealthy.\n\\(\\mathbb N\\)\n725.\n“true negative”\n\n\n\n\nFrom the above table, you can easily compute three measurements of the performance of the test:\n\n\nsensitivity: The fraction of the diseased subjects who tested positive. In the example, that’s 150/(150+50) = 75%.\nspecificity: The fraction of the healthy subjects who had a negative test result. Here that’s 725/(725+75) = 90.6%.\n\nNote that neither the sensitivity nor the specificity reflect the training prevalence, since each is calculated within either the disease group or the healthy group.\n\naccuracy: The “accuracy” is the fraction of all subjects who received a correct test result, that is, one that matches the patient outcome. Here that’s the fraction with “true” in the description, that is, (150+725)/1000 = 87.5%. The problem with “accuracy” is that it depends strongly on the training prevalence.\n\n\nThe training prevalence is an artifact of the data collection process and is typically much higher than the prevalence in the overall population, that is, the population prevalence.\nRemember that we are still in the process of evaluating the merits of the candidate threshold used to construct the table above. Now we want to re-arrange the counts in that table to reflect the population prevalence.\n\nThe training prevalence was 20%. Suppose that the prevalence in the population to whom the test is targeted is 5%.\nConstruct a new table of counts where the sensitivity and specificity are exactly as found in the table from the training-data/candidate-threshold but where the prevalence is the desired 5%. We can accomplish this by arithmetically increasing the numbers in the healthy category so that there are altogether 4000 people in the table. (200 diseased out of 4000 total is a prevalence of 5%.) In increasing the number of healthy people, we need to be careful to keep the specificity at the level found from the training data: 90.6. That is, we will change the healthy numbers so that there are 3800 healthy people (giving 4000 total, including the diseased) of whom 90.6% got a \\(\\mathbb N\\) result. This gives us 38000.906 = 3443 in the last row and 3800(1-0.906) = 357 in the second to last row.\n\n\nThe adjusted table that reflects the population prevalence (as opposed to the training prevalence) is:\n\n\n\npatient outcome\ntest output\ncount\ndescription\n\n\n\n\nDisease\n\\(\\mathbb P\\)\n150\n“true positive”\n\n\nDisease\n\\(\\mathbb N\\)\n50\n“false negative”\n\n\nHealthy\n\\(\\mathbb P\\)\n357\n“false positive”\n\n\nHealthy\n\\(\\mathbb N\\)\n3443\n“true negative”\n\n\n\n\nTime to evaluate the candidate threshold.\n\nWe can measure how good the threshold is by calculating the “accuracy” of the test on the tests results on the adjusted table. Unlike the accuracy on the training results, which depends on an unrealistic prevalence, the adjusted table has the right prevalence. The accuracy is (150 + 3443)/4000 = 89.8%. Looking at things from the other side, the “mistake” rate is 11.2%.\nIt’s usually the case that the cost of making a mistake is very different for a “false negative” than a “false positive.” A person with a false negative will not receive the appropriate treatment for his or her disease. We call this a “loss,” and it may be a very big problem for that person. On the other hand, a person with a false positive will receive treatment that is unnecessary and potentially harmful, risky, or imposes financial or emotional costs. That’s also a loss. It is a matter of judgement what the relevant sizes of the loss from a false negative and the loss from a false positive.\n\nLet’s suppose that the false-negative loss is 10, and the false-positive loss is 0.5. (These are just made-up numbers for the example.) Then, referring to the table above, the total loss is 50$\\(10 + 357\\)\\(0.5 = 678.5\\). The loss rate, that is, the loss per person, is 678.5/4000 = 0.17 per person.\n\nThe test developers can repeat all these calculations for other candidate thresholds. That will give them a set of thresholds and a set of corresponding loss rates. Pick the threshold with the lowest loss rate.\nNote the importance of the relative sizes of the false-positive loss and the false-negative loss. Expertise in the area of application of the test and follow-up treatments is important to assign meaningful values to the comparative losses. Our 10 and 0.5 are just for the purposes of example."
  },
  {
    "objectID": "posts/Named-tests/index.html",
    "href": "posts/Named-tests/index.html",
    "title": "Named tests from Stat 101",
    "section": "",
    "text": "A Stat 101 course will cover many hypothesis tests among which are the one-sample t-test, the two-sample t-test, the one and two sample p-tests, and ANOVA. All these different tests are in reality just linear regression.\nTo see the translation between regression and the sundry named tests, imagine some data with quantitative variables x and y, a zero-one variable z, and a categorical variable (with potentially many levels) g. Each named hypothesis test corresponds to a particular model specification.\n\none-sample t-test: y ~ 1 and take the p-value from the intercept.\ntwo-sample t-test: y ~ g when g has just two levels. Take the p-value from the g coefficient.\none-sample p-test: z ~ 1 and take the p-value from the intercept.\ntwo-sample p-test: z ~ g when g has just two levels. Take the p-value from the g coefficient.\ntest on simple regression: y ~ x and take the p-value from the x coefficient.\n“one-way” ANOVA: y ~ g when g has more than two levels. There will be a p-value for each level of g (except the reference level), but the p-value for this test is the one from R2() or anova_summary(), which doesn’t refer to a particular coefficient.\n\nThere are many other “forms of ANOVA” that are not covered in Stat101. These all fit in with the linear regression framework. For instance:\n\n“two-way” ANOVA: y ~ g1*g2 and take the p-value from the interaction term in the anova_summary() report.\nanalysis of covariance (ANCOVA): y ~ x + g and take the p-value from the g term in the anova_summary() report."
  },
  {
    "objectID": "posts/Competing-two-hypotheses/index.html",
    "href": "posts/Competing-two-hypotheses/index.html",
    "title": "Competing two hypotheses: a better graph",
    "section": "",
    "text": "In Lesson 35, in the context of medical screening tests, we presented diagrams like this one.\n\n\n\n\n\nThis diagram is based on only three basic numbers—sensitivity, specificity, and prevalence. Exactly the same information could be presented in a 2x2 table:\n\n\n\nTest result\nSick patients\nHealthy patients\n\n\n\n\n\\(\\mathbb P\\)\n12% (true positives)\n26% (false positives)\n\n\n\\(\\mathbb N\\)\n3% (false negatives)\n60% (true negatives)\n\n\n\nThe four numbers necessarily add up to 100%, so one of the numbers is redundant. To generate the table we only need the three basic numbers:\n\nprevalence: 15%, that is, true positives + false negatives\nsensitivity: 12%/(12%+3%) = 80%, that is, true positives divided by prevalence\nspecificity: 70.6%, that is, true negatives/(1-prevalence). Filling in the numbers 60%/(1-15%) = 70.6%.\n\nUsually in statistical graphics, we place the scales on the horizontal and vertical axes, which is not the case with the above diagram. Sticking with the scales-on-axes convention, here is a streamlined graph:\n\n\n\n\n\nWe’ve generalized the notation a bit and emphasized (1-specificity) rather than the specificity itself.\n\nPrior(Alternative) = width of “Alternative” box.\nLikelihood for Alternative hypothesis, that is, p(\\(\\mathbb P\\) | Alternative) (corresponds to sensitivity)\nLikelihood for Null hypothesis, p(\\(\\mathbb P\\) | Null) (corresponds to 1-specificity.)\n\nThe area of each box is, as expected, the width times the height. The two areas printed on the graph are the ingredients for the calculation of the posterior:\nposterior(Alternative | \\(\\mathbb P\\)) = 0.12/(0.12 + 0.255) = 32%"
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html",
    "href": "posts/Hyp-test-Bayesian/index.html",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "",
    "text": "The Bayesian paradigm provides a complete framework for competing two hypotheses against one another. We introduced that framework in the context of screening tests where the hypotheses—diseased vs healthy—are concrete, the likelihoods—p(\\(\\mathbb P\\)) | diseased) and p(\\(\\mathbb P\\) | healthy)—can be found by cross tabulation of test results and disease state.\nThis essay contrasts the Bayesian framework with Fisher’s significance testing and Neyman-Pearson hypothesis testing. The point is to show what each of these testing frameworks leaves out. To simplify the comparison, we’ll use Null and Alternative as the names for the two Bayesian hypotheses, even though Bayes allows a more general choice (or even multiple hypotheses).\nHere is a graphic showing the three inputs to the Bayesian calculation of the posterior probability p(Alternative | obs ): i. the prior p(Alternative) and the two likelihoods ii. p(obs | Alternative) and iii. p(obs | Null)."
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html#comparing-the-three-frameworks",
    "href": "posts/Hyp-test-Bayesian/index.html#comparing-the-three-frameworks",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "Comparing the three frameworks",
    "text": "Comparing the three frameworks\nThe three frameworks—Bayes, Significance, and Neymann-Pearson—all produce a outcome after data have been collected. In Bayes, this structure is more evident from the name “posterior” for the outcome, but it’s equally true in the other two frameworks.\n\nTest set-up\nThe Bayes and Neymann-Pearson frameworks involve two stages; one can occur before the data observations are made, the other only after the observations.\nIn Neymann-Pearson, the set-up amounts to the study design:\n\nspecification of an Alternative hypothesis. (The Null is typically obvious, e.g. a model coefficient would be found to be zero with infinite data.)\nwhat data is to be collected and what test statistic to use to summarize the data,\ndetermining the sample size.\n\nThese three items go into a calculation of the “power” of the test. The power is the conditional probability that the outcome of the test will be to reject the Null. The “given” part of the conditional probability is the Alternative hypothesis. Calculation of the power is done by simulation (or the algebraic equivalent); power does not depend on the eventual observation.\nStudy design is an iterative process which aims to find a feasible study design that maximizes the power. A study design which achieves a power of 80% is generally considered adequate.\nIn Bayes the set-up involves:\n\nSelection of a prior, p(Alternative). This might be subjective or based on existing knowledge (before the data are available). \nCalculation of two likelihood functions, one under the Null and the other under the Alternative. Each of these functions will, eventually, be given the results from the data as input and produce a numerical likelihood as the output.  These functions are often produced by applying a relevant probability model.\n\nIn the case of screening tests, the prior amounts to the prevalence of the disease in the relevant population.With screening tests, this calculation is simple because there are only two possible results from the data: \\(\\mathbb P\\) or \\(\\mathbb N\\). The research project which developed the test determines the sensitivity and specificity, p(\\(\\mathbb P\\) | Alternative) and p(\\(\\mathbb N\\) | Null). The likelihood functions consist of these two likelihoods and their complements.Significance testing has no set-up phase. This is one of the most attractive features of the framework.\n\n\nAllowed inputs to determine the test output\nNeither Significance nor Neymann-Pearson need a prior to be specified for either hypothesis. Bayes requires a prior for both. [In the Bayesian framework, the prior on the Null is simply 1 minus the prior on the Alternative. So only one of the priors needs to be specified.]\n::: {#fig-fisher-neymann diagrams .cell .column-page-right layout-ncol=“2”} ::: {.cell-output-display} {#fig-fisher-neymann diagrams-1 width=672} :::\n\n{#fig-fisher-neymann diagrams-2 width=672}\n\nModification of the Bayes diagram showing the information used to determine the outputs from the Significance framework and Neymann-Pearson framework. :::\n\n\nCalculation of test output\nSignificance framework: The likelihood of the observation under the Null hypothesis is calculated and called the “p-value.” If p is small (usually p < 0.05) then the output is “reject the Null.” Otherwise the output is “fail to reject the Null.”\nNeymann-Pearson framework: Same as for the Significance framework. (Note that the power isn’t used for the calculation of the test output. It’s only involved in the study design.)\nBayes framework: The test output is a posterior probability, not a phrase like “reject the Null.” Calculation of the posterior can be stated in terms of the two areas shown in the graph:\np(Alternative | Obs) = green/(green + blue)\nFor the example in Figure 1, this amounts to 0.12/(0.12+0.17) = 41%. That’s about three times larger than the prior on the Alternative."
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html#alignment-of-the-tests",
    "href": "posts/Hyp-test-Bayesian/index.html#alignment-of-the-tests",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "Alignment of the tests",
    "text": "Alignment of the tests\nThe output of the Significance and Neymann-Pearson tests is always the same, at least if the study design calculated under Neymann-Pearson happened to be the same as for Significance testing.\nThe output from the Bayes framework can be compatible or not, depending as it does on the choice of prior and the likelihood under the Alternative hypothesis.\nPurely for the sake of comparison, let’s look at the Significance and Neymann-Pearson calculations as if there were a meaningful prior in those frameworks."
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html#the-bayesian-paradigm",
    "href": "posts/Hyp-test-Bayesian/index.html#the-bayesian-paradigm",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "The Bayesian paradigm",
    "text": "The Bayesian paradigm\nThe Bayesian paradigm is not limited to considering only two hypotheses; that’s just one setting for it. Let’s adopt this setting, accepting the names “Null” and “Alternative” for the two competing hypotheses. The Bayesian calculation of the posterior requires statement of a prior for each of the hypotheses. (The prior for one hypothesis will be one minus the prior of the other.)\nThere is a nice version of the Bayes calculation in terms of the likelihood ratio and prior and posterior odds.\n\\[\\underbrace{odds(H_a | \\text{obs})}_{\\Large\\text{posterior for } H_a} = \\underbrace{\\left[\\frac{{\\cal L}_a(\\text{obs})}{{\\cal L}_0(\\text{obs})} \\right]}_{\\Large\\text{Likelihood ratio}}\\ \\times\\ \\underbrace{odds(H_a)}_{\\Large\\text{prior for } H_a}\\]\nIf we stipulate that the same prior odds applies to all three frameworks, the critical quantity in shaping the posterior odds is the “likelihood ratio.” A large likelihood ratio pushes the posterior odds on the Alternative higher, which is analogous to “reject the Null.”\nThe argument being made here is that a small p-value corresponds to a large likelihood ratio.\nIn Significance testing, there is no such thing as \\({\\cal L}_a(obs)\\): the only quantity that comes into the test is \\({\\cal L}_0(obs)\\). The same is true in Neymann-Pearson, but the “power” has a similar spirit to \\({\\cal L}_a(obs)\\). Let’s use 80% (the convention for adequate power) is a placeholder for \\({\\cal L}_a(obs)\\), but the precise value is not critical to the argument, just that there is some non-zero placeholder.\nIn Significance and Neymann-Pearson \\({\\cal L}_0(obs)\\) is called the p-value. A low p-value (conventionally, p < 0.05) argues against the Null. Using the placeholder for \\({\\cal L}_a(obs)\\), a low p-value leads to a high likelihood ratio, which argues for the Alternative.\nSince the Significance framework has nothing even analogous to \\({\\cal L}_a(obs)\\), the choice of the threshold level (e.g. p < 0.05) is never informed by the setting for data collection.\nIn Neymann-Pearson, the power is analogous to \\({\\cal L}_a(obs)\\). Using a power of 80%, then p < 0.05 corresponds to a likelihood ratio of 16 or more. Likelihoods in this range are often described as providing “strong evidence” in favor of the Alternative."
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html#strength-of-evidence",
    "href": "posts/Hyp-test-Bayesian/index.html#strength-of-evidence",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "“Strength of evidence”",
    "text": "“Strength of evidence”\nIt is common to describe the p-value as measuring the “strength of evidence” against the Null hypothesis. For instance, p=0.10 is described as “weak” evidence, while \\(p=0.001\\) constitutes “strong” evidence.\nIn the Bayes framework, “strength of evidence” can be given a reasonable mathematical definition: the likelihood ratio. The “strength of evidence” is the balance (or lack thereof) between \\({\\cal L}_0(obs)\\) and \\({\\cal L}_a(obs)\\).\nIn the Significance and Neymann-Pearson frameworks, there is no such thing as \\({\\cal L}_a(obs)\\). Lacking that, \\({\\cal L}_0(obs)\\) has nothing to be balanced against. In other words, without a \\({\\cal L}_a(obs)\\), there is no way to define what constitutes a very “small” \\({\\cal L}_0(obs)\\). “Small” can only be defined relevant to the general convention, p < 0.05, which is arbitrary. The only way in which a p-value like 0.001 is “small” is that it is below 0.05. No further claim is justified.\nFairness to Fisher and the Significance framework requires that his actual description of the method (e.g. p < 0.05) be considered, as opposed to the interpretation that is been layered on top of it by later research workers.\nIn his 1926 paper introducing Significance testing, Fisher wrote about the 0.05 (“one in twenty”) threshold. Note that he uses the phrase “high enough odds” where today we would say “low enough p-value.”R.A. Fisher (1926) “The arrangement of field experiments” Journal of the Ministry of Agriculture of Great Britain 33:505-513, link\n\n“If one in twenty does not seem high enough odds, we may if we prefer it, draw the line at one in fifty (the 2 per cent. point), or one in a hundred (the 1 per cent. point). Personally, the writer prefers to set a low standard of significance at the 5 per cent. point, and ignore entirely all results which fail to reach this level.”\n\nUp to this point in his narrative, Fisher’s description does not deviate from contemporary practice with \\(p < 0.05\\), although “ignore entirely” doesn’t align with those today who interpret \\(p > 0.05\\) as supporting the Null.\nBut in the next sentences, Fisher describes what it takes for “a scientific fact [to] be regarded as experimentally established,” which I take to be much the same thing as “strength of evidence.”\n\n“A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails* to give this level of significance. The very high odds sometimes claimed for experimental results should usually be discounted, for inaccurate methods of estimating error have far more influence than the particular standard of significance chosen.*”\n\nNeymann-Pearson’s “alternative hypothesis” and “power” are a mathematical statement of what constitutes a “properly designed experiment” and “rarely fails.”"
  },
  {
    "objectID": "posts/Takeaways-36/index.html",
    "href": "posts/Takeaways-36/index.html",
    "title": "Lesson 36 take-aways",
    "section": "",
    "text": "We are spending this week on a topic that constitutes about one-quarter of the consensus Stat 101 course (e.g. “AP Statistics”). We are calling the topic “Null hypothesis testing” (NHT), but other names are also used:\n\n“Significance testing”—the name used by the originator of the method, Ronald Fisher when he introduced it in 1926.\n“Null hypothesis significance testing” (HST—what to say if you can’t decide between calling it NHT or “significance testing.”\n“Hypothesis testing,” the name almost always used in statistics textbooks, but which is misleading in that it suggests something broader than NHT actually is.\n\nOur agenda today is to describe the terminology of NHT and give an example of an NHT calculation (which is easy to do with any statistical software at all).\n\nNHT involves a quantity called the “p-value” which is a number between zero and one.\nWe have talked previously about tests that give a result of either \\(\\mathbb P\\) or \\(\\mathbb N\\). In NHT, the test results are stated differently: either “reject the Null” or “fail to reject the Null.”\nOnce you have the numerical p-value, translation into test results is trivial: if \\(p < 0.05\\) the conclusion is “reject the Null.” Otherwise, that is if p is bigger than 0.05, the conclusion is “fail to reject the Null.” This is admittedly stilted language, and we owe you and explanation for why things are this way. That will come later.\nWhen using linear regression (our main method in 300Z), the software for summarizing models always provides a p-value: you just have to ask for it. To illustrate, consider the model height ~ nkids with respect to Galton’s height data. (Galton (1822-1911) and Fisher (1890-1962) were near contemporaries.)\nThink of height ~ nkids as asking a question: Is the adult height of a child correlated with the number of siblings? (Why might someone offer the hypothesis that the number of siblings has a connection to adult height? Perhaps the growing children had to compete for food. Or perhaps contagious disease is more prevalent in large families, and childhood disease might be correlated with height. But in NHT, there’s no requirement to explain why one is interested to test a hypothesis.) Here’s the calculation, done in four different ways of summarizing a model.\n\n\nmodel <- lm(height ~ nkids, data=Galton)\nmodel |> conf_interval(show_p=TRUE)\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n    p.value \n  \n \n\n  \n    (Intercept) \n    67.2185704 \n    67.7997464 \n    68.380922 \n    0.0000000 \n  \n  \n    nkids \n    -0.2561222 \n    -0.1693416 \n    -0.082561 \n    0.0001372 \n  \n\n\n\nmodel |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    898 \n    1 \n    0.0161062 \n    14.66737 \n    0.0150081 \n    0.0001372 \n    1 \n    896 \n  \n\n\n\nmodel |> regression_summary()\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    67.7997464 \n    0.2961233 \n    228.9578 \n    0.0000000 \n  \n  \n    nkids \n    -0.1693416 \n    0.0442168 \n    -3.8298 \n    0.0001372 \n  \n\n\n\nmodel |> anova_summary()\n\n\n\n \n  \n    term \n    df \n    sumsq \n    meansq \n    statistic \n    p.value \n  \n \n\n  \n    nkids \n    1 \n    185.4636 \n    185.46365 \n    14.66737 \n    0.0001372 \n  \n  \n    Residuals \n    896 \n    11329.5987 \n    12.64464 \n    NA \n    NA \n  \n\n\n\n\nIn the regression summary report and the confidence interval report, a p-value is listed for each coefficient. We are interested in the nkids coefficient.\nIn the R-squared and ANOVA report, there is no p-value on the intercept term; only nkids is at issue.\nNote that the p-value is the same for all four reports.\nAn NHT consists of calculating a p-value, comparing it to 0.05, and drawing the corresponding conclusion. Since \\(p = 0.000137 < 0.05\\), the proper conclusion is to “reject the Null.”\n\n“The Null” is short for “the Null hypothesis.” The dictionary definitions for “null” relevant here include “having or associated with the value zero” or “amounting to nothing.” In the context of linear regression the Null for a given coefficient always means that, with a sufficiently large (“infinite”) sample, the coefficient would be zero.\nThe p-value calculation is done in a mathematical world where the Null is true. Other expressions often used: “assuming the Null,” “given the Null hypothesis,” “under the Null.”\nNaturally, our samples are finite in size. Consequently, because of sampling variation, we cannot expect the coefficient to be exactly zero even under the Null. Instead, we expect the coefficient to be small.\n\nWe have already discussed one operational definition of “small,” that the confidence interval includes zero. If so, then “fail to reject the Null.” Otherwise (as with the nkids example above), then “reject the Null.”\nThe p-value is just another way of encoding the notion of “small.” Indeed, in linear regression we can calculate the p-value from the same information used to construct the confidence interval.\n\n\n\n\n\n\n\n\nMany types of statistical tests?\n\n\n\nA Stat 101 course will cover many hypothesis tests among which are the one-sample t-test, the two-sample t-test, the one and two sample p-tests, and ANOVA. All these different tests are in reality just linear regression. See this blog post.\nThere is one hypothesis test that is not exactly equivalent to regression: the chi-squared test. However, in the context where chi-squared often appears, the result corresponds to the z ~ g model specification. [Blog post not yet available.]"
  },
  {
    "objectID": "posts/Takeaways-36/index.html#abstracting-the-situation",
    "href": "posts/Takeaways-36/index.html#abstracting-the-situation",
    "title": "Lesson 36 take-aways",
    "section": "Abstracting the situation",
    "text": "Abstracting the situation\nNow that we have the formula for the posterior for H1, we can generalize from the medical screening test to other situations.\n\nTwo mutually exclusive hypotheses: H1 & H2\nA prior for H1, indicating our level of belief in H1.\n\nThe rest of our belief goes to H2.\n\nTwo likelihoods, one for each hypothesis. These are:\n\np(obs | H1) and p(obs | H2)\n\nThe posterior for H1 gives our level of belief in H1 after making the observation."
  },
  {
    "objectID": "posts/Takeaways-36/index.html#example-the-ivory-billed-woodpecker-sighting",
    "href": "posts/Takeaways-36/index.html#example-the-ivory-billed-woodpecker-sighting",
    "title": "Lesson 36 take-aways",
    "section": "Example: The Ivory-billed woodpecker sighting",
    "text": "Example: The Ivory-billed woodpecker sighting\nThe Ivory-billed woodpecker has been thought to be extinct; the last known observation of one was in 1944. A new sighting in the Pearl River forest in Arkansas in 2004 became national news and excited efforts to confirm the sighting. These have not been successful and there is skepticism whether the reported 2004 sighting was correct. This is not an easy matter since the 2004 sighting was fleeting and the Ivory-billed woodpecker is very similar to a relatively common bird, the Pileated woodpecker.\n& \\ \\ \\ \\end{tabular}} \nHypothesis 1: The Ivory-billed is extinct.\nHypothesis 2: They really saw an Ivory-billed woodpecker.\nAn experiment was done using two different decoy birds. What did birdwatchers report for each of the different birds?\n\n\n\nObservation\nCode\nIvory-billed\nPileated\n\n\n\n\nShort & Dull\nA\n0.01\n0.60\n\n\nLong & Dull\nB\n0.10\n0.13\n\n\nShort & Glossy\nC\n0.04\n0.20\n\n\nLong & Glossy\nD\n0.60\n0.05\n\n\nShort & White Back\nE\n0.05\n0.01\n\n\nLong & White Back\nF\n0.20\n0.01\n\n\n\nEach of the numbers is a likelihood, e.g. 0.01 is the probability of observing Short & Dull given the bird was an Ivory-billed."
  }
]