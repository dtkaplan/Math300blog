[
  {
    "objectID": "posts/Takeaways-22/index.html",
    "href": "posts/Takeaways-22/index.html",
    "title": "Lesson 22 take-aways",
    "section": "",
    "text": "A DAG (directed acyclic graph) is a way of representing a hypothesis or belief or possibility to be considered of the causal connections among variables.\n\n\n\n\n\n\n\nFigure 1: A drawing of dag06, to use as an example.\n\n\n\n- DAGs consist of **nodes**---`a`, `b`, `c`, and `d` above, each representing a variable.\n- Any pair of nodes can be connected (or not!) by a **directed edge**. The edge means that the two variables are causally connected. The one-way flow of influence from one variable to the other is indicated by the direction.\n- The **A** in D**A**G stands for \"acyclic,\" meaning that there are no loops (cycles) in the flow.\n\nWe will use DAGs for three purposes:\n\nTo encapsulate hypotheses about causal connections in a variety of real-world settings.\nAs a means to reason about which covariates should and should not be included in a model. (Lesson 30)\nTo generate data from a source whose mechanism is exactly known. This will allow us to learn how and how much we can learn from data. We can then carry these lessons to real situations where the mechanism is only hypothetically known.\n\nThe software for our use of DAGs is built in to the {math300} package. This includes:\n\nAbout a dozen different DAGs providing examples of different sorts of causal connection. This includes those named dag01 through dag12, which are meant to be schematic (abstract). Other built-in DAGs encode hypotheses about causal connections in real-world systems.\nsample(dag01, size=10) generates data.\ndag_draw(dag01) draws a picture of the graph.\nprint(dag01) shows the formulas used in the generation of simulated data.\nIn Lesson 32 (about experimentation) you will see how to change a DAG using dag_intervene().\ndag_make(..formulas..) creates a DAG, but you won’t have much occasion to use it."
  },
  {
    "objectID": "posts/Takeaways-22/index.html#lesson-22",
    "href": "posts/Takeaways-22/index.html#lesson-22",
    "title": "Lesson 22 take-aways",
    "section": "Lesson 22",
    "text": "Lesson 22\n\nOften, the data we have at hand is a sample of specimens from a much larger population of objects (called a “population”). We store the sample’s observations in a data frame.\nEven when the data are a census, we often analyze them as if they are a sample from a population.\nA statistical point of view is that the data in our data frame are merely one sample collected at random from the population. Consequently, we imagine for the purpose of constructing many statistical methods, that there are infinitely many other samples that are equivalent to the one we have but which just happened not to be selected.\nSample statistics are numbers that we calculate from our sample. For us in Math 300Z, such statistics will typically be coefficients from a model (but we will also use some other sample statistics).\nOwing to the randomness involved in collecting our sample, we regard any sample statistic as a random draw from a population of sample statistics that could have been computed on other samples (as in (3)). That is, every sample statistic is a combination of “signal” and “noise.”\nSince our sample statistic includes noise, it is appropriate to quantify how much noise there is. Knowing this can, for example, enable us to decide whether two different samples come from different populations or not."
  },
  {
    "objectID": "posts/Takeaways-25/index.html",
    "href": "posts/Takeaways-25/index.html",
    "title": "Lesson 25 take-aways",
    "section": "",
    "text": "We contrasted the very different tasks of ….\n\nEstimation, which is mainly what we have been talking about until today. Estimation has to do with coefficients and effect sizes, understanding the relationships among values. Confidence intervals are a very important part of estimation methodology. Estimation focuses on “average” or “typical” or “central” patterns.\nPrediction which focuses on outcomes for individuals, and doesn’t benefit much from averaging. Graphically, a reasonable job drawing a prediction “interval” can be done from a plot of the data: look at the range of outcomes for the points near the given input levels.\n\nThe proper form for a prediction is to list all the possible outcomes, then assign a probability to each possible outcome.\n\nWhen the outcomes are numeric over a continuous range, then “probability” should be interpreted as “probability density” (a technical term from calculus) or, in more everyday language, a “relative probability.”\nA violin plot gives a reasonable representation of the relative probability of the different outcomes. But this is suitable only when the explanatory variables are categorical. For continuous, numerical explanatory variables, we will need another technique.\n\nEstimation of an effect size or a coefficient is accompanied by a confidence interval, which has a lower and an upper bound (the “confidence bounds”). The specific interval depends on the “confidence level,” but you won’t be mislead if you always use a 95% level, which is the convention.\n\nHistorical aside: The term “confidence” rather than “probability” was used to step around philosophical debates about the nature of probability. The confidence interval is not intended to be translated into a probability. Such a translation would look like this, “There is a 95% probability that the true value falls into the range covered by the interval.” This is what almost everybody does, even though it is not exactly legitimate. The “mathematically correct” formulation for translating a confidence interval into a probability is more subtle and not satisfying. (It is, “If I build confidence intervals according to the rules, then I can expect that in 95% of the situations being studied the true value will be within the confidence interval. But I can’t know for any one situation whether this is the case.”)\n\nPredictions are often formatted into an interval, so it’s tempting to think that the same principles (e.g., use a 95% level) are applicable. But useful predictions often have to do with extreme events. So levels like 80% are often appropriate. Strictly speaking, what’s presented as an interval ought really to be presented as a probability distribution (like a violin plot). Experts learn how to reverse engineer the probability distribution from the interval.\nNotwithstanding (4), the proper form for a prediction is to assign a probability to each possible outcome, as in (2). It is this form that is useful for decision making.\nWe looked at a case study about SAT scores"
  },
  {
    "objectID": "posts/Takeaways-24/index.html",
    "href": "posts/Takeaways-24/index.html",
    "title": "Lesson 24 take-aways",
    "section": "",
    "text": "The lm() model-training function is an entirely automatic machine for turning two inputs into model coefficients (which are stored in a “model object”).\n\nInput 1: A model specification in the form of a tilde-expression. Example: height ~ mother + sex\nInput 2: A data frame holding the variables used in the tilde espression. Example: Galton\n\n\n\nthe_model <- lm(height ~ mother + sex, data = Galton)\ncoefficients(the_model)\n\n(Intercept)      mother        sexM \n 41.4495235   0.3531371   5.1766949 \n\n\n\nModel coefficients are a convenient and historically important way to present a model. For fundamental, however, is the idea of a model function that takes as inputs the explanatory variables and returns a corresponding output to be interpreted as a value of the response variable.\nThere are other ways to represent model functions. The field of “machine learning” is largely about the variety of ways of representing model functions. At an elementary level, when there are few explanatory variables, a graph will do:\n\n\nmodel_plot(the_model, interval=\"confidence\")\n\n\n\n\nIn the above graph, a confidence interval has been added to indicate the precision that can justifiably be claimed for the model function. Any line that fits within the shaded region is a reasonable claim.\n\nOften, the interest is in measuring the size of the connection between an explanatory variable and the response variable. This is called the effect size. For instance, in the above graph there is obviously a connection between mother’s height and her children’s height. The size of the connection is the amount by which the child’s height would change if the mother’s height were magically altered. Here, the effect size is about 0.35. A two-inch gain in mother’s height would lead to a 0.70 inch gain in child’s height.\nEffect size is always “with respect to” a single, selected explanatory variable. Each explanatory variable has its own effect size. An effect size always means to change the selected variable while holding every other component of the system contant. In mathematical language, an effect size is a partial derivative (if the selected explanatory variable is quantitative) or a partial change (if the selected explanatory variable is categorical).\nWhen we talk about effect size, we are not necessarily implying any causal connection in the real world. Obviously, it’s absurd to think that changing a mother’s height (and nothing else!) would lead to any change in her children’s heights. The effect size describes how the model function output will change when the input is changed. The model function may or may not be faithful to the causal mechanisms in the world. For our mother/child height example, the model height ~ mother + sex does not capture the real-world genetics/environment determinants of child’s height.\nAlmost all the models we will construct in Math 300Z have only “linear” terms, so in every case the effect size with respect to a variable will be exactly the same as the coefficient on that variable. This is just to keep the accounting simple for us. (In class we showed a couple of models that have nonlinear terms, the most common of which are called “interactions,” but which also include curvy (rather than straight-line) functions. You won’t be responsible for this material.)"
  },
  {
    "objectID": "posts/Takeaways-23/index.html",
    "href": "posts/Takeaways-23/index.html",
    "title": "Lesson 23 take-aways",
    "section": "",
    "text": "lm() creates a model, which we can summarize in several ways. These numerical summaries—for instance, the coefficients reported by lm()—are called sample statistics. Mathematically, the sample statistics are exact, that is, the arithmetic is done correctly and everyone will get the same sample statistics when building the same model on the same data.\nStatistically, we take another point of view. We see the sample that we are working with as just one of the many samples that might have been collected. Imagine calculating a sample statistic on each of the many samples. The sample statistic would vary from one hypothetical sample to another. We call this sampling variation: note the “ing” ending on “sampling.”\nA confidence interval indicates the amount of sampling variation. It always consists of two numbers, the lower and the upper limits of the interval. Compute them from a model using conf_interval().\nThe width of a confidence interval is proportional to \\(1/\\sqrt{n}\\); the more data you have, the narrower will be the confidence interval.\nPrecision and accuracy are two different concepts. Accuracy refers to whether the measurement is “on target” or “close to reality.” Confidence intervals have nothing at all to say about accuracy. To get an accurate measurement of a coefficient, we need to choose the model that represents reality. Usually, we have no way to do this for sure. (With DAG simulations, we can read reality from the formulas, letting us match the model to the formula.)\nPrecision refers to the reliability or repeatability of the measurement. Confidence intervals are a good way to describe the precision of your measurements."
  },
  {
    "objectID": "posts/ChatGPT-and-R/index.html",
    "href": "posts/ChatGPT-and-R/index.html",
    "title": "Can ChatGPT help in interpreting or correcting R commands?",
    "section": "",
    "text": "The results are pretty good, to judge from a couple of initial trials. But, clearly, you cannot be completely oblivious about how R works to make sense of the results.\nChatGPT: This R code runs a linear regression model on the mtcars dataset, which is a built-in dataset in R containing information about 32 different cars. The model aims to predict the miles per gallon (mpg) of a car based on its horsepower (hp) and number of cylinders (cyl).\nHere’s a breakdown of what each part of the code means:\nTogether, the code lm(mpg ~ hp + cyl, data=mtcars) fits a linear regression model with mpg as the response variable and hp and cyl as the predictors, using the mtcars dataset. The output of the lm() function will include information about the coefficients of the model, as well as measures of model fit and statistical significance.\nChatGPT: This R code does not work because there is a syntax error in the formula provided to the lm() function. Specifically, there is an extra comma (,) after the cy variable, which causes R to interpret the formula incorrectly.*\nThe correct syntax for the lm() function requires that the formula is separated from the data argument using the data= parameter. Here’s the corrected code:\nThis code specifies the formula mpg ~ hp + cyl as before, and the data= parameter specifies that the data for the analysis should be taken from the mtcars dataset."
  },
  {
    "objectID": "posts/ChatGPT-and-R/index.html#comments",
    "href": "posts/ChatGPT-and-R/index.html#comments",
    "title": "Can ChatGPT help in interpreting or correcting R commands?",
    "section": "Comments",
    "text": "Comments\n\nChatGPT has a preferred set of words for describing computation. All the ones I see here are legitimate, but they don’t reflect the usage in the Math 300 textbook. Examples:\n\n“parameter”—We tend to use “argument” or “input,” as in “The data= input to lm().”\n“formula”—We use the phrase “tilde expression.” That’s because our course is mathematical in nature. In mathematics, “formula” refers to an arithmetical combination of quantities, as in \\(mx + b\\). If you want to construct a linear regression model along the lines of \\(mx + b\\), your tilde expression will look like y ~ 1 + x, assuming that y is the name of your response variable and x is the name of the explanatory variable.\n\nIn the 2nd example, ChatGPT didn’t mention that cy is not a variable in the mtcars data frame. It did, however, correct it to cyl in the example code."
  },
  {
    "objectID": "posts/Prediction-vs-estimation/index.html",
    "href": "posts/Prediction-vs-estimation/index.html",
    "title": "Case study: Prediction vs estimation",
    "section": "",
    "text": "This case study is motivated by the common misconception that “estimation” is more appropriate than “prediction.” The setting is gender preconceptions, specifically the claim that “boys are better at math than girls.” The data we looked at are math SAT scores for males and for females.\n\n\n\nAlthough published studies use actual individual-level data about SAT scores—for instance, a data frame with 1.5 million rows and the variables sex and score—those data are not available to us. Instead, I’ve reconstructed 20,000 rows of such raw data from the summary information provided by the College Board. The data frame is called Math_scores. (It is not available from R packages.)\nHere is an estimation: What is the difference in mean SAT scores for females and males?\n\nMath_scores |>\n  lm(score ~ sex, data=_) |>\n  conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    493.26869 \n    495.44183 \n    497.61497 \n  \n  \n    sexmale \n    19.33271 \n    22.40599 \n    25.47927 \n  \n\n\n\n\nThis model summary points to males having a higher average math SAT score than females, by about 20 points. The confidence interval has a width of about 6 points. In the actual data, which had 1.5 million rather than 20 thousand rows, the confidence interval would be much narrower, about 1 point.\nThere are two major problems with interpreting such results as meaning “boys are better at math than girls.”\n\nThe calculations for the confidence interval on the sexmale effect size are factually correct, but they are incomplete and misleading. The model represents an oversimplified DAG: \\(\\mathtt{sex} \\rightarrow \\mathtt{score}\\). In reality there are many other factors at work. One that we talked about in class is that a higher proportion of women take the SAT than of men. We know from other sources (and common sense) that the smaller the fraction of a group who take the SAT, the higher the average scores will be. We can estimate the effect size of SAT with respect to “fraction taking the test,” and it accounts for the observed difference in mean scores between males and females. To summarize, female test scores are on average lower than males because a more inclusive group of women take the test than of men. This is ironic: because women are more likely to go to college than men (a sign of academic success for women), the women’s scores are on average worse.\n\nMore germane to today’s topic of prediction …\n\nA statement like “boys are better at math than girls” tends to get translated to valuations of the skills of individuals. But if it is individuals we are thinking of, we should be framing the data analysis in terms of prediction rather than estimation. In this case, since sex is a categorical explanatory variable, the predictive probability distribution is well indicated by a violin plot, as in Figure 1.\n\n\ngf_jitter(score ~ sex, data = Math_scores, size=0.2, width = 0.15, height = 0, alpha = 0.3) %>% \n  gf_violin(color = NA, fill = \"blue\", alpha = 0.5) %>%\n  gf_labs(y = \"SAT math score\", x = \"\")\n\n\n\n\nFigure 1: The violin plot shows the probability distribution associated with each sex. They are almost identical.\n\n\n\n\nA reasonable interpretation of this graph: Knowing the sex of a person tells nothing about the person’s SAT score.\nAnother way to look at the problem is whether knowing the test score gives us insight into the sex of the person. We will study the modeling technique used to answer such a question in Lesson 33. But for now, all you need to know is that it’s possible to model the probability that a person is female given the test score.\n\nMath_scores <- Math_scores |> mutate(female = zero_one(sex, one=\"female\"))\nmodel <- glm(female ~ splines::ns(score,4), data = Math_scores, family=\"binomial\")\nmodel_plot(model, interval=\"none\", show_data=FALSE)\n\n\n\n\nExcept for scores above 650, the difference between the model values and 50% probability would be statistically undetectable unless averaging over groups of 400 or larger. At the very highest scores (around 750-800), there are about 2 males for every female. Even that hardly justifies applying the claim that “boys have high math scores than girls” to individuals."
  },
  {
    "objectID": "posts/Stat-and-non-stat-questions/index.html",
    "href": "posts/Stat-and-non-stat-questions/index.html",
    "title": "Statistical or not?",
    "section": "",
    "text": "Some questions and answers are statistical in nature, and others not. How can you know which is which?\nI’m thinking of starting my statistics class today by asking my students their ages. Next, a mathematical question: Is everyone in the class of the same age? That’s an easy question to answer, of course. The question is mathematical in the sense that it involves mathematical stuff—numbers—and there is a single, correct answer.\nI expect that not everyone will be the same age. That is, age varies among the members of the class. Now the question is: Why does age vary? When a statistical answer will be in the form, “Age varies because this other quantity or characteristic varies among the group.” This style of answer attempts to account for the variation in age by the variation in other things. For instance, the students don’t all have the same major, and different majors may place the stat course at different points in the students’ studies. Or, students entered the academy at different points in time. Presumably there would be less variation in age if we adjusted for each student’s age at matriculation.\nAnother question. Why are the statistics students typically about 20 years old? The answer that occurs to me is that almost all of the students are from the US and in the US students enter first grade around age 6 and continue through 12 years of primary and secondary education. Then they go to college and take stats in their second or third years. Few students have a gap between high-school graduation and college matriculation.\nThis answer is not statistical in nature. A statistical answers explains variation in one characteristic by appealing to the simultaneous variation in other characteristics. May statisticians may quarrel with my using words like “why” and “because.” But that is not central to the matter. What’s central to a question and answer being statistical in nature is that the question is about variation in a characteristic and the answer pairs that variation with variation in other characteristics.\n\nExample:\n\nNot a statistical question or answer: “Why is the sky blue?” Because blue light from the sun is scattered by small particles in the atmosphere.\nA statistical question and answer: “Why does the color of the sky change from time to time?” Because the weather is sometimes cloudy or foggy, and because the sun is sometimes high in the sky, sometimes rising or setting, and sometimes not visible at all."
  },
  {
    "objectID": "posts/Takeaways-26/index.html",
    "href": "posts/Takeaways-26/index.html",
    "title": "Lesson 26 take-aways",
    "section": "",
    "text": "In Lesson 25 we pointed out that the proper form for a prediction is a list of the potential outcomes, each matched to a probability of that outcome occurring. The probabilities across all outcomes (the “probability distribution”) should add to 1. (If the outcomes are quantitative and continuous, a probability density is used and “adding” is replaced with integration. But this is not important to us here.)\nWe are not going to be spending much time on the topic of probability distributions. This is a technical matter and we don’t have enough time to do it justice on a technical level. (We’ll come back to it, however, in Lessons 34 and 35, in a simple setting.)\nThere are some things you ought to learn, even if you don’t develop a mastery of probability.\n\nThere is a small set of mathematically defined probability distributions that are often used as models to organize prediction.\nThere is a probability logic called “Bayesian updating” that provides the means to update a probability distribution as new data come in. The relevant terms here are:\n\nprior distribution: our predictions before we see the new data.\nlikelihood: a mathematical model of the plausibility of each possible value for the new data observation.\nposterior distribution: the updated prediction upon seeing the new data.\nThe relationship among these three terms is mathematically simple: posterior is proportional to likelihood times prior. A demonstration, using self-driving cars as an example, is in this blog post.\n\n\nJust for general background, we looked at two of the distributions in the small set (3.i) of probability distributions:\n\nThe normal or bell-shaped distribution (available in R as dnorm()). This is the distribution that underlies the shorthand of using prediction intervals rather than probability distributions. Experts know exactly how to translate the prediction interval into the corresponding bell-shaped distribution.\nThe binomial distribution (available in R as dbinom()). The example we used for this had to do with hospital supplies. Suppose there are drugs or other medical material that is needed only rarely, for instance, having a 1% chance of use in any given hospital or clinic in the time before the material expires. If all hospitals and clinic keep it in stock, just in case, then roughly 99% will be wasted: an expensive proposition. However, if we can provide a central warehouse which can quickly send the material where and when it’s needed, we can dramatically reduce the amount needed to be in stock. For instance, if there are 500 hospitals and clinics, each of which has a 1% probability of needing a given drug, then we can virtually guarantee the availability of the drug with a small amount in stock. The binomial distribution provides the prediction of the amount of drug that will be called for across all hospitals and clinics. For the 500-hospital, 1% use case, the warehouse need only keep 15 doses in stock."
  },
  {
    "objectID": "posts/Takeaways-19/index.html",
    "href": "posts/Takeaways-19/index.html",
    "title": "Lesson 19 take-aways",
    "section": "",
    "text": "Measure variation using the “variance.” In R,\n\n\nGalton %>% summarize(vh = var(height))\n\n\nResponse variable will always be quantitative/numerical in a regression model.\nExpect graphics in our section to always have the response variable on the vertical axis."
  },
  {
    "objectID": "posts/Takeaways-19/index.html#administrative",
    "href": "posts/Takeaways-19/index.html#administrative",
    "title": "Lesson 19 take-aways",
    "section": "Administrative",
    "text": "Administrative\n\nDO THE DAILY WORKSHEET! You’ll fall behind if you don’t.\nBookmark this class site in your browser. That way you don’t have to detour through Teams.\nUse the Z-section project on posit.cloud. Bring in the worksheet using\n\n\nmath300::get_lesson_worksheet( LESSON NUMBER )"
  },
  {
    "objectID": "posts/Takeaways-21/index.html",
    "href": "posts/Takeaways-21/index.html",
    "title": "Lesson 21 take-aways",
    "section": "",
    "text": "Figure 1: A drawing of dag06, to use as an example.\n\n\n\n\nDAGs (“directed acyclic graphs”) have three properties, all of which are essential for representing causality.\n\nThey are “graphs” in the technical vocabulary of mathematics. That is, they consist of nodes and edges. Each edge connects two (and only two) of the nodes. In Figure 1 there are four nodes, labeled a, b, c, and d. Coincidentally, there are also four edges, one of which is c\\(\\longrightarrow\\)d.\nEvery edge is directed, that is, it points from a source node to a target node. Think of the directed edge as if it were a pipe carrying causal “fluid” from the source to the target. All the pipes in a graph are one-way only. In Figure 1 each of the edges is an arrow. The edge a\\(\\longrightarrow\\)b means that causal “fluid” can flow from a to b, but not from b to a.\nThere are no loops in the fluid flow (that is, there are no “cycles” of flow). In the name DAG, this is the meaning of the A: “acyclic,” meaning “no cycles.”\n\n\nExercise: Figure 1 is a DAG. By reversing the direction of a particular edge, you can modify the graph so that it has a cycle of flow. Which edge will do the job.\nExercise: Figure 1 does not have an edge connecting c and a. Add in that edge, directing it so that the new graph is still acyclic.\n\nStarting in Lesson 30, we will get into the ways to use DAGs in order to select explanatory variables that produce a model that is a faithful representation of the causal flows.\nFor the next few lessons, we will use DAGs for another purpose: to simulate data and make it easy to conduct random trials.\nOther than for teaching purposes (as in (3)), the role of DAGs in statistics and data analysis is to encode hypotheses about how elements of a system might be connected. Usually, you work with DAGs that reflect your and your colleague’s beliefs about how things are connected in the real world. Of course, believing a hypothesis does not make it true. Think of a DAG as a piece of fiction. Sometimes fiction is close to real life, and sometimes not. Both situations have their purposes for story-telling."
  },
  {
    "objectID": "posts/Takeaways-21/index.html#lesson-21",
    "href": "posts/Takeaways-21/index.html#lesson-21",
    "title": "Lesson 21 take-aways",
    "section": "Lesson 21",
    "text": "Lesson 21\n\nIt is helpful to think of any response variable as a combination of “signal” and “noise.” The signal reflects how the explanatory variables are related to the response. The noise is the unexplained part of the response variable. More precisely, the “noise” is that part that we do not care to explain in terms of relationships to other variables.\nThe idea that there is always noise in the response variable allows us to train models that do not go through every (or even any) data point. This enables us to claim that simple shapes of models can be good representations of relationships among variables.\nIn Math 300Z, with few exceptions we will work with models that have one or two explanatory variables. A nicer feature of such models is that we can draw a graphic of the model using just two or three aesthetics.\n\naes(y= ), the vertical axis: always will be assigned to the response variable. (This is a Math 300Z convention, and a good one, but not universal.)\naes(x= ), the horizontal axis: the first explanatory variable will be assigned to this.\naes(color= ) if there is a second explanatory variable, it will be assigned to color.\n\nThe model_plot() function will take care of all this assignment of variables to aesthetics.\nSince explanatory variables can be either categorical or quantitative, there are only a handful of model shapes we need to deal with. (The response variable is always quantitative.) These are enumerated in the Instructor’s notes for Lesson 21.\nAn excellent type of exam question would show you the graph of a model and ask you to identify whether there is one or two explanatory variables, and what type(s) it (they) are: quantitative or categorical.\nThere is also a role in statistics for models that have zero explanatory variables. The tilde expression for such models (letting y be the response variable) is y ~ 1. We have not yet discussed what the use is of such models.\nThe data used to build a regression model is called the training data. It is a data frame containing both the response variable and any explanatory variables. Once a model is built, we often run the rows of the training data through the model function. Doing this divides the response variable values into two components:\n\nThe signal, which is the model function output for each row of the training data. We call this the “model value.”\nThe noise, which for each row of the training data is the difference between the value of the response variable (the “response value”) and the “model value.” This difference—one for each row of the training data—is called the “residual”.\n\nRemember this simple relationship:\n\nresponse value = model value + residual\n\nOften we will need to measure how big these three things are. We use the variance as the measure of “how big.” The variance is only one of many possible ways to quantify “how big.” But it has the great advantage that\n\nvar(response value) = var(model value) + var(residual)"
  },
  {
    "objectID": "posts/Takeaways-28/index.html",
    "href": "posts/Takeaways-28/index.html",
    "title": "Lesson 28 take-aways",
    "section": "",
    "text": "Recall that the fundamental method in the second half of Math 300Z is linear regression. That involves identifying a response variable and one or more explanatory variables.1\n\nWe are adding some more detail to the vocabulary of explanatory variables in order to be able to talk about a very common situation:\n\nThere is one explanatory variable that is of primary interest to us.\nThere are also other explanatory variables that we might choose to include in the model specification, even though they aren’t of direct interest to us.\n\nThe generic name covariates is used for the explanatory variables in (b). A covariate is an ordinary variable, but it is always one that is being considered for inclusion in a model specification. We have a choice in this matter since the covariate is not of direct interest. Using this vocabulary gives us a way to say concisely, “The variable may be important in an explanatory role, but it is not of direct interest to us.”\nAn important critical thinking skill for evaluating a claim is to ask, “What other factors might be involved?” This is exactly the role for covariates. The statistics of covariates lets us examine the consequences of incorporating such “other factors.”\nCovariates often show up implicitly in models. For instance, the model life_expectancy ~ GDP has little explanatory power. Instead, we likely want to consider “per capita GDP, which takes into account the size of the population. In our modeling framework, this is quite like adding a covariatepopulation, that islife_expectancy ~ GDP + population`.\n\n\n\n\n\n\n\nAside: Intensive, extensive, and logarithms\n\n\n\nIn physical chemistry it’s common to distinguish between intensive and extensive variables. An extensive variable refers to the size (or “extent”) of the system: e.g. mass, volume, or energy. An intensive variable is not a measure of the “extent” of the system, but of a property like temperature or density. GDP is an extensive variable, as is population or land area. GDP is one way of describing how “big” the country is. Life expectancy does not tell you about the extent of the country, it’s an intensive quantity that refers to individuals.\nAs a modeling rule of thumb, whenever you are working with an extensive quantity, think seriously about using the logarithm of that quantity.\nDoing this in the context of life-expectancy and GDP (with population size as a covariate) would involve the model specification life_expectancy ~ log(GDP) + log(population). This model incorporates the “per capita” adjustment, since $(GDP/population) = (GDP) - ln(population). In other words,life_expectancy ~ log(GDP) + log(population)` is a generalization of the per-capita adjustment.\n\n\n\nIn Lesson 30, we will consider reasons why or why not to include covariates in a model. But in this Lesson 28, we want to make a technical point about comparing the coefficients between two nested models. An example of a pair of nested models is BFat ~ Hips and BFat ~ Hips + DThigh (to use the Anthro_F example). The models are nested because they both have the same response variable and the larger model includes all the explanatory variables in the smaller models.\nComparing the Hips coefficient for the two models shows something that surprises many people. Adding the covariate PThigh leads to a change in the coefficient on the explanatory variable Hips.\n\n\nlm(BFat ~ Hips, data=Anthro_F) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -53.00 \n    -44.00 \n    -35.00 \n  \n  \n    Hips \n    0.58 \n    0.68 \n    0.78 \n  \n\n\n\nlm(BFat ~ Hips + PThigh, data=Anthro_F) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -50.000 \n    -40.00 \n    -31.00 \n  \n  \n    Hips \n    0.083 \n    0.31 \n    0.54 \n  \n  \n    PThigh \n    0.240 \n    0.56 \n    0.87 \n  \n\n\n\n\nNotice the the confidence interval on the `Hips` coefficient from the smaller model doesn't overlap at all with the confidence interval on `Hips` from the larger model.\n\nMany people fallaciously believe that a situation as in (5) indicates some kind of deficiency in statistical methods. Such a person might ask, “How can I take a coefficient or confidence interval seriously if adding in another factor changes things completely?” In fact, the situation in (5) is entirely a mathematical phenomenon that depends on how closely related the explanatory variable (Hips) is to the covariate (PThigh).\n\nWe can measure how closely two variables are related using R2\n\nlm(Hips ~ PThigh, data=Anthro_F) |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    180 \n    1 \n    0.83 \n    880 \n    0.83 \n    0 \n    1 \n    180 \n  \n\n\n\n\nWhen the covariate is not closely related to the explanatory variable, the dramatic shift in the confidence interval of the explanatory variable is not seen. To illustrate, suppose we use Calf as the covariate rather than PThigh.\n\nlm(Hips ~ Calf, data=Anthro_F) |> R2() # not so closely related\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    180 \n    1 \n    0.2 \n    47 \n    0.2 \n    0 \n    1 \n    180 \n  \n\n\n\n\nThe high R2 indicates that the two variables are closely related. But with Calf as the covariate, the confidence interval on Hips is almost identical to what’s seen in the smaller model (see above) BFat ~ Hips.\n\nlm(BFat ~ Hips + Calf, data=Anthro_F) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -55.000 \n    -45.000 \n    -35.00 \n  \n  \n    Hips \n    0.540 \n    0.650 \n    0.76 \n  \n  \n    Calf \n    -0.091 \n    0.095 \n    0.28 \n  \n\n\n\n\nThe decision to include a covariate should rest on one’s understanding of the system being models. We’ve been using DAGs to describe such understanding. Lesson 30 will show how a DAG can be transformed into a decision about whether to include covariates.\n\n\n\n\nFootnotes\n\n\nTo be pedantic, I should say “zero or more explanatory variables.” That’s because y ~ 1 is also a regression model, even though it has no explanatory variables.↩︎"
  },
  {
    "objectID": "posts/Takeaways-20/index.html",
    "href": "posts/Takeaways-20/index.html",
    "title": "Lesson 20 take-aways",
    "section": "",
    "text": "Measure variation using the “variance.” In R,\n\n\n\nGalton %>% summarize(vh = var(height))\n\n\n\n \n  \n    vh \n  \n \n\n  \n    12.8 \n  \n\n\n\n\n\nInterpretation: The heights of the people in the Galton data frame vary. The amount of this variability is the variance: 12.8 square-inches. In less strange units, the standard deviation is \\(\\sqrt{12.8\\ \\text{square-inches}} = 3.6\\) inches\n\n\nThe most common action for the rest of this course will be to create a model and summarize it.\nExample: lm(height ~ mother + father, data=Galton)\n\nheight ~ mother + father is a tilde expression that specifies the roles of variables in the model. height is the response variable. mother and father are the explanatory variables.\ndata = Galton tells lm() to use the Galton data frame to construct the model corresponding to the tilde expression.\n\nExample: Summarizing functions R2() and conf_interval()\n\n\n\nlm(height ~ mother + father, data=Galton) |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    898 \n    2 \n    0.109 \n    54.7 \n    0.107 \n    0 \n    2 \n    895 \n  \n\n\n\n\nInterpretation: mother and father jointly explain about 10% of the variance in the height of their adult children.\n\nlm(height ~ mother + father, data=Galton) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    13.900 \n    22.300 \n    30.80 \n  \n  \n    mother \n    0.187 \n    0.283 \n    0.38 \n  \n  \n    father \n    0.290 \n    0.380 \n    0.47 \n  \n\n\n\n\n\nInterpretation: The model equation for a person’s height in inches is, according to this model:\n\\[\\text{person's height} = 22.3 + 0.283\\ \\mathtt{mother} + 0.380\\ \\mathtt{father}\\]\n\n\nRegression models (such as those built by lm()) quantify how to account-for/explain the variation in the response variable in terms of the variation in the explanatory variables."
  },
  {
    "objectID": "posts/Takeaways-20/index.html#new-topics",
    "href": "posts/Takeaways-20/index.html#new-topics",
    "title": "Lesson 20 take-aways",
    "section": "New topics",
    "text": "New topics\n\nAccount-for/explain. Often when we use the word “explain” we mean to suggest a causal connection. For instance, this randomized clinical trial established that a particular blood-pressure drug leads to lower blood pressure, that is, it causes the blood pressure to go down.\nWhen we say that “A causes B,” we don’t necessarily mean that A is the complete and total explanation for B. More often, we mean that “A contributes in some way to the value of B.” For instance, “high blood pressure increases mortality” does not mean that high blood pressure is the sole determinant of mortality. Instead, it means that high blood pressure contributes to an increased risk of mortality.\nA DAG (Directed acyclic graph—unnecessarily intimidating name!) is a way of encoding a hypothesis of what causes what in a system. We discussed the system involving treating a battlefield casualty with a tourniquet. (Link to in-class activity.) The system—a “system” is a collection of components—involved USE of a tourniquet, SEVERITY of injury, staying alive long enough for ADMISSION to hospital, and post-hospital SURVIVAL. Common sense suggests some causal connections:\n\nSEVERITY \\(\\longrightarrow\\) USE, since tourniquets are used for severe injuries involving bleeding from a limb.\nSEVERITY \\(\\longrightarrow\\) ADMISSION. The more severe the injury, the less likely the soldier is to reach the hospital.\nADMISSION \\(\\longrightarrow\\) SURVIVAL. There would be little point to having hospitals if they didn’t contribute to survival!\nSEVERITY \\(\\longrightarrow\\) SURVIVAL. Even if reaching the hospital, the soldier’s survival is in part determined by the SEVERITY of the injury.\n\nOther link were more hypothetical:\n\nUSE \\(\\longrightarrow\\) ADMISSION\n\n\nUSE \\(\\longrightarrow\\) SURVIVAL\n\nA DAG describes the hypothesized causal links among all the system components.\n\n\n\n\n\n\n\n\n\n\n\n\nMore detail\n\n\n\nThe clinical study looking at the relationship between tourniquet use and survival included only those patients admitted to the hospital, not the ones who died before admission. Their data did not demonstration a relationship between USE and SURVIVAL. That is to say, the hypothesized DAG as a result of the study was this:\n\n\n\n\n\nIn this second DAG, there is no causal connection between USE and SURVIVAL.\nA critique of the clinical trial is that it didn’t consider ADMISSION, that is, both those soldiers who reached the hospital and those who didn’t. Consequently, even if there were no direct link between USE and SURVIVAL, a more complete DAG includes a possible indirect path, as in this third DAG:\n\n\n\n\n\nThe path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL could not be demonstrated by the (imperfect) clinical study, even though it might well exist. Any decision about the efficacy of tourniquets would need to consider the possible path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL.\nHere’s a paradox: Effective use of tourniquets increases hospital mortality rates. Explain why this says something positive about tourniquet use."
  },
  {
    "objectID": "posts/Takeaways-20/index.html#not-gotten-to-in-class",
    "href": "posts/Takeaways-20/index.html#not-gotten-to-in-class",
    "title": "Lesson 20 take-aways",
    "section": "NOT gotten to in class",
    "text": "NOT gotten to in class\n\nUse of sample() and dag_draw() with DAGs.\nWhat is a “random trial”\nHow (and why) to automate replication of random trials.\n\nYou can learn these things from the text and the worksheet for Lesson 20."
  },
  {
    "objectID": "posts/Takeaways-27/index.html",
    "href": "posts/Takeaways-27/index.html",
    "title": "Lesson 27 take-aways",
    "section": "",
    "text": "In our original plan for Math 300Z, Lesson 27 was to be the first of two exam days. That plan changed; we dropped the first exam, keeping only the second. Consequently, we passed directly from Lesson 26 to Lesson 28."
  },
  {
    "objectID": "posts/Self-driving-safety/index.html",
    "href": "posts/Self-driving-safety/index.html",
    "title": "Bayesian updating for self-driving cars",
    "section": "",
    "text": "Suppose ordinary new-ish cars have a mean distance between accidents of 200,000 miles. (This is roughly consistent with the Internet factoid that the probability of a car accident in 1000 miles is 1/366.)\nWhat might a skeptical regulator reasonably believe about newly released self-driving cars?\n“These things are crazy. Very likely to get in an accident.”\n“Perhaps a 1% chance that they are safer than regular cars.”\nIt takes some math to translate these views into a prediction about the actual mean time between accidents. We teach that in other courses. But here is a graph of such a probability function.\nThis is called a “prior” probability distribution: our starting point.\nNow data comes in. Each day we get a report from all the self-driving cars:\nBased on these data, we update our beliefs to produce a “posterior” probability distribution. The Bayesian updating rule is:\nPosterior(m) \\(\\propto\\) Likelihood(m, observations) \\(\\times\\) prior(m)\nSuppose the observation is: car crashed at 23,241 miles. A plausible **likelihood function* is based on simple probability, not the prior.\nThere is also a likelihood function for a car having driven 30,000 miles without an accident."
  },
  {
    "objectID": "posts/Self-driving-safety/index.html#now-for-the-data",
    "href": "posts/Self-driving-safety/index.html#now-for-the-data",
    "title": "Bayesian updating for self-driving cars",
    "section": "Now for the data",
    "text": "Now for the data\nThe New York Times report indicates 400 crashes out of 360,000 self-driving cars. Suppose we observe these data for Tesla\n95 cars have driven 20K miles without an accident; 5 cars had accidents respectively at 1K, 4K, 8K, 12K, 16K\n\n\nCode\nlog_likelihood_observed <- function(m) {\n  ( log10(nocrash_likelihood(m, 20000))*95) +\n    log10(crash_likelihood(m, 1000)) +\n    log10(crash_likelihood(m, 4000)) +\n    log10(crash_likelihood(m, 8000)) +\n    log10(crash_likelihood(m, 12000)) +\n    log10(crash_likelihood(m, 16000)) \n}\n\n\n\n\nCode\nlog_posterior <- function(m) log_likelihood_observed(m) + log10(belief(m))\nslice_plot(10^((log_posterior(m)+38)) ~ m, bounds(m=2000:500000), npts=1001) +\n  geom_vline(xintercept=200000, color=\"red\") +\n  ylab(\"Posterior: relative probability\") + xlab(\"Average miles per accident\")\n\n\n\n\n\nThe posterior indicates that data on 100 cars, 5 of which had accidents before 20,000 miles, places most belief that the self-driving cars are safer than regular cars: about twice as safe.\nThis is counter-intuitive, since we have no data on cars that drove farther than 20,000 miles. But would be hard to get 95 out of 100 cars to 20,000 without an accident if the mean distance betwee accidents were even 100,000 miles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300 Blog",
    "section": "",
    "text": "Trade deficits and the Opium Wars\n\n\n\n\n\n\n\np-values\n\n\nsignificance\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nInstructor’s Orientation\n\n\n\n\n\n\n\npopulation\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nInstructor’s Orientation\n\n\n\n\n\n\n\np-values\n\n\nsignificance\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nStats Vocabulary\n\n\n\n\n\n\n\nreduction\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nHypothesis testing and poker\n\n\n\n\n\n\n\nHypothesis testing\n\n\nNull hypothesis\n\n\np-value\n\n\nshuffling\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nA Bayes interpretation of Hypothesis testing\n\n\n\n\n\n\n\nlikelihood\n\n\nprior\n\n\nposterior\n\n\nlikelihood ratio\n\n\nodds\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 36 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nlikelihood\n\n\nnull hypothesis\n\n\nalternative hypothesis\n\n\nprior\n\n\nposterior\n\n\np-value\n\n\nconfidence intervals\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nNamed tests from Stat 101\n\n\n\n\n\n\n\none- and two-sample tests\n\n\np-test\n\n\nt-test\n\n\nANOVA\n\n\nsimple regression\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nA bad graph for medical screening\n\n\n\n\n\n\n\nprevalence\n\n\nsensitivity\n\n\nspecificity\n\n\nprior\n\n\nposterior\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nCompeting two hypotheses\n\n\n\n\n\n\n\nlikelihood\n\n\nprior\n\n\nposterior\n\n\nlikelihood ratio\n\n\nodds\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 35 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nhypothesis (definition)\n\n\nsensitivity/specificity\n\n\nprevalence\n\n\nprior\n\n\nposterior\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 34 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nclassifier\n\n\nlogistic regression\n\n\nthreshold\n\n\nloss function\n\n\nfalse-positive/false-negative\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 32 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nexperiment\n\n\nblocking\n\n\nrandom assignment\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 28 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\ncovariate\n\n\nadjustment\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 27 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 26 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nprediction\n\n\nprobability distribution\n\n\nbayesian updating\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 25 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nprediction\n\n\nestimation\n\n\nintervals\n\n\nprobability distribution\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nCase study: Prediction vs estimation\n\n\n\n\n\n\n\nSAT\n\n\nprediction\n\n\nestimation\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nBayesian updating for self-driving cars\n\n\n\n\n\n\n\nself-driving cars\n\n\nprediction\n\n\nbelief\n\n\nBayes\n\n\nprobability distribution\n\n\nlikelihood\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 24 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\neffect size\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 23 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nconfidence intervals\n\n\nprecision\n\n\naccuracy\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 22 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nsignal and noise\n\n\nsampling variability\n\n\nsampling variance\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 21 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nsignal and noise\n\n\nlinear model shapes\n\n\nmodel function\n\n\nmodel value\n\n\nresidual\n\n\nresponse value\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 20 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nvariance\n\n\nDAGs\n\n\ncausality\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nLesson 19 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nvariance\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nStatistical or not?\n\n\n\n\n\n\n\nvariation\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n  \n\n\n\n\nCan ChatGPT help in interpreting or correcting R commands?\n\n\n\n\n\n\n\nR\n\n\nChatGPT\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Notes, comments, tools, and suggestions relating to Math 300Z, the prototype run of the core statistics course at USAFA.\nThis blog contains Danny Kaplan’s personal opinions. Nothing on this blog necessarily reflects the views or policies of the US Air Force Academy, the Department of Defense, or the US government generally."
  },
  {
    "objectID": "posts/Takeaways-32/index.html",
    "href": "posts/Takeaways-32/index.html",
    "title": "Lesson 32 take-aways",
    "section": "",
    "text": "Statisticians divide data into two broad categories, depending on how it was collected.\n\nObservational data, where the mechanism behind the data operates in its natural way.\nExperimental data, where the investigator intervenes to change the mechanism in order to simplify the mechanism or reduce possible ambiguity in the data analysis.\n\nFigure 1 is an example of system in which treatment is the explanatory variable of interest and outcome is the response.\n\n\n\nCode\nNatural <- dag_make(\n  covar ~ exo(),\n  unknown ~ exo(),\n  treatment ~ covar + unknown,\n  outcome ~ treatment + covar + unknown\n)\nExpt <- dag_make(\n  investigator ~ exo(),\n  covar ~ exo(),\n  unknown ~ exo(),\n  treatment ~ investigator,\n  outcome ~ treatment + covar + unknown\n)\nset.seed(101); dag_draw(Natural, vertex.label.cex=1)\nset.seed(101); dag_draw(Expt, vertex.label.cex=1)\n\n\n\n\n\n\n\n\n(a) System as it operates naturally.\n\n\n\n\n\n\n\n(b) System with investigator intervention.\n\n\n\n\nFigure 1: The investigator is interested in studying the direct effect of treatment on outcome. A natural system compared to another system in which the investigator has intervened to simplify the system with respect to data analysis.\n\n\n\n\nThe data analysis techniques we have been using apply equally well to both observational and experimental data, although sometimes simpler techniques can be used in working with experimental data because the data-generation mechanism is simpler.\n\nIn the mechanism of Figure 1(a), studying the direct relationship treatment \\(\\rightarrow\\) outcome requires adjusting for the confounders covar and unknown which lie on back-door pathways between treatment and outcome. But without data on unknown, this is impossible.\nIn the mechanism of Figure 1(b), there are no backdoor pathways from treatment to outcome, so the direct link can be studied without adjustments for covariates, known or unknown. In other words, analysis is simple and unambiguous despite the unknown factors.\n\nThe trick to disconnecting treatment from the causal influences of covar or unknown is for the investigator to assign the values of treatment. Consider a medical investigation where treatment is whether a patient takes aspirin, which might be influenced by the overall health of the patient, some aspects of which might be measured (covar) and some not (unknown). In a medical experiment (often called a “clinical trial”), the patient would be assigned to either of two treatments: take aspirin or not. (In good experiments, “not” might mean taking a placebo, to be explained below.)\nThere’s a potential confusion introduced by using the word “treatment” for the experimental variable. In everyday speech, “treatment” refers to giving a drug or surgery or some other active intervention. In statistical vocabulary, a “treatment” can be “doing nothing.” Keep in mind that our data analysis techniques are always about relating the variation in an explanatory variable (and, as we choose, covariates) to the variation in the response. This means that the investigator must create variation in the treatment variable. This is done by giving different types of treatment to the different experimental units (e.g. patients).\nCare should be taken in the investigator’s choice of which treatment should be given to which experimental unit. The key consideration is to avoid re-creating a link covar \\(\\rightarrow\\) investigator or unknown \\(\\rightarrow\\) investigator. A simple and effective safeguard is to use random assignment, that is, the value of investigator is set at random. Experience demonstrates that even when an investigator thinks his choice of treatment is random, there is some influence from outside factors. Example: The investigator prefers to assign placebo to those patients who are thought to be more likely to have a bad reaction. Another example: It’s convenient for record-keeping purposes to assign the intervention to those in the first months of a study and reserve the placebo for those in the later months. However, overall conditions might be different over time, e.g. winter turns into spring, medical suppliers or personnel change.\nConsequently, it’s best for the investigator to let the random assignment be done by a computer random-number generator. This is an important part of experimental method and ought to be reported in any write-up of the experimental results.\nAnother situation that can create unintended backdoor pathways between treatment and outcome is when knowledge of the treatment can alter behavior by the experimental unit (patient) or the person who measures the outcome. For instance, in a vaccine trial, patients who know they have received a vaccine might be more willing to expose themselves, as with people returning to public venues after getting the COVID vaccine. Similarly, a doctor diagnosing a patient might be more inclined to look for other causes than COVID if the doctor knows that the patient was vaccinated.\nTwo methods are widely used to avoid such situations. The patients can be, in many circumstances, made unaware of whether they received the active treatment. A study that does this is called “blinded.” One way to accomplish this is to use a “placebo” or other sham treatment as the alternative to the active treatment. In a drug trial, for instance, all patients are given pills to take, but the patient doesn’t know whether the pill is an active treatment or not. In a vaccine trial, all patients are given the injection so the patient doesn’t know if they received the vaccine or an inert substance.\nOften it is possible to arrange things so that the investigator doesn’t know whether treatment or placebo was given. (This information is hidden from the investigator until all the data has been collected, and revealed only in the data analysis phase.) Studies which keep the investigator ignorant in this way are called “double blinded,” since usually the patient is also kept ignorant of the treatment type.\n\nGOING FURTHER. Sometimes it’s impossible to keep the treatment from being affected by other factors. For instance, patients might not comply with the instructions for taking a drug. Or, suppose treatment is assignment to a training program. Not all people so assigned might be active participants in the program. The non-compliance can, potentially, re-create the back-door pathways between treatment and outcome, as in Figure 2.\n\n\nCode\nIntent <- dag_make(\n  investigator ~ exo(),\n  covar ~ exo(),\n  unknown ~ exo(),\n  treatment ~ investigator + covar + unknown,\n  outcome ~ treatment + covar + unknown\n)\nset.seed(101); dag_draw(Intent, vertex.label.cex=1)\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nThere are two methods available to deal with such situations.\n\nIntent to treat involves a very easy change in the data analysis: instead of modeling outcome ~ treatment, model outcome ~ investigator. In other words, look toward the intent of the investigator rather than the actual value of treatment. Notice in Figure 2 that there are no back-door pathways between investigator and outcome. The model outcome ~ investigator therefore avoids confounding. This is not a panacea, since in the case of non-compliance, the effect size of investigator on outcome will tend to be weaker than the direct effect size of treatment on outcome.\nA more mathematically sophisticated analysis goes by the name instrumental variables, a technique especially esteemed in econometrics. Remarkably, the intrumental variable technique is used even in a non-experimental setting, where the investigator has not intervened in the system. In such cases, the technique relies on identifying a measurable variable, called an “instrument,” that takes the place of investigator. The theory of instrumental variables is too difficult to summarize here. The term “natural experiment” is often used to describe a study undertaken with instrumental variables. But such studies are not “experiments” in the sense used above, which involves an investigator intervening in the causal system."
  },
  {
    "objectID": "posts/Takeaways-35/index.html",
    "href": "posts/Takeaways-35/index.html",
    "title": "Lesson 35 take-aways",
    "section": "",
    "text": "Define a hypothesis (in a statistical sense) as “a statement that might or might not be true.” This is really just a wordy way of saying, “a statement,” but we want to emphasize that the statement doesn’t have to be true. Examples of types of hypotheses we’ve encountered in this course:\n\nModels\nDAGs\n\nBut here we are interested in statments about the world, such as “the patient has congestive heart failure,” or (in an utterly different setting) “the aircraft is a threat.”\nDefine an argument (in the sense of critical thinking) as “reasoning to demonstrate the truth/falsity/credibility of a hypothesis.” The conclusion from an argument is a statement about the truth/falsity or credibility of a hypothesis.\nIn science, there are two main forms of argument:\n\nDeductive reasoning where one starts with premises, that is, statements that are assumed to be true, and modify those statements using a mechanism that is generally accepted as correct. Example: Algebra where you start with an equation and then apply operations to both sides (e.g. multiply both sides by x) to create a new equation. Example: DAGs, where we learned the machinary to identify covariates that can block or open causal paths, and used this to decide which covariates ought be included in or excluded from a model.\nAlmost all of your math and science training has featured deductive reasoning, and you know how to check an argument to determine if you should believe the conclusion: a) check the premises for truth, b) check the mechanical steps of going from one statement to another.\nInductive reasoning where one starts with observations/data and moves to a conclusion about one or more hypotheses. (Inductive arguments generally contain premises such as models and DAGs and deductive reasoning.)\n\nThe setting for inductive reasoning we will work with involves two mutually exclusive hypotheses relevant to the question of interest, then using observations to reach a conclusion about the respective credibility of the hypotheses. Examples of mutually exclusive hypotheses: the patient has the disease versus the patient does not have the disease; the aircraft is a threat vs it is not a threat. (There are also important settings with multiple hypotheses, and the methods we’ll discuss are relevant to those settings, with more complicated book-keeping. For us, the two-hypothesis setting will suffice. Happily, many real-world situations correspond to the two-hypothesis deductive reasoning setting.)\nThe two-hypothesis setting is directly relevant to decision-making, insofar as heightened credibility in one of the hypotheses often corresponds to taking a particular action, for instance, medical treatment for a patient who has the disease.\n\nThe framework for deductive reasoning we consider first is exemplified by medical screening tests, such as testing for colon, breast, and prostate cancers. The two contending hypotheses will be labelled “D” and “H,” which you can think of as “disease” and “healthy” respectively.\n\nIn this framework, much preliminary work (see Lesson 34) has been done before the test is applied to any patient. For instance, researchers have developed a testing method which might involve measuring antigen levels in blood or reading X-rays. The testing method amounts to a machine which takes the measurement of antigen levels or the X-ray and translates it into one of two results, a positive test result or a negative test result. Such a machine is called a classifier.\n\nOnce the classifier has been developed, applying it to a patient is an essentially automatic procedure that takes the patient as “input” and produces the test result (“positive” or “negative”) as output.\nThe question for the inductive reasoner is how to decide whether any given classifier accomplishes what you want, how to frame “what you want,” and how to interpret the result of the test. For instance, studies have shown that physicians and patients are likely to read something into the test result that is not at all justified. For instance, the women who test positive on a mammogram, and their physicians, often conclude that this is a definitive diagnosis, which is not at all the case.\n\nIn Lesson 34, we considered the perspective of the researchers who are developing a test. Refer to those notes for a refresher. The upshot of that lesson is that the proper way to describe the quality of performance of the test is with two numbers, each of which is a probability. These numbers are the sensitivity and specificity of the test. It’s important to understand that it takes two numbers to describe the test performance. Many people wrongly assume that only one number is required, the test accuracy. In reality, accuracy is a naive and incomplete concept and should not be used.\nThe patient comes in, the test is performed, and a result is found: positive or negative. Now we need to translate that result into the credibility level to be assigned to each of the hypotheses D and H. This is done by using ancillary information to calculate a probability level for hypothesis D. The probability level for hypothesis H will be the complement of that for D. For instance, if D has a 75% probability, H has a 25% probability.\nWhat is the ancillary information needed to translate the positive-or-negative test result into a probability for D? In the setting of disease, this information is the prevalence of the disease. Prevalence is the fraction of the population who have the disease. (Which population is relevant to any given patient is a question of some importance, but for us, the prevalence is just a number between zero and 100%)\nThe three numbers needed to translate the positive/negative test result into a probability for the D hypotheses are:\n\nThe sensitivity\nThe specificity\nThe prevalence\n\n\nThis diagram shows how to put these three numbers together to find the probability of hypothesis D given a positive test using a sensitivity of 90%, specificity of 80%, and prevalence of 10%.\n\n\n\n\n\nThe sensitivity is the proportion of the left (D) bar where the test is +. The specificity is the proportion of the right (H) bar where the test is -.\nThe prevalence is the proportion of the whole that is in the left (D) bar.\nThe probability of D given a + test is 9/(17+9)\nThe probability of H given a - test is 1/(73+1)"
  },
  {
    "objectID": "posts/Takeaways-34/index.html",
    "href": "posts/Takeaways-34/index.html",
    "title": "Lesson 34 take-aways",
    "section": "",
    "text": "We talked about “classifiers,” a mechanism aimed to distinguish people with a given condition (e.g. prospective congestive heart failure, or colon cancer, …) from people without that conditions.\nThe classifier takes measurements from a person and returns either a positive (\\(\\mathbb P\\)) or a negative (\\(\\mathbb N\\)) result.\nThe two numbers that summarize the performance of the test are:\n\nSensitivity: The fraction of people with the condition who test \\(\\mathbb P\\), which is the correct result for such people.\nSpecificity: The fraction of people without the condition to test \\(\\mathbb N\\), which is the correct result for this other group of people.\n\nAn incorrect result is called “false,” a correct result is “true.”\n\nFor people who do not have the condition, a \\(\\mathbb P\\) result is false. Such people are called “false positives.” A good classifier keeps the rate of false positives low because such people may receive treatment which is not needed.\nFor people who have the condition, a \\(\\mathbb N\\) result is false. Such people are called “false negatives.” A good classifier keeps the rate of false negatives low because such people do not receive the treatment which is called for.\n\nBy changing a number called the “threshold,” the false negative rate can be improved, but at the cost of a higher false positive rate. And vice versa. That is, there is always a trade-off between false positives and false negatives.\nWe resolve the trade-off by looking at the overall loss due to the mistaken classifications. The loss for a false positive is generally different (often, less) than the loss for a false negative. To calculate the overall loss, multiply the false-positive rate and the false-negative rate by their respective losses, and add. Choose the threshold that minimizes the overall loss."
  },
  {
    "objectID": "posts/Takeaways-34/index.html#more-detail",
    "href": "posts/Takeaways-34/index.html#more-detail",
    "title": "Lesson 34 take-aways",
    "section": "More detail",
    "text": "More detail\n\nA classifier is a machine that, based on measurements of some sort assigns a categorical level to the object the measurements came from. We can make this less abstract by talking about classifiers in the context of medical screening. There, the “object” is a patient; the possible categorical levels are \\(\\mathbb{P}\\) and \\(\\mathbb{N}\\), meaning a “positive” test result or a “negative” test result.\nIn this Lesson, we demonstrated how to build a classifier. The classifier takes the form of a statistical model fitted to training data.\nSkipping, for the moment, to the use of a classifier that’s already been built, this is a matter of evaluating the statistical model at the inputs relevant to the patient, then applying a threshold to the model output to make the choice between \\(\\mathbb{P}\\) and \\(\\mathbb{N}\\). Interpreting the meaning of \\(\\mathbb{P}\\) and \\(\\mathbb{N}\\) requires that we know about:\n\nThe performance of the classifier, which is summarized by two numbers, the sensitivity and the specificity. Using these two numbers (along with a third number, called the prevalence) is the topic of Lesson 35.\nYou may hear people talk about the “accuracy” of a classifier. We’ll show how, in the process of building the classifier, to calculate the accuracy, specificity, and sensitivity. Note that the “accuracy” is inadequate for describing the classifier.\n\nThere are three steps to building a classifier:\n\nAssembling training data. This is a very difficult and drawn-out process, usually done in a clinical setting and involving hundreds or thousands of test subjects.\n\nIn our example, we used the math300::Framingham data frame, which is the product of a lot of work by many people.\nThe training data includes one or more measurements made on each of the test subjects. In Framingham such measurements included age, sex, smoking status, whether the subject was taking medicine for high blood pressure, BMI, and so on. In the well known screening tests (e.g. for breast cancer, prostate cancer, colon cancer) the measurement is often the concentration in blood or stool of a particular antigen.\nThe training data must include an outcome variable. With Framingham, the outcome is TenYearCHD which records whether or not the subject developed congestive heart failure (CHD) in the ten-year follow up period (that is, the ten years after the measurements in (b) were made. The outcome variable is a zero-one variable.\n\nBuilding a statistical model of the outcome variable as a function of one or more of the measurements in (b). The role of this model, once built, is to convert the measurements in (b) into a score. Often in such model building, care is taken to identify the measurements in (b) that give the best “performance.” Performance refers to producing a wide range of model output values that correlate well with the outcome variable.\nThe final step is to establish a threshold. This is a number, in the units of the score. When the score is above this threshold, the classifier produces a \\(\\mathbb P\\) result. A score below the threshold means the result is \\(\\mathbb N\\).\n\nIn class, we used logistic regression to build the statistical model (section 4.ii). Then we evaluated the model on the training data to produce a score for each subject in the training data. Each of these subjects either developed CHD (in the ten-year follow-up) or did not. The fraction of subjects who developed CHD is called the “training prevalence.” In the example constructed for the table below (in (5)), there are 1000 subjects altogether, of whom 50 +150=200 had CHD as the outcome. The training prevalence is the fraction of the whole who had the CHD outcome: 200/1000 = 20% in the example below.\nThe process of choosing a threshold (section 4.iii) is an important part of the decision-making guided by the test. It works like this:\n\nChoose a candidate threshold. Apply this to each of the scores in the model-evaluated training data to produce a new column which we can call the output of the candidate test. This will have entries that are \\(\\mathbb P\\) or \\(\\mathbb N\\) depending on whether the score is above the candidate threshold.\nTally up the table using the outcome (disease or not) and the test result (\\(\\mathbb P\\) or \\(\\mathbb N\\)). This will produce a table like this:\n\n\n\n\n\npatient outcome\ntest output\ncount\ndescription\n\n\n\n\nDisease.\n\\(\\mathbb P\\)\n150.\n“true positive”\n\n\nDisease.\n\\(\\mathbb N\\)\n50.\n“false negative”\n\n\nHealthy\n\\(\\mathbb P\\)\n75.\n“false positive”\n\n\nHealthy.\n\\(\\mathbb N\\)\n725.\n“true negative”\n\n\n\n\nFrom the above table, you can easily compute three measurements of the performance of the test:\n\n\nsensitivity: The fraction of the diseased subjects who tested positive. In the example, that’s 150/(150+50) = 75%.\nspecificity: The fraction of the healthy subjects who had a negative test result. Here that’s 725/(725+75) = 90.6%.\n\nNote that neither the sensitivity nor the specificity reflect the training prevalence, since each is calculated within either the disease group or the healthy group.\n\naccuracy: The “accuracy” is the fraction of all subjects who received a correct test result, that is, one that matches the patient outcome. Here that’s the fraction with “true” in the description, that is, (150+725)/1000 = 87.5%. The problem with “accuracy” is that it depends strongly on the training prevalence.\n\n\nThe training prevalence is an artifact of the data collection process and is typically much higher than the prevalence in the overall population, that is, the population prevalence.\nRemember that we are still in the process of evaluating the merits of the candidate threshold used to construct the table above. Now we want to re-arrange the counts in that table to reflect the population prevalence.\n\nThe training prevalence was 20%. Suppose that the prevalence in the population to whom the test is targeted is 5%.\nConstruct a new table of counts where the sensitivity and specificity are exactly as found in the table from the training-data/candidate-threshold but where the prevalence is the desired 5%. We can accomplish this by arithmetically increasing the numbers in the healthy category so that there are altogether 4000 people in the table. (200 diseased out of 4000 total is a prevalence of 5%.) In increasing the number of healthy people, we need to be careful to keep the specificity at the level found from the training data: 90.6. That is, we will change the healthy numbers so that there are 3800 healthy people (giving 4000 total, including the diseased) of whom 90.6% got a \\(\\mathbb N\\) result. This gives us 38000.906 = 3443 in the last row and 3800(1-0.906) = 357 in the second to last row.\n\n\nThe adjusted table that reflects the population prevalence (as opposed to the training prevalence) is:\n\n\n\npatient outcome\ntest output\ncount\ndescription\n\n\n\n\nDisease\n\\(\\mathbb P\\)\n150\n“true positive”\n\n\nDisease\n\\(\\mathbb N\\)\n50\n“false negative”\n\n\nHealthy\n\\(\\mathbb P\\)\n357\n“false positive”\n\n\nHealthy\n\\(\\mathbb N\\)\n3443\n“true negative”\n\n\n\n\nTime to evaluate the candidate threshold.\n\nWe can measure how good the threshold is by calculating the “accuracy” of the test on the tests results on the adjusted table. Unlike the accuracy on the training results, which depends on an unrealistic prevalence, the adjusted table has the right prevalence. The accuracy is (150 + 3443)/4000 = 89.8%. Looking at things from the other side, the “mistake” rate is 11.2%.\nIt’s usually the case that the cost of making a mistake is very different for a “false negative” than a “false positive.” A person with a false negative will not receive the appropriate treatment for his or her disease. We call this a “loss,” and it may be a very big problem for that person. On the other hand, a person with a false positive will receive treatment that is unnecessary and potentially harmful, risky, or imposes financial or emotional costs. That’s also a loss. It is a matter of judgement what the relevant sizes of the loss from a false negative and the loss from a false positive.\n\nLet’s suppose that the false-negative loss is 10, and the false-positive loss is 0.5. (These are just made-up numbers for the example.) Then, referring to the table above, the total loss is 50$\\(10 + 357\\)\\(0.5 = 678.5\\). The loss rate, that is, the loss per person, is 678.5/4000 = 0.17 per person.\n\nThe test developers can repeat all these calculations for other candidate thresholds. That will give them a set of thresholds and a set of corresponding loss rates. Pick the threshold with the lowest loss rate.\nNote the importance of the relative sizes of the false-positive loss and the false-negative loss. Expertise in the area of application of the test and follow-up treatments is important to assign meaningful values to the comparative losses. Our 10 and 0.5 are just for the purposes of example."
  },
  {
    "objectID": "posts/Named-tests/index.html",
    "href": "posts/Named-tests/index.html",
    "title": "Named tests from Stat 101",
    "section": "",
    "text": "A Stat 101 course will cover many hypothesis tests among which are the one-sample t-test, the two-sample t-test, the one and two sample p-tests, and ANOVA. All these different tests are in reality just linear regression.\nTo see the translation between regression and the sundry named tests, imagine some data with quantitative variables x and y, a zero-one variable z, and a categorical variable (with potentially many levels) g. Each named hypothesis test corresponds to a particular model specification.\n\none-sample t-test: y ~ 1 and take the p-value from the intercept.\ntwo-sample t-test: y ~ g when g has just two levels. Take the p-value from the g coefficient.\none-sample p-test: z ~ 1 and take the p-value from the intercept.\ntwo-sample p-test: z ~ g when g has just two levels. Take the p-value from the g coefficient.\ntest on simple regression: y ~ x and take the p-value from the x coefficient.\n“one-way” ANOVA: y ~ g when g has more than two levels. There will be a p-value for each level of g (except the reference level), but the p-value for this test is the one from R2() or anova_summary(), which doesn’t refer to a particular coefficient.\n\nThere are many other “forms of ANOVA” that are not covered in Stat101. These all fit in with the linear regression framework. For instance:\n\n“two-way” ANOVA: y ~ g1*g2 and take the p-value from the interaction term in the anova_summary() report.\nanalysis of covariance (ANCOVA): y ~ x + g and take the p-value from the g term in the anova_summary() report."
  },
  {
    "objectID": "posts/Competing-two-hypotheses/index.html",
    "href": "posts/Competing-two-hypotheses/index.html",
    "title": "Competing two hypotheses: a better graph",
    "section": "",
    "text": "In Lesson 35, in the context of medical screening tests, we presented diagrams like this one.\n\n\n\n\n\nThis diagram is based on only three basic numbers—sensitivity, specificity, and prevalence. Exactly the same information could be presented in a 2x2 table:\n\n\n\nTest result\nSick patients\nHealthy patients\n\n\n\n\n\\(\\mathbb P\\)\n12% (true positives)\n26% (false positives)\n\n\n\\(\\mathbb N\\)\n3% (false negatives)\n60% (true negatives)\n\n\n\nThe four numbers necessarily add up to 100%, so one of the numbers is redundant. To generate the table we only need the three basic numbers:\n\nprevalence: 15%, that is, true positives + false negatives\nsensitivity: 12%/(12%+3%) = 80%, that is, true positives divided by prevalence\nspecificity: 70.6%, that is, true negatives/(1-prevalence). Filling in the numbers 60%/(1-15%) = 70.6%.\n\nUsually in statistical graphics, we place the scales on the horizontal and vertical axes, which is not the case with the above diagram. Sticking with the scales-on-axes convention, here is a streamlined graph:\n\n\n\n\n\nWe’ve generalized the notation a bit and emphasized (1-specificity) rather than the specificity itself.\n\nPrior(Alternative) = width of “Alternative” box.\nLikelihood for Alternative hypothesis, that is, p(\\(\\mathbb P\\) | Alternative) (corresponds to sensitivity)\nLikelihood for Null hypothesis, p(\\(\\mathbb P\\) | Null) (corresponds to 1-specificity.)\n\nThe area of each box is, as expected, the width times the height. The two areas printed on the graph are the ingredients for the calculation of the posterior:\nposterior(Alternative | \\(\\mathbb P\\)) = 0.12/(0.12 + 0.255) = 32%"
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html",
    "href": "posts/Hyp-test-Bayesian/index.html",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "",
    "text": "The Bayesian paradigm provides a complete framework for competing two hypotheses against one another. We introduced that framework in the context of screening tests where the hypotheses—diseased vs healthy—are concrete, the likelihoods—p(\\(\\mathbb P\\)) | diseased) and p(\\(\\mathbb P\\) | healthy)—can be found by cross tabulation of test results and disease state.\nThis essay contrasts the Bayesian framework with Fisher’s significance testing and Neyman-Pearson hypothesis testing. The point is to show what each of these testing frameworks leaves out. To simplify the comparison, we’ll use Null and Alternative as the names for the two Bayesian hypotheses, even though Bayes allows a more general choice (or even multiple hypotheses).\nHere is a graphic showing the three inputs to the Bayesian calculation of the posterior probability p(Alternative | obs ): i. the prior p(Alternative) and the two likelihoods ii. p(obs | Alternative) and iii. p(obs | Null)."
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html#comparing-the-three-frameworks",
    "href": "posts/Hyp-test-Bayesian/index.html#comparing-the-three-frameworks",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "Comparing the three frameworks",
    "text": "Comparing the three frameworks\nThe three frameworks—Bayes, Significance, and Neymann-Pearson—all produce a outcome after data have been collected. In Bayes, this structure is more evident from the name “posterior” for the outcome, but it’s equally true in the other two frameworks.\n\nTest set-up\nThe Bayes and Neymann-Pearson frameworks involve two stages; one can occur before the data observations are made, the other only after the observations.\nIn Neymann-Pearson, the set-up amounts to the study design:\n\nspecification of an Alternative hypothesis. (The Null is typically obvious, e.g. a model coefficient would be found to be zero with infinite data.)\nwhat data is to be collected and what test statistic to use to summarize the data,\ndetermining the sample size.\n\nThese three items go into a calculation of the “power” of the test. The power is the conditional probability that the outcome of the test will be to reject the Null. The “given” part of the conditional probability is the Alternative hypothesis. Calculation of the power is done by simulation (or the algebraic equivalent); power does not depend on the eventual observation.\nStudy design is an iterative process which aims to find a feasible study design that maximizes the power. A study design which achieves a power of 80% is generally considered adequate.\nIn Bayes the set-up involves:\n\nSelection of a prior, p(Alternative). This might be subjective or based on existing knowledge (before the data are available). \nCalculation of two likelihood functions, one under the Null and the other under the Alternative. Each of these functions will, eventually, be given the results from the data as input and produce a numerical likelihood as the output.  These functions are often produced by applying a relevant probability model.\n\nIn the case of screening tests, the prior amounts to the prevalence of the disease in the relevant population.With screening tests, this calculation is simple because there are only two possible results from the data: \\(\\mathbb P\\) or \\(\\mathbb N\\). The research project which developed the test determines the sensitivity and specificity, p(\\(\\mathbb P\\) | Alternative) and p(\\(\\mathbb N\\) | Null). The likelihood functions consist of these two likelihoods and their complements.Significance testing has no set-up phase. This is one of the most attractive features of the framework.\n\n\nAllowed inputs to determine the test output\nNeither Significance nor Neymann-Pearson need a prior to be specified for either hypothesis. Bayes requires a prior for both. [In the Bayesian framework, the prior on the Null is simply 1 minus the prior on the Alternative. So only one of the priors needs to be specified.]\n::: {#fig-fisher-neymann diagrams .cell .column-page-right layout-ncol=“2”} ::: {.cell-output-display} {#fig-fisher-neymann diagrams-1 width=672} :::\n\n{#fig-fisher-neymann diagrams-2 width=672}\n\nModification of the Bayes diagram showing the information used to determine the outputs from the Significance framework and Neymann-Pearson framework. :::\n\n\nCalculation of test output\nSignificance framework: The likelihood of the observation under the Null hypothesis is calculated and called the “p-value.” If p is small (usually p < 0.05) then the output is “reject the Null.” Otherwise the output is “fail to reject the Null.”\nNeymann-Pearson framework: Same as for the Significance framework. (Note that the power isn’t used for the calculation of the test output. It’s only involved in the study design.)\nBayes framework: The test output is a posterior probability, not a phrase like “reject the Null.” Calculation of the posterior can be stated in terms of the two areas shown in the graph:\np(Alternative | Obs) = green/(green + blue)\nFor the example in Figure 1, this amounts to 0.12/(0.12+0.17) = 41%. That’s about three times larger than the prior on the Alternative."
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html#alignment-of-the-tests",
    "href": "posts/Hyp-test-Bayesian/index.html#alignment-of-the-tests",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "Alignment of the tests",
    "text": "Alignment of the tests\nThe output of the Significance and Neymann-Pearson tests is always the same, at least if the study design calculated under Neymann-Pearson happened to be the same as for Significance testing.\nThe output from the Bayes framework can be compatible or not, depending as it does on the choice of prior and the likelihood under the Alternative hypothesis.\nPurely for the sake of comparison, let’s look at the Significance and Neymann-Pearson calculations as if there were a meaningful prior in those frameworks."
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html#the-bayesian-paradigm",
    "href": "posts/Hyp-test-Bayesian/index.html#the-bayesian-paradigm",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "The Bayesian paradigm",
    "text": "The Bayesian paradigm\nThe Bayesian paradigm is not limited to considering only two hypotheses; that’s just one setting for it. Let’s adopt this setting, accepting the names “Null” and “Alternative” for the two competing hypotheses. The Bayesian calculation of the posterior requires statement of a prior for each of the hypotheses. (The prior for one hypothesis will be one minus the prior of the other.)\nThere is a nice version of the Bayes calculation in terms of the likelihood ratio and prior and posterior odds.\n\\[\\underbrace{odds(H_a | \\text{obs})}_{\\Large\\text{posterior for } H_a} = \\underbrace{\\left[\\frac{{\\cal L}_a(\\text{obs})}{{\\cal L}_0(\\text{obs})} \\right]}_{\\Large\\text{Likelihood ratio}}\\ \\times\\ \\underbrace{odds(H_a)}_{\\Large\\text{prior for } H_a}\\]\nIf we stipulate that the same prior odds applies to all three frameworks, the critical quantity in shaping the posterior odds is the “likelihood ratio.” A large likelihood ratio pushes the posterior odds on the Alternative higher, which is analogous to “reject the Null.”\nThe argument being made here is that a small p-value corresponds to a large likelihood ratio.\nIn Significance testing, there is no such thing as \\({\\cal L}_a(obs)\\): the only quantity that comes into the test is \\({\\cal L}_0(obs)\\). The same is true in Neymann-Pearson, but the “power” has a similar spirit to \\({\\cal L}_a(obs)\\). Let’s use 80% (the convention for adequate power) is a placeholder for \\({\\cal L}_a(obs)\\), but the precise value is not critical to the argument, just that there is some non-zero placeholder.\nIn Significance and Neymann-Pearson \\({\\cal L}_0(obs)\\) is called the p-value. A low p-value (conventionally, p < 0.05) argues against the Null. Using the placeholder for \\({\\cal L}_a(obs)\\), a low p-value leads to a high likelihood ratio, which argues for the Alternative.\nSince the Significance framework has nothing even analogous to \\({\\cal L}_a(obs)\\), the choice of the threshold level (e.g. p < 0.05) is never informed by the setting for data collection.\nIn Neymann-Pearson, the power is analogous to \\({\\cal L}_a(obs)\\). Using a power of 80%, then p < 0.05 corresponds to a likelihood ratio of 16 or more. Likelihoods in this range are often described as providing “strong evidence” in favor of the Alternative."
  },
  {
    "objectID": "posts/Hyp-test-Bayesian/index.html#strength-of-evidence",
    "href": "posts/Hyp-test-Bayesian/index.html#strength-of-evidence",
    "title": "A Bayes interpretation of Hypothesis testing",
    "section": "“Strength of evidence”",
    "text": "“Strength of evidence”\nIt is common to describe the p-value as measuring the “strength of evidence” against the Null hypothesis. For instance, p=0.10 is described as “weak” evidence, while \\(p=0.001\\) constitutes “strong” evidence.\nIn the Bayes framework, “strength of evidence” can be given a reasonable mathematical definition: the likelihood ratio. The “strength of evidence” is the balance (or lack thereof) between \\({\\cal L}_0(obs)\\) and \\({\\cal L}_a(obs)\\).\nIn the Significance and Neymann-Pearson frameworks, there is no such thing as \\({\\cal L}_a(obs)\\). Lacking that, \\({\\cal L}_0(obs)\\) has nothing to be balanced against. In other words, without a \\({\\cal L}_a(obs)\\), there is no way to define what constitutes a very “small” \\({\\cal L}_0(obs)\\). “Small” can only be defined relevant to the general convention, p < 0.05, which is arbitrary. The only way in which a p-value like 0.001 is “small” is that it is below 0.05. No further claim is justified.\nFairness to Fisher and the Significance framework requires that his actual description of the method (e.g. p < 0.05) be considered, as opposed to the interpretation that is been layered on top of it by later research workers.\nIn his 1926 paper introducing Significance testing, Fisher wrote about the 0.05 (“one in twenty”) threshold. Note that he uses the phrase “high enough odds” where today we would say “low enough p-value.”R.A. Fisher (1926) “The arrangement of field experiments” Journal of the Ministry of Agriculture of Great Britain 33:505-513, link\n\n“If one in twenty does not seem high enough odds, we may if we prefer it, draw the line at one in fifty (the 2 per cent. point), or one in a hundred (the 1 per cent. point). Personally, the writer prefers to set a low standard of significance at the 5 per cent. point, and ignore entirely all results which fail to reach this level.”\n\nUp to this point in his narrative, Fisher’s description does not deviate from contemporary practice with \\(p < 0.05\\), although “ignore entirely” doesn’t align with those today who interpret \\(p > 0.05\\) as supporting the Null.\nBut in the next sentences, Fisher describes what it takes for “a scientific fact [to] be regarded as experimentally established,” which I take to be much the same thing as “strength of evidence.”\n\n“A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails* to give this level of significance. The very high odds sometimes claimed for experimental results should usually be discounted, for inaccurate methods of estimating error have far more influence than the particular standard of significance chosen.*”\n\nNeymann-Pearson’s “alternative hypothesis” and “power” are a mathematical statement of what constitutes a “properly designed experiment” and “rarely fails.”"
  },
  {
    "objectID": "posts/Takeaways-36/index.html",
    "href": "posts/Takeaways-36/index.html",
    "title": "Lesson 36 take-aways",
    "section": "",
    "text": "We are spending this week on a topic that constitutes about one-quarter of the consensus Stat 101 course (e.g. “AP Statistics”). We are calling the topic “Null hypothesis testing” (NHT), but other names are also used:\n\n“Significance testing”—the name used by the originator of the method, Ronald Fisher when he introduced it in 1926.\n“Null hypothesis significance testing” (HST—what to say if you can’t decide between calling it NHT or “significance testing.”\n“Hypothesis testing,” the name almost always used in statistics textbooks, but which is misleading in that it suggests something broader than NHT actually is.\n\nOur agenda today is to describe the terminology of NHT and give an example of an NHT calculation (which is easy to do with any statistical software at all).\n\nNHT involves a quantity called the “p-value” which is a number between zero and one.\nWe have talked previously about tests that give a result of either \\(\\mathbb P\\) or \\(\\mathbb N\\). In NHT, the test results are stated differently: either “reject the Null” or “fail to reject the Null.”\nOnce you have the numerical p-value, translation into test results is trivial: if \\(p < 0.05\\) the conclusion is “reject the Null.” Otherwise, that is if p is bigger than 0.05, the conclusion is “fail to reject the Null.” This is admittedly stilted language, and we owe you and explanation for why things are this way. That will come later.\nWhen using linear regression (our main method in 300Z), the software for summarizing models always provides a p-value: you just have to ask for it. To illustrate, consider the model height ~ nkids with respect to Galton’s height data. (Galton (1822-1911) and Fisher (1890-1962) were near contemporaries.)\nThink of height ~ nkids as asking a question: Is the adult height of a child correlated with the number of siblings? (Why might someone offer the hypothesis that the number of siblings has a connection to adult height? Perhaps the growing children had to compete for food. Or perhaps contagious disease is more prevalent in large families, and childhood disease might be correlated with height. But in NHT, there’s no requirement to explain why one is interested to test a hypothesis.) Here’s the calculation, done in four different ways of summarizing a model.\n\n\nmodel <- lm(height ~ nkids, data=Galton)\nmodel |> conf_interval(show_p=TRUE)\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n    p.value \n  \n \n\n  \n    (Intercept) \n    67.2185704 \n    67.7997464 \n    68.380922 \n    0.0000000 \n  \n  \n    nkids \n    -0.2561222 \n    -0.1693416 \n    -0.082561 \n    0.0001372 \n  \n\n\n\nmodel |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    898 \n    1 \n    0.0161062 \n    14.66737 \n    0.0150081 \n    0.0001372 \n    1 \n    896 \n  \n\n\n\nmodel |> regression_summary()\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    67.7997464 \n    0.2961233 \n    228.9578 \n    0.0000000 \n  \n  \n    nkids \n    -0.1693416 \n    0.0442168 \n    -3.8298 \n    0.0001372 \n  \n\n\n\nmodel |> anova_summary()\n\n\n\n \n  \n    term \n    df \n    sumsq \n    meansq \n    statistic \n    p.value \n  \n \n\n  \n    nkids \n    1 \n    185.4636 \n    185.46365 \n    14.66737 \n    0.0001372 \n  \n  \n    Residuals \n    896 \n    11329.5987 \n    12.64464 \n    NA \n    NA \n  \n\n\n\n\nIn the regression summary report and the confidence interval report, a p-value is listed for each coefficient. We are interested in the nkids coefficient.\nIn the R-squared and ANOVA report, there is no p-value on the intercept term; only nkids is at issue.\nNote that the p-value is the same for all four reports.\nAn NHT consists of calculating a p-value, comparing it to 0.05, and drawing the corresponding conclusion. Since \\(p = 0.000137 < 0.05\\), the proper conclusion is to “reject the Null.”\n\n“The Null” is short for “the Null hypothesis.” The dictionary definitions for “null” relevant here include “having or associated with the value zero” or “amounting to nothing.” In the context of linear regression the Null for a given coefficient always means that, with a sufficiently large (“infinite”) sample, the coefficient would be zero.\nThe p-value calculation is done in a mathematical world where the Null is true. Other expressions often used: “assuming the Null,” “given the Null hypothesis,” “under the Null.”\nNaturally, our samples are finite in size. Consequently, because of sampling variation, we cannot expect the coefficient to be exactly zero even under the Null. Instead, we expect the coefficient to be small.\n\nWe have already discussed one operational definition of “small,” that the confidence interval includes zero. If so, then “fail to reject the Null.” Otherwise (as with the nkids example above), then “reject the Null.”\nThe p-value is just another way of encoding the notion of “small.” Indeed, in linear regression we can calculate the p-value from the same information used to construct the confidence interval.\n\n\n\n\n\n\n\n\nMany types of statistical tests?\n\n\n\nA Stat 101 course will cover many hypothesis tests among which are the one-sample t-test, the two-sample t-test, the one and two sample p-tests, and ANOVA. All these different tests are in reality just linear regression. See this blog post.\nThere is one hypothesis test that is not exactly equivalent to regression: the chi-squared test. However, in the context where chi-squared often appears, the result corresponds to the z ~ g model specification. [Blog post not yet available.]"
  },
  {
    "objectID": "posts/Takeaways-36/index.html#abstracting-the-situation",
    "href": "posts/Takeaways-36/index.html#abstracting-the-situation",
    "title": "Lesson 36 take-aways",
    "section": "Abstracting the situation",
    "text": "Abstracting the situation\nNow that we have the formula for the posterior for H1, we can generalize from the medical screening test to other situations.\n\nTwo mutually exclusive hypotheses: H1 & H2\nA prior for H1, indicating our level of belief in H1.\n\nThe rest of our belief goes to H2.\n\nTwo likelihoods, one for each hypothesis. These are:\n\np(obs | H1) and p(obs | H2)\n\nThe posterior for H1 gives our level of belief in H1 after making the observation."
  },
  {
    "objectID": "posts/Takeaways-36/index.html#example-the-ivory-billed-woodpecker-sighting",
    "href": "posts/Takeaways-36/index.html#example-the-ivory-billed-woodpecker-sighting",
    "title": "Lesson 36 take-aways",
    "section": "Example: The Ivory-billed woodpecker sighting",
    "text": "Example: The Ivory-billed woodpecker sighting\nThe Ivory-billed woodpecker has been thought to be extinct; the last known observation of one was in 1944. A new sighting in the Pearl River forest in Arkansas in 2004 became national news and excited efforts to confirm the sighting. These have not been successful and there is skepticism whether the reported 2004 sighting was correct. This is not an easy matter since the 2004 sighting was fleeting and the Ivory-billed woodpecker is very similar to a relatively common bird, the Pileated woodpecker.\n& \\ \\ \\ \\end{tabular}} \nHypothesis 1: The Ivory-billed is extinct.\nHypothesis 2: They really saw an Ivory-billed woodpecker.\nAn experiment was done using two different decoy birds. What did birdwatchers report for each of the different birds?\n\n\n\nObservation\nCode\nIvory-billed\nPileated\n\n\n\n\nShort & Dull\nA\n0.01\n0.60\n\n\nLong & Dull\nB\n0.10\n0.13\n\n\nShort & Glossy\nC\n0.04\n0.20\n\n\nLong & Glossy\nD\n0.60\n0.05\n\n\nShort & White Back\nE\n0.05\n0.01\n\n\nLong & White Back\nF\n0.20\n0.01\n\n\n\nEach of the numbers is a likelihood, e.g. 0.01 is the probability of observing Short & Dull given the bird was an Ivory-billed."
  },
  {
    "objectID": "posts/Hyp-test-and-poker/index.html",
    "href": "posts/Hyp-test-and-poker/index.html",
    "title": "Hypothesis testing and poker",
    "section": "",
    "text": "Suppose I had a statistics class with 19 students. One day I walk in and announce a that we are going to play poker. Any student who has a better hand than I will get a grade of “A.”\nI pull two decks out of my pocket, each deck in its box, which has already been opened.\n“I shuffled these two decks this morning,” I announce. “Deal yourself five cards and then pass the remainder of the deck on to your neighbor.”\nI hand one complete deck to a student in the back of the room. Then I deal myself five cards from the other deck, and pass the remainder to a student in the front row. The front-row student likes to joke around, which I like. She says, “Are you sure that you shuffled this deck?”\n“Maybe not,” I reply. “Why don’t you make sure by shuffling it again.” But I hold on to my original five cards.\nOnce each of the 19 students has a hand of five cards, I ask them to rank their hands. Poker experience is not a pre-requisite for my stats class so I go over the rules, trying to turn it into a lesson in probability.\nA quick web search later, I am on this site and projecting this diagram:\n\n\n\nNow the reveal: I got two pair: 5s and aces.\nThe students groan. I’ve beaten them all.\nThe student in the front row once again asks, “Are you sure that you shuffled this deck?” She remembers that I had already selected my cards before she reshuffled them.\nQuestions:\n\nIs her skepticism warranted? Calculate the p-value for my getting two pair under the Null hypothesis that the deck was fairly shuffled before class.\nIf one of the students had happened to beat me, would that change the p-value on my hand?\nSuppose that my hand had been one pair. What would be the p-value? Should I conclude that the deck had been fairly shuffled before class.\n\n \nThis problem was suggested by Lt. Col. Kenneth Horton.\n\n\n\n\n\n\nDigression on probability\n\n\n\nThe probabilities in the above table are wrong. There are 2,598,960 distinct poker hands. (This is after sorting the cards within each hand.) There are indeed 4 ways to get a royal flush, putting the probability at 1/649,740.\nHere’s a deck of cards in R:\n\n\nCode\nsuits <- \"♣♠♡♢\"|> strsplit(split=NULL)|> unlist()\nvalues <- c(2:10, \"A\", \"J\", \"Q\", \"K\")\ndeck <- outer(suits, values, FUN=paste0) |> c()\ndeck\n\n\n [1] \"♣2\"  \"♠2\"  \"♡2\"  \"♢2\"  \"♣3\"  \"♠3\"  \"♡3\"  \"♢3\"  \"♣4\"  \"♠4\"  \"♡4\"  \"♢4\" \n[13] \"♣5\"  \"♠5\"  \"♡5\"  \"♢5\"  \"♣6\"  \"♠6\"  \"♡6\"  \"♢6\"  \"♣7\"  \"♠7\"  \"♡7\"  \"♢7\" \n[25] \"♣8\"  \"♠8\"  \"♡8\"  \"♢8\"  \"♣9\"  \"♠9\"  \"♡9\"  \"♢9\"  \"♣10\" \"♠10\" \"♡10\" \"♢10\"\n[37] \"♣A\"  \"♠A\"  \"♡A\"  \"♢A\"  \"♣J\"  \"♠J\"  \"♡J\"  \"♢J\"  \"♣Q\"  \"♠Q\"  \"♡Q\"  \"♢Q\" \n[49] \"♣K\"  \"♠K\"  \"♡K\"  \"♢K\" \n\n\nHow many different five-card hands are there?\n\n\nCode\nAll_hands <- combinat::combn(deck, 5) |> \n  apply(2, FUN=function(x) paste0(c(x), collapse=\"|\"))\nlength(All_hands)\n\n\n[1] 2598960\n\n\nHere’s a possible set of 10 hands dealt from a thoroughly shuffled deck:\n\n\nCode\nsample(All_hands, size=10)\n\n\n [1] \"♡3|♢3|♣5|♡6|♢10\"  \"♠5|♡5|♣6|♠7|♠A\"   \"♢2|♡6|♡10|♢10|♡A\" \"♡4|♢4|♢5|♢J|♣Q\"  \n [5] \"♣4|♣6|♠8|♢8|♡K\"   \"♢3|♠4|♡8|♠J|♣K\"   \"♠2|♠6|♡6|♢8|♡K\"   \"♣3|♢8|♣10|♢10|♠A\"\n [9] \"♠2|♢3|♠A|♢A|♣J\"   \"♣3|♣6|♡6|♠8|♠J\""
  },
  {
    "objectID": "posts/Reduction_verbs/index.html",
    "href": "posts/Reduction_verbs/index.html",
    "title": "Shrinkage verbs",
    "section": "",
    "text": "Much of the time when we use mutate(), we are generating a new variable out of those already in the data frame. All sorts of mathematical and character transformations are available. For instance, the KidsFeet data frame records the length and width of 39 third- and fourth-grade children. If we would like to work with the aspect ratio of the feet, which is length divided by width, mutate() will do the work for us.\nThis Lesson introduces transformations of a different kind, some of which you are already familiar with. We will call these “shrinkage” transformations because, rather than dealing with the data frame rows one at a time, these transformations work on the rows collectively.\nPerhaps the simplest shrinkage transformation is averaging. An average, of course, combines (shrinks) many numerical values to give a single representative one. Two examples of averages are the mean and median. When mutate() encounters a shrinkage transformation of this sort, it inserts the same value for all of the rows. You can think of mean or median as shrinking the range of values of its argument into a single number.\nUsually, we prefer to give column names that are short and have no special characters. To accomplish this, use named arguments to mutate(). The names are up to you. Here’s an example:"
  },
  {
    "objectID": "posts/Reduction_verbs/index.html#groupwise-operations",
    "href": "posts/Reduction_verbs/index.html#groupwise-operations",
    "title": "Shrinkage verbs",
    "section": "Groupwise operations",
    "text": "Groupwise operations\nThe group_by() wrangling verb sets up mutate() to use shrinkage transformations separately for each group. For instance:\n\nKidsFeet |> \n  group_by(sex) |>\n  mutate(rank=rank(length))\n\n\n\n\n\n\n \n  \n    sex \n    length \n    rank \n  \n \n\n  \n    B \n    26 \n    3 \n  \n  \n    B \n    24 \n    2 \n  \n  \n    B \n    24 \n    1 \n  \n  \n    G \n    26 \n    2 \n  \n  \n    G \n    24 \n    1 \n  \n\n\nTable 5:  group_by() sets up mutate() to calulate shrinkage values group-by-group. \n\n\nRank 1 appears twice in Table 5, once for the girls and once for the both. Within both groups, rank 1 is assigned to the row with the smallest length.\nIn the next Lesson we will work extensively with groupwise means as a way of summarizing the similarities within a group and the differences between groups."
  },
  {
    "objectID": "posts/Shrinkage_verbs/index.html",
    "href": "posts/Shrinkage_verbs/index.html",
    "title": "Shrinkage verbs",
    "section": "",
    "text": "Much of the time when we use mutate(), we are generating a new variable out of those already in the data frame. All sorts of mathematical and character transformations are available. For instance, the KidsFeet data frame records the length and width of 39 third- and fourth-grade children.  If we would like to work with the aspect ratio of the feet, which is length divided by width, mutate() will do the work for us.We will use a short subsample of the KidsFeet data for the purposes of demonstration.\nThis Lesson introduces transformations of a different kind, some of which you are already familiar with. We will call these “shrinkage” transformations because, rather than dealing with the data frame rows one at a time, these transformations work on the rows collectively.\nPerhaps the simplest shrinkage transformation is averaging. An average, of course, combines (shrinks) many numerical values to give a single representative one. Two examples of averages are the mean and median. When mutate() encounters a shrinkage transformation of this sort, it inserts the same value for all of the rows. You can think of mean or median as shrinking the range of values of its argument into a single number.\nUsually, we prefer to give column names that are short and have no special characters. To accomplish this, use named arguments to mutate(). The names are up to you. Here’s an example:"
  },
  {
    "objectID": "posts/Shrinkage_verbs/index.html#groupwise-operations",
    "href": "posts/Shrinkage_verbs/index.html#groupwise-operations",
    "title": "Shrinkage verbs",
    "section": "Groupwise operations",
    "text": "Groupwise operations\nThe group_by() wrangling verb sets up mutate() to use shrinkage transformations separately for each group. For instance:\n\nKidsFeet |> \n  group_by(sex) |>\n  mutate(rank=rank(length))\n\n\n\n\n\n\n \n  \n    sex \n    length \n    rank \n  \n \n\n  \n    B \n    26.1 \n    3 \n  \n  \n    B \n    24.5 \n    2 \n  \n  \n    B \n    23.6 \n    1 \n  \n  \n    G \n    26.0 \n    2 \n  \n  \n    G \n    24.5 \n    1 \n  \n\n\nTable 5:  group_by() sets up mutate() to calulate shrinkage values group-by-group. \n\n\nRank 1 appears twice in Table 5, once for the girls and once for the both. Within both groups, rank 1 is assigned to the row with the smallest length.\nIn the next Lesson we will work extensively with groupwise means as a way of summarizing the similarities within a group and the differences between groups."
  },
  {
    "objectID": "posts/Prelude-to-modeling/index.html",
    "href": "posts/Prelude-to-modeling/index.html",
    "title": "Prelude to modeling",
    "section": "",
    "text": "Using a shrinkage transformation like mean() along with group_by() constitutes a simple form of statistical modeling, a powerful way to summarize data and to characterize the relationships among variables. Statistical modeling has its own vocabulary and a major goal of this Lesson is to introduce that vocabulary.\nPicking up on the example from the previous Lesson, here are the mean foot widths of the children recorded in the KidsFeet data frame.\nWe have given the name modval to the column added by mutate. This is short for “model value,” which will become our generic name for the quantity calculated using shrinkage transforms in this way. Two additional important new vocabulary terms are “response variable” and “explanatory variable(s).” The response variable is the quantity we seek to explain via modeling. The explanatory variable(s) are the materials for the explanation. In the wrangling command that produces Table 1, width the response variable. Naturally, width varies from row to row; this is no more than saying that kids differ one from another in this way.\nThe goal of modeling is to shrink away unwanted variation in the response variable, keeping only that variation associated with the explanatory variable(s). You can see that modval still varies from row to row, but the variation is simpler. All that is retained from the response variable width is the variation that goes along with the explanatory variable sex.\nFigure 1 displays such shrinkage visually. The transluscent dots show each childs actual foot width. The darker dots are the model values; all the girls have one model value, all the boys another.\nEven though all the girls share the same model value and similarly with the boys, each child has his or her own “residual,” which is the difference between that child’s actual foot width and the model value for that child. Calculating the residuals is very easy: just subtract the model value from the response variable’s value.\nFigure 2 shows the residuals for each child as a vertical line segment. When the actual foot width is higher than the model value, the residual is positive. When the foot width is lower than the model value, the residual is negative.\nThe purpose of statistical modeling is to divide the variation in the response variable into two components: the variation in the model values and the variation in the residuals."
  },
  {
    "objectID": "posts/Prelude-to-modeling/index.html#groupwise-operations",
    "href": "posts/Prelude-to-modeling/index.html#groupwise-operations",
    "title": "Shrinkage verbs",
    "section": "Groupwise operations",
    "text": "Groupwise operations\nThe group_by() wrangling verb sets up mutate() to use shrinkage transformations separately for each group. For instance:\n\nKidsFeet |> \n  group_by(sex) |>\n  mutate(rank=rank(length))\n\n\n\n\n\n\n \n  \n    sex \n    length \n    rank \n  \n \n\n  \n    B \n    26 \n    3 \n  \n  \n    B \n    24 \n    2 \n  \n  \n    B \n    24 \n    1 \n  \n  \n    G \n    26 \n    2 \n  \n  \n    G \n    24 \n    1 \n  \n\n\nTable 5:  group_by() sets up mutate() to calulate shrinkage values group-by-group. \n\n\nRank 1 appears twice in Table 5, once for the girls and once for the both. Within both groups, rank 1 is assigned to the row with the smallest length.\nIn the next Lesson we will work extensively with groupwise means as a way of summarizing the similarities within a group and the differences between groups."
  },
  {
    "objectID": "posts/Prelude-to-modeling/index.html#quantitive-explanatory-variables",
    "href": "posts/Prelude-to-modeling/index.html#quantitive-explanatory-variables",
    "title": "Prelude to modeling",
    "section": "Quantitive explanatory variables",
    "text": "Quantitive explanatory variables\nWhen the explanatory variable is categorical—sex in the previous examples—the mean() is a good way to shrink the individual values of the response onto a single value for each level of the explanatory variable.\n\n\n\nBut often an explanatory variable is quantitative. For instance, we might decide to model foot width using foot length as the explanatory variable. Among the 39 kids in KidsFeet, there are 25 unique foot lengths. If we use group_by(length) then there will be 25 different model values, as in Figure 3.\n\nKidsFeet |> \n  group_by(length) |>\n  mutate(modval = mean(width),\n         resid = width - modval,\n         linetype = resid > 0) |>\n  ggplot(aes(x=length)) +\n  geom_point(aes(y=width), alpha=0.2) +\n  geom_line(aes(y=modval), color=\"blue\") +\n  geom_segment(aes(y=modval, yend=width, x=length, xend=length, \n                   linetype=linetype), \n               linewidth=0.2) +\n  guides(linetype=\"none\")\n\n\n\n\nFigure 3: Grouping by length creates 25 different groups, one for each unique value of length. For groups with just one kid, the model value goes exactly through the value of foot width for that kid. For groups with two or three kids, the model value goes somewhere in the middle vertically. Here, the model values are connected by line segments to make it easier to see how they vary with foot length.\n\n\n\n\nWith so many different model values, there’s hardly any shrinkage apparent; the model values vary almost as much as the response variable (width) does.\n[Modeling with ntiles in the exercises.]\nA better  approach for quantitative explanatory variables is to shrink down to a line or curve that goes through the middle of the various width values. In this Lesson, we will use the shrink_linear() function to create the model values. The first argument is the response variable, the second the explanatory variable.We won’t be using shrink_linear() for long. It turns out that the wrangling notation is not very well suited to building anything but simple models.\nTo illustrate, the next command shows the usage of shrink_linear().\n\nKidsFeet |> mutate(modval = shrink_linear(width, length))\n\n\n\n\n\nTable 2:  Model values calculated with width as the response variable and length as the explanatory variable. \n \n  \n    sex \n    length \n    width \n    modval \n  \n \n\n  \n    B \n    26.1 \n    9.1 \n    9.33 \n  \n  \n    B \n    24.5 \n    9.7 \n    8.94 \n  \n  \n    B \n    23.6 \n    9.0 \n    8.71 \n  \n  \n    G \n    26.0 \n    9.0 \n    9.31 \n  \n  \n    G \n    24.5 \n    9.0 \n    8.94 \n  \n\n\n\n\n\nJust to look at Table 2, the model values seem a jumble. It’s easier to see what’s going on by graphing the data and model values, as in Figure 4.\n\n\nCode\nP2 <- KidsFeet |> \n  mutate(modval = shrink_linear(width, length),\n         resid = width - modval,\n         linetype = resid > 0) |>\n  ggplot(aes(x=length)) +\n  geom_point(aes(y=width), alpha=0.2) +\n  geom_point(aes(y=modval)) +\n  geom_line(aes(y=modval), color=\"blue\") + \n  geom_segment(aes(y=modval, yend=width, \n                   x=length, xend=length, \n                   linetype=linetype), \n               linewidth=0.2) +\n  guides(linetype=\"none\")\nP2\n\n\n\n\n\nFigure 4: A point plot of width versus length from the KidsFeet data frame. The model values (black dots) are set by imposing a linear relationship between the response and explanatory variables.\n\n\n\n\nThe scatter of the width and length variables in a point plot is shrunk down to a line by the shrink_linear() function. Overall, the pattern of the model values shows a steady increase in foot width as a function of foot length.\nIt’s also possible to use shrink_linear() in conjunction with group_by().\n\nKidsFeet |> \n  group_by(sex) |> mutate(modval = shrink_linear(width, length))\n\n\n\n\n\nTable 3:  Grouping KidsFeet by sex produces model values that are on separate straight lines for girls and boys. \n \n  \n    sex \n    length \n    width \n    modval \n  \n \n\n  \n    B \n    26.1 \n    9.1 \n    9.40 \n  \n  \n    B \n    24.5 \n    9.7 \n    9.06 \n  \n  \n    B \n    23.6 \n    9.0 \n    8.87 \n  \n  \n    G \n    26.0 \n    9.0 \n    9.17 \n  \n  \n    G \n    24.5 \n    9.0 \n    8.83 \n  \n\n\n\n\n\n\n\nCode\nP3 <- KidsFeet |> \n  group_by(sex) |>\n  mutate(modval = shrink_linear(width, length),\n         resid = width - modval,\n         linetype = resid > 0) |>\n  ggplot(aes(x=length, color=sex)) +\n  geom_point(aes(y=width), alpha=0.2) +\n  geom_point(aes(y=modval)) +\n  geom_line(aes(y=modval)) +\n  scale_color_manual(values=c(\"blue\", \"red\")) +\n  geom_segment(aes(y=modval, yend=width, \n                   x=length, xend=length, \n                   linetype=linetype), \n               linewidth=0.2) +\n  guides(linetype=\"none\")\nP3\n\n\n\n\n\nFigure 5: A point plot of width versus length with sex as a grouping variable."
  },
  {
    "objectID": "posts/Prelude-to-modeling/index.html#is-simpler-better",
    "href": "posts/Prelude-to-modeling/index.html#is-simpler-better",
    "title": "Prelude to modeling",
    "section": "Is simpler better?",
    "text": "Is simpler better?\nEarlier in this Lesson we said,\n\n“The goal of modeling is to shrink away unwanted variation in the response variable, keeping only that variation associated with the explanatory variable(s).”\n\nFigure 6 shows six different ways of shrinking the width variable.\n\n\n\n\n\n\n\n(a) mean(width)\n\n\n\n\n\n\n\n(b) mean(width) grouped by sex\n\n\n\n\n\n\n\n(c) linear by length\n\n\n\n\n\n\n\n\n\n(d) linear by length grouped by sex\n\n\n\n\n\n\n\n(e) grouping by length\n\n\n\n\n\n\n\n(f) grouping by length and sex\n\n\n\n\nFigure 6: Six different ways of shrinking width based on length and sex.\n\n\n\nExercise: Show that the sum of squares and mean square for modeling using the median is larger than for the mean."
  },
  {
    "objectID": "posts/Prelude-to-modeling/index.html#are-simpler-models-better",
    "href": "posts/Prelude-to-modeling/index.html#are-simpler-models-better",
    "title": "Prelude to modeling",
    "section": "Are simpler models better?",
    "text": "Are simpler models better?\nFigure 6 shows six different ways of constructing model values. In each case, the model values constitute the shrunken values of the response variable width. In panel (a), the shrinkage is the most possible, all the way down to a single model value of width. In (b) the shrinkage is a little less; one model value for each sex. In successive panels (c) through (e), the shrinkage gets smaller and smaller. By panel (f), there is hardly any shrinkage in width at all.\n\n\n\n\n\n\n\n(a) mean(width)\n\n\n\n\n\n\n\n(b) mean(width) grouped by sex\n\n\n\n\n\n\n\n(c) linear by length\n\n\n\n\n\n\n\n\n\n(d) linear by length grouped by sex\n\n\n\n\n\n\n\n(e) grouping by length\n\n\n\n\n\n\n\n(f) grouping by length and sex\n\n\n\n\nFigure 6: Six different ways of shrinking width based on length and sex.\n\n\nEarlier in this Lesson we stated,\n\n“The goal of modeling is to shrink away unwanted variation in the response variable, keeping only that variation associated with the explanatory variable(s).”\n\n\nExercise: Show that the sum of squares and mean square for modeling using the median is larger than for the mean."
  },
  {
    "objectID": "posts/Stat-vocabulary/index.html",
    "href": "posts/Stat-vocabulary/index.html",
    "title": "Stats Vocabulary",
    "section": "",
    "text": "When reviewing for a course, it’s often a good idea to start with the specialized words and phrases that are used in the subject.\nTo that end, I have tallied up the most common specialized words and phrases in the documents associated with the second half of 300Z: the textbook, instructor notes, wooksheets, and blog posts. Here are the most common along with the count of how many times they appear in the documents. I’ve added some of the less common ones because I think their importance is bigger than their count."
  },
  {
    "objectID": "posts/Stat-vocabulary/index.html#data",
    "href": "posts/Stat-vocabulary/index.html#data",
    "title": "Stats Vocabulary",
    "section": "Data",
    "text": "Data\ndata frame, unit of observation, variable, column, row variable: numerical (quantitative) vs categorical categorical: - levels: the allowed values, e.g., “red”, “blue”, “green” or “smoker”, “nonsmoker” - zero-one transformation (for categorical variables with two levels)"
  },
  {
    "objectID": "posts/Stat-vocabulary/index.html#sample",
    "href": "posts/Stat-vocabulary/index.html#sample",
    "title": "Stats Vocabulary",
    "section": "Sample",
    "text": "Sample\nsampling variability, sampling bias, sample size (\\(n\\)), sample statistic, random selection"
  },
  {
    "objectID": "posts/Stat-vocabulary/index.html#summaries",
    "href": "posts/Stat-vocabulary/index.html#summaries",
    "title": "Stats Vocabulary",
    "section": "Summaries",
    "text": "Summaries\nsample statistic\n\nFor a single variable/column:\n\nmost used in 300Z: mean, variance (var()), standard deviation (sd())\nmany others, e.g. median\n\nFor relating multiple variables: our main method is regression modeling"
  },
  {
    "objectID": "posts/Stat-vocabulary/index.html#regression-modeling",
    "href": "posts/Stat-vocabulary/index.html#regression-modeling",
    "title": "Stats Vocabulary",
    "section": "Regression modeling",
    "text": "Regression modeling\nresponse variable, explanatory variables, covariates\n\nFitting (or, equivalently, “model training”)\n\nmodeler chooses model specification (e.g. y ~ x + g) and provides training data\nlm() is our main tool for fitting: e.g. lm(y ~ x + g, data=my_data_frame)\n\nwe have also used glm(..., family=binomial) (“logistic regression”) when response variable is in zero-one format\n\nproduces model coefficients\n\nintercept term (always present)\nquantitative explanatory variable: only one coefficient which is rate of change of model output with respect to this input.\ncategorical explanatory variable:\n\none level is (automatically) used as a reference.\none coefficient for each other level, gives difference from reference level\n\n\nresiduals are response values minus the model output\n\nModel summaries:\n\nConfidence interval (CI)\n\nestimate of “precision” of estimated coefficient, describes how much coefficient might be different if we collected a new sample.\nan interval for each model coefficient. Intervals have a lower end and upper end.\nconfidence level (e.g. 95% is standard)\n\n“sampling variance” is estimate of size of sampling variation\n“standard error” (square-root of sampling variance)\nmargin of error \\(\\approx 2 \\times\\) standard error\nconfidence interval \\(\\equiv\\) coefficient \\(\\pm\\) margin of error\n\nCI can be translated to a p-value\n\np < 0.05 means that CI (at 95% confidence level) does not include zero.\n\n“Effect size” summarizes how much the model output changes when the value of an input changes. For our models in 300Z, the effect size is the same as the model coefficient.\n\nR-squared:\n\nR2 = var(model values) / var(response variable)\nR2 measures fraction of response variance “explained” or “accounted for” by the explanatory variables.\n1 - R2 is fraction of response variance that remains unexplained.\n\nMentioned but not much used in Math 300Z\n\n“Regression report” gives for each coefficient the coef. itself, its standard error, and a p-value. Essentially a different format for the same info that’s in the confidence interval report.\nANOVA: describes categorical variables as one unit rather than as separate levels."
  },
  {
    "objectID": "posts/Stat-vocabulary/index.html#hypothesis-testing",
    "href": "posts/Stat-vocabulary/index.html#hypothesis-testing",
    "title": "Stats Vocabulary",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nHypothesis: a statement that might or might not be true. We have focussed on competing two hypotheses.\n\nNull hypothesis: the explanatory variable is not related to the response variable.\nAlternative hypothesis: the explanatory variable is related\n\n“Significance testing” or “Null hypothesis testing” looks only at the Null hypothesis.\n\np-value is likelihood of observed sample statistic (e.g. a coefficient) on “Planet Null,” a world where the Null is true.\nSmall p-value means that the observed sample statistic is unlikely on Planet Null. If p is small, then “reject the Null.” Otherwise, “fail to reject the Null.”\n\nOften people rephrase “reject the Null” as “the result (e.g. a coefficient) is ‘statistically significant.’” But this is misleading; the p-value on its own is not a measure of the practical importance of the result. Better to say “statistically discernible,” or even just p < 0.05.\nUse of 0.05 as a threshold for “reject the Null” is merely a convention, but that convention is so widely used as to have become the operational definition of “reject the Null.”\n\nCalculating a p-value. Usually done by software but you could do it yourself in different ways:\n\np-value on a coefficient: Does the 95% CI encompass zero?\nCarry out many trials in which you shuffle the explanatory variable, fit the model to the shuffled data, and calculate the model summary (e.g. model coefficient or R2). Find the fraction of trials where the model summary is larger in magnitude than the model summary from the unshuffled data.\n\n\nBayesian inference considers both the Null and the Alternative hypotheses. - Overall result is the relative probability of the Alternative and the Null - prior probability: prob(Alternative | no observation yet). This is often a subjective guess. - posterior probability: prob(Alternative | after observation taken into account) - two likelihoods:\n\nprob(observation | Alternative)\nprob(observation | Null) - Likelihoods and the prior probability are combined to calculate the posterior probability. This is a matter of using a formula, but we haven’t emphasized this calculation (except in the concrete setting of medical screening).\n\nMedical screening (our concrete example of Bayesian inference)\n\nexamples of screening for diseases: mammography for breast cancer, prostate specific antigen (PSA) for prostate cancer, COVID antibodies for COVID infection.\nthe patient is tested, returning a positive or a negative result.\n\nCommon sense suggests that a positive test means the patient has the disease, a negative test means otherwise. But in reality, a positive test merely suggests the patient is more likely than a random person to have the disease.\n\nposterior probability: prob(disease | positive test)\nprior probability: prob(disease | no test yet). In medical screening, the prior is the probability that a randomly selected person will genuinely have the disease.\ntwo likelihoods\n\nprob(positive test | genuinely has the disease). Called the “sensitivity.”\nprob(positive test | genuinely does not have the disease). This is 1 - “specificity.”\n\nLikelihoods and the prior are combined to calculate the posterior probability. We did this by comparing the two blue areas in this diagram:"
  },
  {
    "objectID": "posts/Stat-vocabulary/index.html#risk",
    "href": "posts/Stat-vocabulary/index.html#risk",
    "title": "Stats Vocabulary",
    "section": "Risk",
    "text": "Risk\n\nTwo components to a risk:\n\nValue at risk, that is, how bad is the outcome going to be.\nRisk level: probability that the pad outcome will happen\n\nThree entirely equivalent ways to describe the risk level\n\nas a probability (p, a number between zero and one)\nas odds (p/(1-p)), a number between zero and infinity\nas “log-odds,” the logarithm of the odds. a number between -infinity and infinity.\n\nRisk factor: a condition or action that changes the risk level, e.g. smoking and lung cancer\n\nrisk ratio: risk level with the condition divided by the risk level without the condition\nbaseline risk: the risk level without any risk-factor conditions\nrisk level with the condition is risk ratio multiplied by baseline risk.\n“absolute” change in risk: risk level with the condition minus baseline risk. Often measured in “percentage points.”\n“relative risk” is about the risk ratio (without the baseline coming into consideration). Risk ratio of 1.5 means a increase in relative risk of 50% (say “percent”, not “percentage points”). Risk ratio of 2.3 means a 130 percent increase in relative risk.\n\nLogistic regression (glm(..., family=binomial)) used to model risk as a function of risk factors. (We didn’t spend much time on this, but it’s good to know what “logistic regression” refers to.)"
  },
  {
    "objectID": "posts/Stat-vocabulary/index.html#prediction",
    "href": "posts/Stat-vocabulary/index.html#prediction",
    "title": "Stats Vocabulary",
    "section": "Prediction",
    "text": "Prediction\n\nclosely related to risk\nproper form for a prediction is to assign a probability to every possible outcome.\n\nmodeling for prediction: outcome level is the response, explanatory variables are the “predictors,” that is, the inputs used to generate the prediction.\n\nprediction interval is a shorthand for (ii) - 95% prediction interval is the range of outcomes that covers 95% of the outcome probabilities. - prediction interval will cover approx. 95% of the response variable"
  },
  {
    "objectID": "posts/Stat-vocabulary/index.html#dags",
    "href": "posts/Stat-vocabulary/index.html#dags",
    "title": "Stats Vocabulary",
    "section": "DAGs",
    "text": "DAGs\n\nA representation of a causal hypothesis.\nStands for “Directed Acyclic Graph” but you can think of it as a plumbing diagram for “causal flow.”\n\nGraph: A mathematical “graph” consisting of “nodes” connected by “edges.”\nDirected: each edge points in one direction\nAcyclic: if you follow the edges from a starting node, you can’t return to (“cycle back to”) the starting node.\n\nPath: A set of edges that connects two nodes. (You can ignore the edge directions.)\nCausal path: A set of edges that connects two nodes, but the edge directions are taken into account. - Criteria for a causal path between nodes X and Y:\n\nThere be a path between X and Y.\nThere is some node on the path from which you can get to both X and Y by following the edges in the proper direction. Examples of causal paths between X and Y:\n\n\\(X \\rightarrow Y\\)\n\\(X \\leftarrow C \\rightarrow Y\\)\n\n\nOther nodes on path between X and Y can be\n\nCollider: \\(X \\rightarrow C \\leftarrow Y\\)\nIntermediary: \\(X \\rightarrow C \\rightarrow Y\\)\nConfounder: Consists of a direct path \\(X \\rightarrow Y\\) and an indirect path \\(X \\leftarrow C \\rightarrow Y\\).\n\nPath analysis: looking at all the paths connecting X and Y in order to figure out which covariates (“C”) we should include in a model or exclude from a model, that is, choosing between the two model specifications Y ~ X pr Y ~ X + C."
  },
  {
    "objectID": "posts/P-values/index.html",
    "href": "posts/P-values/index.html",
    "title": "Instructor’s Orientation",
    "section": "",
    "text": "To judge from the tables of contents of the most widely used introductory statistics texts, there is strong consensus about the appropriate topics to include in a university-level introduction to statistics, a course we will label “Stat 101.” Yet there remains considerable dis-enchantment with Stat 101. Among these: a semester course is too crowded with topics; there is too much algebra; there is too strong an emphasis on p-values. Leading statistics educators, such as the 11 authors of the 2016 American Statistical Association Guidelines for Assessment and Instruction in Statistics Education (GAISE) College Report) point to stresses placed on the introductory course from two directions: first, the increasing coverage of statistics in grades 6-12 and, second, the emergence of data science as the paradigm for real-world professional work with data.\nThere has been incremental progress in addressing the shortcomings of Stat 101. Much of this progress fall under four themes.\nIn many ways, the free, online texts are the engine for innovation. They are typically produced by one or two statistics educators working independently of any publisher and motivated not to capture a paying market but to advance statistics education. In contrast, publishers are conservative in their choice of topics and technology, seeming with the goal of not alienating their current client base. [Notable exceptions include the Lock5 and Tintle texts.]{aside} Instructors recognize that the next edition of a textbook will be different only in superficial ways from the previous editions. One text, widely used in two-year colleges and broadly disparaged in other contexts, is in its thirteenth edition. The first edition appeared in 1980.\nOne of the most highly regarded statistics educators of the last 30 years, George Cobb, entitled a 2015 paper “Mere Renovation is Too Little Too Late: We Need to Rethink Our Undergraduate Curriculum from the Ground Up.” But ground-up innovation is problematic. It calls for heroic effort by statistics instructors to learn new topics and tools. More subtly, in order to teach a coherent course where topics build on one another and reinforce one another, instructors have to develop a feel for the connections among the initially unfamiliar topics.\nThose who advocate a ground-up innovation ultimately have to face an ethical formula: “Ought implies can.”. The innovator must reach past the arguments for innovation to demonstrate that a ground-up innovation is a feasible way to teach introductory statistics, including data science. In my view, one also has to recognize the constraints imposed by other aspects of the educational system. Although a one-semester statistics course is one of the most widespread requirements across university disciplines, there is little advocacy among non-statisticians for a heavier, longer requirement. Similarly, the successful statistics course cannot call for extensive pre- or co-requisites in computing or mathematics.\nLessons in Statistical Thinking is my attempt to go beyond outlining ground-up innovation to a living, breathing, complete course. The purpose of this Orientation is to help instructors understand the overall structure of the course, the reasons for including some topics and excluding others, and to highlight some of the innovations that make the course a feasible option for many statistics instructors."
  },
  {
    "objectID": "posts/P-values/index.html#improving-stat-101",
    "href": "posts/P-values/index.html#improving-stat-101",
    "title": "Instructor’s Orientation",
    "section": "Improving Stat 101",
    "text": "Improving Stat 101\nAn excellent summary of how to improve Stat 101 is provided by Jeff Witmer’s 2023 article in the Journal of Statistics and Data Science Education, “What should we do differently in STAT 101?” Witmer, one of the authors of the 2016 GAISE report, proposes fifteen changes for Stat 101, placing them in three categories:\n\nChanges you could make with little effort or planning.\nChanges that you could implement with the investment of a day or two of planning.\nChanges that require “quite a bit of planning but that are worth considering nonetheless.”\n\nWitmer provides a discussion of each of the fifteen changes, an extremely useful reference for the reader who might not be familiar with terms like causal diagrams, causal diagrams, effect size, etc. For ease of reference, I retain Witmer’s numbering system in the short summaries that follow.\nParadigm for statistical description\n\n\nUse the use the framework of response variable versus predictor variables. (Category a.) \n\n\nUse more modeling and estimation and less formal inference. (b.)\n\n\nTalk about effect size. (c.)\n\n\nUse logistic regression when modeling a categorical response variable with two levels. (b.)\n\n\nPresent causal diagrams. (c.)\n\n\nInclude more emphasis on prediction. (c.)\n\n\nBe explicit about how to adjust for a confounder and the Cornfield conditions. (c.)\n\n\nTeach Berkson’s paradox. In terms of causal diagrams, this corresponds to a collider. (c.)\n\n\nIn Lessons, I prefer “explanatory variable” rather than “predictor variable but there is no real difference.Changes concerning statistical inference\n\n\nReplace “statistically significant” with “statistically discernible.” (Category a.)\n\n\nReport p-values to only 2 or 3 decimal places. (a.)\n\n\nRefer to “conditions” that can potentially be checked rather than “assumptions” that may or may not be relevant to a real-world context. (a.)\n\n\nInclude “power” and other aspects of the Neymann-Pearson approach, rather than just Null Hypothesis Significance Testing. (b.)\n\n\nCover the inference of relative risk, e.g. \\(p_1 / p_2\\). Also, 9. consider teaching about paired proportions (b.)\n\n\n(continued) Show that a test statistic is an effect size multiplied by a sample size inflation. (c.) Example: The effect size corresponding to \\(R^2\\) is \\(R^2 / (1- R^2)\\) while the sample size inflation is \\(\\frac{n - (k+1)}{k}\\), that is, \\[F \\equiv \\frac{R^2}{1-R^2} \\frac{n - (k+1)}{k}\\ .\\]\n\n\nTeach about the perils of multiple testing and about the “garden of forking paths.” (c.)\n\n\nThe topics and practices in categories (a) and (b) fit easily into the 1-semester Stat 101. But many of the category-(c) topics require surgery on Stat 101: making space for new topics by removing or consolidating some traditional topics.\nLessons touches on all fifteen of Witmer’s suggested Stat 101 topics and practices. But Lessons does not merely swap out old topics for new. Rather, Lessons provides new foundations on which a modern statistical edifice can be constructed."
  },
  {
    "objectID": "posts/P-values/index.html#accessible-computing",
    "href": "posts/P-values/index.html#accessible-computing",
    "title": "Instructor’s Orientation",
    "section": "Accessible computing",
    "text": "Accessible computing\n\nLessons in Statistical Thinking is an introductory textbook that is, like most textbooks, oriented to students. Students come to the book with fresh minds, unaware of the history that leads some topics to be emphasized and others to be minimized or entirely excluded.\nFor instructors, however, that history plays an important role in setting attitudes and expectations. Instructors have already studied statistics—sometimes learning it from the textbook they teach from!—and have pre-formed opinions about which topics ought to be included in an introduction to statistics. This orientation seeks to explain to instructors the unconventional choices made in Lessons and highlight the important connections between subjects that might not yet be familiar to the instructor contemplating using Lessons as a course text."
  },
  {
    "objectID": "posts/P-values/index.html#stat-101",
    "href": "posts/P-values/index.html#stat-101",
    "title": "Instructor’s Orientation",
    "section": "Stat 101",
    "text": "Stat 101\nMany readers have had experience teaching a conventional introductory course. A first step in understanding the choices made in putting together the Lessons text is to become aware of the problems and deficiencies of the conventional approach. For brevity, we’ll use the name “Stat 101” to label the conventional practices that are so widely used that they can be taken as the consensus view.\nA widely held opinion among instructors is that there is already so much material in Stat 101 that there is no room for including even important new topics such as adjusting for covariates, causal inference, or data-science skills and techniques. This over-crowding becomes a deficiency in Stat 101 when you consider alternative approaches that can substantially streamline the Stat 101 material.\nAnother deficiency has to do with the emphasis on p-values found in Stat 101. Typically, about one-third of the course focuses on statistical tests. The American Statistical Association has called for a de-emphasis on p-values [CITATION HERE], recognizing the many ways they are routinely misused. For instance, the ASA statement states that “a p-value does not provide a good measure of evidence regarding a model or hypothesis.” This statement, widely accepted in the professional community, is incomprehensible to many Stat-101 teachers. (You can judge for yourself whether or not the ASA statement is reflected in the orientation of your own teaching.)\nThe 2016 American Statistical Association Guidelines for Assessment and Instruction in Statistics Education (GAISE) College Report) includes a section on “suggestions for topics that might be omitted from introductory statistics courses.” These include probability theory and drills with “\\(z\\)-, \\(t\\)-, \\(\\chi^2\\), and \\(F\\)-tables.” For instance, early discussions by the committee charged by the American Statistical Association with updating the GAISE report favored dividing the widely accepted requirement for statistics into two courses, one in statistics, the other in data science. My own opinion is that such an expansion of requirements is likely to be rejected as overly burdensome by many programs. this is both impractical and unnecessary. Few departments that\nFirst and foremost is the divergence of professional statistical practice from the topics of Stat 101. Quantities and methods routinely found in the applied research literature are often not even hinted at in Stat 101.\n: the intentional omission of many Stat-101 topics and the inclusion of other topics that never found space in the Stat-101 canon or that violated the ethos that historically that course.\n\nTests versus models"
  },
  {
    "objectID": "posts/P-values/index.html#p-values-and-causation",
    "href": "posts/P-values/index.html#p-values-and-causation",
    "title": "Instructor’s Orientation",
    "section": "P values and causation",
    "text": "P values and causation\nStatistics instructors very often have their knowledge base shaped by the introductory books from which they teach. What’s in the books, rather than what’s useful or needed, is the operational definition of the field. This book drops many traditional methods and introduces important methods used in contemporary work with data. A little background may help instructors new to this book in understanding two major decisions shaping that book. They both have to do with what is called “statistical inference,” reasoning from data to the broader world: first, minimizing the role in this book of p-values; second, taking causality seriously.\nThere are dozens of introductory statistics textbooks, many in their tenth or fifteenth editions. Unfortunately, the topics of traditional introductory statistics texts have long been obsolescent and fail to address the needs of the contemporary data scientist and decision-maker. The traditional canon stems from an influential 1925 book, Ronald Fisher’s Statistical Methods for Research Workers. Research workers of that era typically ran small benchtop or field experiments with a dozen or fewer observations on each of two treatments. A first task with such small data is to rule out the possibility that calculated differences might reflect only the accidental arrangement of numbers into groups. This task was given the odd name “significance testing” and is at the core of most statistics textbooks.\nUnfortunately, significance testing has little to do with the everyday meaning of “significant” as “important” or “relevant.” This article in the prestigious science journal Nature details the controversy. Figure 1 reproduces a cartoon from that article that puts the shortcomings of “statistical significance” in a historical context.\n\n\n\n\n\nFigure 1: A cartoon published along with an article in Nature, “Retire statistical significance,” showing this once-respected idea heading to the graveyard for outdated and misleading “scientific” concepts such as phlogiston and aether.\n\n\n\n\nObsession with the mantra, “Correlation is not causation,” is another sign of the obsolescence of the traditional introduction to statistics. Around 1910, the pioneers of statistics were the first to emphasize an important innovation in scientific method: the randomized controlled trial (RCT). Adoption of the RCT in the twentieth century put several branches of science on a new footing. And many statistics instructors see “correlation is not causation” as a slogan pointing to the genuine importance of RCTs. Unfortunately, the mantra has been over-interpreted to mean the impossibility of causal knowledge without RCTs, as in the following cartoon (Figure 2):\n\n\n\n\n\nFigure 2: XKCD’s take on correlation and causation.\n\n\n\n\nNowadays, when data are used to inform policy decisions in many areas, being statistically literate includes the need to make justifiable conclusions about causality. One approach to this was highlighted by the 2019 Nobel Prize in economics; breaking down complex issues of global poverty into smaller, more manageable questions where an RCT is feasible. Another approach, using “natural experiments” was honored by the 2021 Nobel Prize.\nThere is no Nobel in computer science. The equivalent is the Turing Award, which in 2011 was awarded for “fundamental contributions to … probabilistic and causal reasoning.” That such prestigious awards are being given in the last decade demonstrates how recent and how important causal reasoning is and why the fundamentals of causality ought to be part a modern introduction to statistics. Consequently, they are a major theme in these Lessons."
  },
  {
    "objectID": "posts/P-values/index.html#taken-out",
    "href": "posts/P-values/index.html#taken-out",
    "title": "Instructor’s Orientation",
    "section": "Taken out",
    "text": "Taken out\nTo set the stage, consider some of the ways Lessons is different from the conventional introductory statistics.\n\nLessons starts with a brief introduction to those elements of data science that provide access to the sorts of data commonly encountered today. It is here that computing is introduced. The new instructor will reasonably wonder whether data-science computing is accessible to her students. ?@sec-simplifying-computing provides the necessary introduction to instructors. Based on this introduction, an instructor can shape an informed opinion about accessibility.\nLessons uses statistical modeling as the core framework for quantifying patterns in data. The modeling approach has two advantages: it actually simplifies the presentation of the many tests found in a conventional introduction; and it provides the means to put covariation and multivariable thinking at the center of the course. (“Multivariable thinking” is one of the primary emphases in the 2016 American Statistical Association Guidelines for Assessment and Instruction in Statistics Education (GAISE) College Report.)"
  },
  {
    "objectID": "posts/P-values/index.html#foundations-in-lessons",
    "href": "posts/P-values/index.html#foundations-in-lessons",
    "title": "Instructor’s Orientation",
    "section": "Foundations in Lessons",
    "text": "Foundations in Lessons"
  },
  {
    "objectID": "posts/Graphics-and-history/index.html",
    "href": "posts/Graphics-and-history/index.html",
    "title": "Trade deficits and the Opium Wars",
    "section": "",
    "text": "Table 1: Annual exports and imports in the trade between England and the East Indies\n\n\nYear\nExports\nImports\n\n\n\n\n1700\n180\n460\n\n\n1701\n170\n480\n\n\n1702\n160\n490\n\n\n1703\n150\n500\n\n\n1704\n145\n510\n\n\n1705\n140\n525\n\n\n1706\n135\n550\n\n\n1707\n125\n565\n\n\n1708\n120\n580\n\n\n1709\n110\n590\n\n\n1710\n105\n625\n\n\n1711\n105\n650\n\n\n1712\n100\n680\n\n\n1713\n100\n710\n\n\n1714\n100\n725\n\n\n1715\n100\n755\n\n\n1716\n100\n795\n\n\n1717\n100\n825\n\n\n1718\n110\n860\n\n\n1719\n120\n880\n\n\n1720\n130\n900\n\n\n1721\n140\n925\n\n\n1722\n145\n940\n\n\n1723\n150\n955\n\n\n1724\n145\n960\n\n\n1725\n135\n965\n\n\n1726\n130\n970\n\n\n1727\n130\n975\n\n\n1728\n145\n980\n\n\n1729\n160\n980\n\n\n1730\n170\n975\n\n\n1731\n170\n980\n\n\n1732\n170\n980\n\n\n1733\n175\n982\n\n\n1734\n205\n985\n\n\n1735\n225\n987\n\n\n1736\n260\n985\n\n\n1737\n285\n982\n\n\n1738\n310\n980\n\n\n1739\n345\n980\n\n\n1740\n370\n980\n\n\n1741\n390\n978\n\n\n1742\n410\n975\n\n\n1743\n440\n975\n\n\n1744\n465\n978\n\n\n1745\n500\n980\n\n\n1746\n520\n980\n\n\n1747\n550\n960\n\n\n1748\n585\n960\n\n\n1749\n650\n955\n\n\n1750\n700\n940\n\n\n1751\n725\n910\n\n\n1752\n750\n880\n\n\n1753\n780\n850\n\n\n1754\n805\n835\n\n\n1755\n810\n830\n\n\n1756\n810\n830\n\n\n1757\n815\n850\n\n\n1758\n825\n890\n\n\n1759\n850\n945\n\n\n1760\n875\n1000\n\n\n1761\n910\n1100\n\n\n1762\n950\n1300\n\n\n1763\n1010\n1400\n\n\n1764\n1120\n1470\n\n\n1765\n1330\n1500\n\n\n1766\n1380\n1510\n\n\n1767\n1395\n1515\n\n\n1768\n1380\n1520\n\n\n1769\n1350\n1525\n\n\n1770\n1330\n1530\n\n\n1771\n1285\n1530\n\n\n1772\n1250\n1535\n\n\n1773\n1190\n1535\n\n\n1774\n1050\n1535\n\n\n1775\n925\n1535\n\n\n1776\n880\n1540\n\n\n1777\n860\n1545\n\n\n1778\n845\n1545\n\n\n1779\n840\n1550\n\n\n1780\n840\n1550\n\n\n\n\n\n\n\nWilliam Playfair (1759-1823) is credited as the inventor of novel graphical forms in which data values are presented graphically, rather than as numbers or text. To illustrate, consider the data from the 1700s (Table 1) that Playfair turned into a picture.\nPlayfair’s innovation, as in Figure 1, was successful because it was powerful. The pattern that is latent in the data frame becomes visually obvious to the human viewer. The picture shows not only the trade values each year but also the trends across the decades.\n\n\n\n\n\nFigure 1: William Playfair’s 1801 presentation of year-by-year data on trade between England and the East Indies. Source: University of Pennsylvania Libraries\n\n\n\n\nThe American revolution is marked out by the graph; you can see the steady fall in English exports from 1775-1780, corresponding to the American boycott during the revolution. Exports pick up again after the revolution, but English imports increase even more rapidly, leading to a steady expanding trade deficit by 1800. The historical consequences of this deficit are profound with continuing implications.\nThe historically minded reader should note that the the growing English trade deficit in 1800 had important historical consequences that are still playing out in the 2020s.\n\n“The roots of the Opium War (or First China War) lay in a trade dispute between the British and the Chinese Qing Dynasty. By the start of the 19th century, the trade in Chinese goods such as tea, silks and porcelain was extremely lucrative for British merchants. The problem was that the Chinese would not buy British products in return. They would only sell their goods in exchange for silver, and as a result large amounts of silver were leaving Britain.\n\n\n“In order to stop this, the East India Company and other British merchants began to smuggle Indian opium into China illegally, for which they demanded payment in silver. This was then used to buy tea and other goods. By 1839, opium sales to China paid for the entire tea trade.”—National Army Museum\n\nBy 1839, facing millions of Chinese citizens addicted to opium, the Chinese government began to successfully intervene to stop the smuggling, much as the US and other countries still do to curtail illegal drug trade. The English government found this unacceptable and went to war with China. With less advanced military technology, that war was a disaster for the Chinese and led to the impoverishment of the country and the forced transfer of territory—such as Hong Kong—to England. In the 1930s, a greatly weakened China fell victim to Japanese imperialism, then communist revolution. Only in the 1990s did China start to overcome extreme poverty. In the 2000s, the Chinese are still seeking to restore the economic pre-eminance they held before the eruption of the English opium trade."
  },
  {
    "objectID": "posts/Populations/index.html",
    "href": "posts/Populations/index.html",
    "title": "Instructor’s Orientation",
    "section": "",
    "text": "DRAFT: Explain why we don’t push the distinction between population and sample.\nInstructors with previous experience may think of the word “population.” This is used in several ways. The applied scientist or statistician may use the word “population” to refer to the entire class of entities about which a claim is entitled to be regarded as valid. For instance, “the population is people over 60 with end-stage congestive heart failure.”\nIn theoretical statistics, a “population” is an infinite, inexhaustible source of specimens. As a theoretical entity, a population possesses “parameters” which, as can happen only in theory, are fixed and unchanging. Ronald Fisher introduced this theoretical entity for a particular purpose: providing a framework in which to define probability as a ratio of counts—the so-called “frequentist” interpretation. The frequentist perspective in statistics has been a fecund source of mathematical theorems. It has also been a source of mischief by defining probability in a way that excludes the Bayesian methods that are so important in engineering and decision making.\nMuch of classical theoretical statistics was concerned with questions that are no longer of relevance. For example given a population with parameters “mean” and “standard deviation,” which formulas for computing a numerical values of a sample statistic will make the most efficient use of data while providing an unbiased estimate. “Efficient” here means lowest variance. But contemporary machine learning techniques take the perspective that it can be worth trading off bias for reduced variance.\nI encourage instructors to continue using “population” in the informal sense of an applied scientist, but to avoid spending time on the theoretical statistics entity. This means, for example, that there is no need to distinguish between “parameters” and “statistics,” no point in emphasizing the differences between \\(\\mu\\) and \\(\\bar{x}\\) or s2 and \\(\\sigma^2\\). Modelers use the word “parameter” in an entirely different sense than theoretical statisticians, and much of statistical thinking is about constructing and interpreting models.\nInsofar as you need to provide a definition of “statistical inference,” you don’t communicate much by saying, “Reasoning from the sample to the (theoretical) population.” Instead, describe inference as establishing the uncertainty in what we think we know from our sample."
  }
]