[
  {
    "objectID": "posts/ChatGPT-and-R/index.html",
    "href": "posts/ChatGPT-and-R/index.html",
    "title": "Can ChatGPT help in interpreting or correcting R commands?",
    "section": "",
    "text": "The results are pretty good, to judge from a couple of initial trials. But, clearly, you cannot be completely oblivious about how R works to make sense of the results.\nChatGPT: This R code runs a linear regression model on the mtcars dataset, which is a built-in dataset in R containing information about 32 different cars. The model aims to predict the miles per gallon (mpg) of a car based on its horsepower (hp) and number of cylinders (cyl).\nHere’s a breakdown of what each part of the code means:\nTogether, the code lm(mpg ~ hp + cyl, data=mtcars) fits a linear regression model with mpg as the response variable and hp and cyl as the predictors, using the mtcars dataset. The output of the lm() function will include information about the coefficients of the model, as well as measures of model fit and statistical significance.\nChatGPT: This R code does not work because there is a syntax error in the formula provided to the lm() function. Specifically, there is an extra comma (,) after the cy variable, which causes R to interpret the formula incorrectly.*\nThe correct syntax for the lm() function requires that the formula is separated from the data argument using the data= parameter. Here’s the corrected code:\nThis code specifies the formula mpg ~ hp + cyl as before, and the data= parameter specifies that the data for the analysis should be taken from the mtcars dataset."
  },
  {
    "objectID": "posts/ChatGPT-and-R/index.html#comments",
    "href": "posts/ChatGPT-and-R/index.html#comments",
    "title": "Can ChatGPT help in interpreting or correcting R commands?",
    "section": "Comments",
    "text": "Comments\n\nChatGPT has a preferred set of words for describing computation. All the ones I see here are legitimate, but they don’t reflect the usage in the Math 300 textbook. Examples:\n\n“parameter”—We tend to use “argument” or “input,” as in “The data= input to lm().”\n“formula”—We use the phrase “tilde expression.” That’s because our course is mathematical in nature. In mathematics, “formula” refers to an arithmetical combination of quantities, as in \\(mx + b\\). If you want to construct a linear regression model along the lines of \\(mx + b\\), your tilde expression will look like y ~ 1 + x, assuming that y is the name of your response variable and x is the name of the explanatory variable.\n\nIn the 2nd example, ChatGPT didn’t mention that cy is not a variable in the mtcars data frame. It did, however, correct it to cyl in the example code."
  },
  {
    "objectID": "posts/Stat-and-non-stat-questions/index.html",
    "href": "posts/Stat-and-non-stat-questions/index.html",
    "title": "Statistical or not?",
    "section": "",
    "text": "Some questions and answers are statistical in nature, and others not. How can you know which is which?\nI’m thinking of starting my statistics class today by asking my students their ages. Next, a mathematical question: Is everyone in the class of the same age? That’s an easy question to answer, of course. The question is mathematical in the sense that it involves mathematical stuff—numbers—and there is a single, correct answer.\nI expect that not everyone will be the same age. That is, age varies among the members of the class. Now the question is: Why does age vary? When a statistical answer will be in the form, “Age varies because this other quantity or characteristic varies among the group.” This style of answer attempts to account for the variation in age by the variation in other things. For instance, the students don’t all have the same major, and different majors may place the stat course at different points in the students’ studies. Or, students entered the academy at different points in time. Presumably there would be less variation in age if we adjusted for each student’s age at matriculation.\nAnother question. Why are the statistics students typically about 20 years old? The answer that occurs to me is that almost all of the students are from the US and in the US students enter first grade around age 6 and continue through 12 years of primary and secondary education. Then they go to college and take stats in their second or third years. Few students have a gap between high-school graduation and college matriculation.\nThis answer is not statistical in nature. A statistical answers explains variation in one characteristic by appealing to the simultaneous variation in other characteristics. May statisticians may quarrel with my using words like “why” and “because.” But that is not central to the matter. What’s central to a question and answer being statistical in nature is that the question is about variation in a characteristic and the answer pairs that variation with variation in other characteristics.\n\nExample:\n\nNot a statistical question or answer: “Why is the sky blue?” Because blue light from the sun is scattered by small particles in the atmosphere.\nA statistical question and answer: “Why does the color of the sky change from time to time?” Because the weather is sometimes cloudy or foggy, and because the sun is sometimes high in the sky, sometimes rising or setting, and sometimes not visible at all."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300 Blog",
    "section": "",
    "text": "Lesson 25 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nprediction\n\n\nestimation\n\n\nintervals\n\n\nprobability distribution\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nCase study: Prediction vs estimation\n\n\n\n\n\n\n\nSAT\n\n\nprediction\n\n\nestimation\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 28 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\ncovariate\n\n\nadjustment\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nBayesian updating for self-driving cars\n\n\n\n\n\n\n\nself-driving cars\n\n\nprediction\n\n\nbelief\n\n\nBayes\n\n\nprobability distribution\n\n\nlikelihood\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 24 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\neffect size\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 23 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nconfidence intervals\n\n\nprecision\n\n\naccuracy\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 22 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nsignal and noise\n\n\nsampling variability\n\n\nsampling variance\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 21 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nsignal and noise\n\n\nlinear model shapes\n\n\nmodel function\n\n\nmodel value\n\n\nresidual\n\n\nresponse value\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 20 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nvariance\n\n\nDAGs\n\n\ncausality\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nLesson 19 take-aways\n\n\n\n\n\n\n\nClass sessions\n\n\nvariance\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nStatistical or not?\n\n\n\n\n\n\n\nvariation\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\nCan ChatGPT help in interpreting or correcting R commands?\n\n\n\n\n\n\n\nR\n\n\nChatGPT\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nDaniel Kaplan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Notes, comments, tools, and suggestions relating to Math 300Z, the prototype run of the core statistics course at USAFA.\nThis blog contains Danny Kaplan’s personal opinions. Nothing on this blog necessarily reflects the views or policies of the US Air Force Academy, the Department of Defense, or the US government generally."
  },
  {
    "objectID": "posts/Takehomes-19/index.html",
    "href": "posts/Takehomes-19/index.html",
    "title": "Lesson 19 take-home points",
    "section": "",
    "text": "Measure variation using the “variance.” In R,\n\n\nGalton %>% summarize(vh = var(height))\n\n\nResponse variable will always be quantitative/numerical in a regression model.\nExpect graphics in our section to always have the response variable on the vertical axis."
  },
  {
    "objectID": "posts/Takehomes-19/index.html#administrative",
    "href": "posts/Takehomes-19/index.html#administrative",
    "title": "Lesson 19 take-home points",
    "section": "Administrative",
    "text": "Administrative\n\nDO THE DAILY WORKSHEET! You’ll fall behind if you don’t.\nBookmark this class site in your browser. That way you don’t have to detour through Teams.\nUse the Z-section project on posit.cloud. Bring in the worksheet using\n\n\nmath300::get_lesson_worksheet( LESSON NUMBER )"
  },
  {
    "objectID": "posts/Takehomes-20/index.html",
    "href": "posts/Takehomes-20/index.html",
    "title": "Lesson 20 take-home points",
    "section": "",
    "text": "Measure variation using the “variance.” In R,\n\n\nGalton %>% summarize(vh = var(height))\n\n       vh\n1 12.8373\n\n\nInterpretation: The heights of the people in the `Galton` data frame vary. The amount of this variability is the variance: 12.8 square-inches. In less strange units, the standard deviation is $\\sqrt{12.8 \\text{square-inches}} = 3.6$ inches\n\nThe most common action for the rest of this course will be to create a model and summarize it.\nExample: lm(height ~ mother + father, data=Galton)\n\nheight ~ mother + father is a tilde expression that specifies the roles of variables in the model. height is the response variable. mother and father are the explanatory variables.\ndata = Galton tells lm() to use the Galton data frame to construct the model corresponding to the tilde expression.\n\nExample: Summarizing functions R2() and conf_interval()\n\n\nlm(height ~ mother + father, data=Galton) |> R2()\n\n    n k  Rsquared       F     adjR2 p df.num df.denom\n1 898 2 0.1088952 54.6856 0.1069039 0      2      895\n\n\nInterpretation: `mother` and `father` jointly explain about 10% of the variance in the `height` of their adult children.\n\nlm(height ~ mother + father, data=Galton) |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) 13.9   22.3   30.8  \n2 mother       0.187  0.283  0.380\n3 father       0.290  0.380  0.470\n\n\nInterpretation: The model equation for a person's height in inches is, according to this model:\n\\[\\text{person's height} = 22.3 + 0.283\\ \\mathtt{mother} + 0.380\\ \\mathtt{father}\\] 3. Regression models (such as those built by lm()) quantify how to account-for/explain the variation in the response variable in terms of the variation in the explanatory variables."
  },
  {
    "objectID": "posts/Takehomes-20/index.html#administrative",
    "href": "posts/Takehomes-20/index.html#administrative",
    "title": "Lesson 20 take-home points",
    "section": "Administrative",
    "text": "Administrative\n\nDO THE DAILY WORKSHEET! You’ll fall behind if you don’t.\nBookmark this class site in your browser. That way you don’t have to detour through Teams.\nUse the Z-section project on posit.cloud. Bring in the worksheet using\n\n\nmath300::get_lesson_worksheet( LESSON NUMBER )"
  },
  {
    "objectID": "posts/Takehomes-20/index.html#new-topics",
    "href": "posts/Takehomes-20/index.html#new-topics",
    "title": "Lesson 20 take-home points",
    "section": "New topics",
    "text": "New topics\n\nAccount-for/explain. Often when we use the word “explain” we mean to suggest a causal connection. For instance, this randomized clinical trial established that a particular blood-pressure drug leads to lower blood pressure, that is, it causes the blood pressure to go down.\nWhen we say that “A causes B,” we don’t necessarily mean that A is the complete and total explanation for B. More often, we mean that “A contributes in some way to the value of B.” For instance, “high blood pressure increases mortality” does not mean that high blood pressure is the sole determinant of mortality. Instead, it means that high blood pressure contributes to an increased risk of mortality.\nA DAG (Directed acyclic graph—unnecessarily intimidating name!) is a way of encoding a hypothesis of what causes what in a system. We discussed the system involving treating a battlefield casualty with a tourniquet. (Link to in-class activity.) The system—a “system” is a collection of components—involved USE of a tourniquet, SEVERITY of injury, staying alive long enough for ADMISSION to hospital, and post-hospital SURVIVAL. Common sense suggests some causal connections:\n\nSEVERITY \\(\\longrightarrow\\) USE, since tourniquets are used for severe injuries involving bleeding from a limb.\nSEVERITY \\(\\longrightarrow\\) ADMISSION. The more severe the injury, the less likely the soldier is to reach the hospital.\nADMISSION \\(\\longrightarrow\\) SURVIVAL. There would be little point to having hospitals if they didn’t contribute to survival!\nSEVERITY \\(\\longrightarrow\\) SURVIVAL. Even if reaching the hospital, the soldier’s survival is in part determined by the SEVERITY of the injury.\n\nOther link were more hypothetical:\n\nUSE \\(\\longrightarrow\\) ADMISSION\n\n\nUSE \\(\\longrightarrow\\) SURVIVAL\n\nA DAG describes the hypothesized causal links among all the system components.\n\n\n\n\n\n\n\n\n\n\n\n\nMore detail\n\n\n\nThe clinical study looking at the relationship between tourniquet use and survival included only those patients admitted to the hospital, not the ones who died before admission. Their data did not demonstration a relationship between USE and SURVIVAL. That is to say, the hypothesized DAG as a result of the study was this:\n\n\n\n\n\nIn this second DAG, there is no causal connection between USE and SURVIVAL.\nA critique of the clinical trial is that it didn’t consider ADMISSION, that is, both those soldiers who reached the hospital and those who didn’t. Consequently, even if there were no direct link between USE and SURVIVAL, a more complete DAG includes a possible indirect path, as in this third DAG:\n\n\n\n\n\nThe path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL could not be demonstrated by the (imperfect) clinical study, even though it might well exist. Any decision about the efficacy of tourniquets would need to consider the possible path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL.\nHere’s a paradox: Effective use of tourniquets increases hospital mortality rates. Explain why this says something positive about tourniquet use."
  },
  {
    "objectID": "posts/Takehomes-20/index.html#not-gotten-to-in-class",
    "href": "posts/Takehomes-20/index.html#not-gotten-to-in-class",
    "title": "Lesson 20 take-home points",
    "section": "NOT gotten to in class",
    "text": "NOT gotten to in class\n\nUse of sample() and dag_draw() with DAGs.\nWhat is a “random trial”\nHow (and why) to automate replication of random trials.\n\nYou can learn these things from the text and the worksheet for Lesson 20."
  },
  {
    "objectID": "posts/Takeaways-20/index.html",
    "href": "posts/Takeaways-20/index.html",
    "title": "Lesson 20 take-aways",
    "section": "",
    "text": "Measure variation using the “variance.” In R,\n\n\n\nGalton %>% summarize(vh = var(height))\n\n\n\n \n  \n    vh \n  \n \n\n  \n    12.8 \n  \n\n\n\n\n\nInterpretation: The heights of the people in the Galton data frame vary. The amount of this variability is the variance: 12.8 square-inches. In less strange units, the standard deviation is \\(\\sqrt{12.8\\ \\text{square-inches}} = 3.6\\) inches\n\n\nThe most common action for the rest of this course will be to create a model and summarize it.\nExample: lm(height ~ mother + father, data=Galton)\n\nheight ~ mother + father is a tilde expression that specifies the roles of variables in the model. height is the response variable. mother and father are the explanatory variables.\ndata = Galton tells lm() to use the Galton data frame to construct the model corresponding to the tilde expression.\n\nExample: Summarizing functions R2() and conf_interval()\n\n\n\nlm(height ~ mother + father, data=Galton) |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    898 \n    2 \n    0.109 \n    54.7 \n    0.107 \n    0 \n    2 \n    895 \n  \n\n\n\n\nInterpretation: mother and father jointly explain about 10% of the variance in the height of their adult children.\n\nlm(height ~ mother + father, data=Galton) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    13.900 \n    22.300 \n    30.80 \n  \n  \n    mother \n    0.187 \n    0.283 \n    0.38 \n  \n  \n    father \n    0.290 \n    0.380 \n    0.47 \n  \n\n\n\n\n\nInterpretation: The model equation for a person’s height in inches is, according to this model:\n\\[\\text{person's height} = 22.3 + 0.283\\ \\mathtt{mother} + 0.380\\ \\mathtt{father}\\]\n\n\nRegression models (such as those built by lm()) quantify how to account-for/explain the variation in the response variable in terms of the variation in the explanatory variables."
  },
  {
    "objectID": "posts/Takeaways-20/index.html#new-topics",
    "href": "posts/Takeaways-20/index.html#new-topics",
    "title": "Lesson 20 take-aways",
    "section": "New topics",
    "text": "New topics\n\nAccount-for/explain. Often when we use the word “explain” we mean to suggest a causal connection. For instance, this randomized clinical trial established that a particular blood-pressure drug leads to lower blood pressure, that is, it causes the blood pressure to go down.\nWhen we say that “A causes B,” we don’t necessarily mean that A is the complete and total explanation for B. More often, we mean that “A contributes in some way to the value of B.” For instance, “high blood pressure increases mortality” does not mean that high blood pressure is the sole determinant of mortality. Instead, it means that high blood pressure contributes to an increased risk of mortality.\nA DAG (Directed acyclic graph—unnecessarily intimidating name!) is a way of encoding a hypothesis of what causes what in a system. We discussed the system involving treating a battlefield casualty with a tourniquet. (Link to in-class activity.) The system—a “system” is a collection of components—involved USE of a tourniquet, SEVERITY of injury, staying alive long enough for ADMISSION to hospital, and post-hospital SURVIVAL. Common sense suggests some causal connections:\n\nSEVERITY \\(\\longrightarrow\\) USE, since tourniquets are used for severe injuries involving bleeding from a limb.\nSEVERITY \\(\\longrightarrow\\) ADMISSION. The more severe the injury, the less likely the soldier is to reach the hospital.\nADMISSION \\(\\longrightarrow\\) SURVIVAL. There would be little point to having hospitals if they didn’t contribute to survival!\nSEVERITY \\(\\longrightarrow\\) SURVIVAL. Even if reaching the hospital, the soldier’s survival is in part determined by the SEVERITY of the injury.\n\nOther link were more hypothetical:\n\nUSE \\(\\longrightarrow\\) ADMISSION\n\n\nUSE \\(\\longrightarrow\\) SURVIVAL\n\nA DAG describes the hypothesized causal links among all the system components.\n\n\n\n\n\n\n\n\n\n\n\n\nMore detail\n\n\n\nThe clinical study looking at the relationship between tourniquet use and survival included only those patients admitted to the hospital, not the ones who died before admission. Their data did not demonstration a relationship between USE and SURVIVAL. That is to say, the hypothesized DAG as a result of the study was this:\n\n\n\n\n\nIn this second DAG, there is no causal connection between USE and SURVIVAL.\nA critique of the clinical trial is that it didn’t consider ADMISSION, that is, both those soldiers who reached the hospital and those who didn’t. Consequently, even if there were no direct link between USE and SURVIVAL, a more complete DAG includes a possible indirect path, as in this third DAG:\n\n\n\n\n\nThe path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL could not be demonstrated by the (imperfect) clinical study, even though it might well exist. Any decision about the efficacy of tourniquets would need to consider the possible path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL.\nHere’s a paradox: Effective use of tourniquets increases hospital mortality rates. Explain why this says something positive about tourniquet use."
  },
  {
    "objectID": "posts/Takeaways-20/index.html#not-gotten-to-in-class",
    "href": "posts/Takeaways-20/index.html#not-gotten-to-in-class",
    "title": "Lesson 20 take-aways",
    "section": "NOT gotten to in class",
    "text": "NOT gotten to in class\n\nUse of sample() and dag_draw() with DAGs.\nWhat is a “random trial”\nHow (and why) to automate replication of random trials.\n\nYou can learn these things from the text and the worksheet for Lesson 20."
  },
  {
    "objectID": "posts/Takeaways-19/index.html",
    "href": "posts/Takeaways-19/index.html",
    "title": "Lesson 19 take-aways",
    "section": "",
    "text": "Measure variation using the “variance.” In R,\n\n\nGalton %>% summarize(vh = var(height))\n\n\nResponse variable will always be quantitative/numerical in a regression model.\nExpect graphics in our section to always have the response variable on the vertical axis."
  },
  {
    "objectID": "posts/Takeaways-19/index.html#administrative",
    "href": "posts/Takeaways-19/index.html#administrative",
    "title": "Lesson 19 take-aways",
    "section": "Administrative",
    "text": "Administrative\n\nDO THE DAILY WORKSHEET! You’ll fall behind if you don’t.\nBookmark this class site in your browser. That way you don’t have to detour through Teams.\nUse the Z-section project on posit.cloud. Bring in the worksheet using\n\n\nmath300::get_lesson_worksheet( LESSON NUMBER )"
  },
  {
    "objectID": "posts/Takeaway-21/index.html",
    "href": "posts/Takeaway-21/index.html",
    "title": "Lesson 21 take-aways",
    "section": "",
    "text": "Figure 1: A drawing of dag06, to use as an example.\n\n\n\n\nDAGs (“directed acyclic graphs”) have three properties, all of which are essential for representing causality.\n\nThey are “graphs” in the technical vocabulary of mathematics. That is, they consist of nodes and edges. Each edge connects two (and only two) of the nodes. In Figure 1 there are four nodes, labeled a, b, c, and d. Coincidentally, there are also four edges, one of which is c\\(\\longrightarrow\\)d.\nEvery edge is directed, that is, it points from a source node to a target node. Think of the directed edge as if it were a pipe carrying causal “fluid” from the source to the target. All the pipes in a graph are one-way only. In Figure 1 each of the edges is an arrow. The edge a\\(\\longrightarrow\\)b means that causal “fluid” can flow from a to b, but not from b to a.\nThere are no loops in the fluid flow (that is, there are no “cycles” of flow). In the name DAG, this is the meaning of the A: “acyclic,” meaning “no cycles.”\n\n\nExercise: Figure 1 is a DAG. By reversing the direction of a particular edge, you can modify the graph so that it has a cycle of flow. Which edge will do the job.\nExercise: Figure 1 does not have an edge connecting c and a. Add in that edge, directing it so that the new graph is still acyclic.\n\nStarting in Lesson 30, we will get into the ways to use DAGs in order to select explanatory variables that produce a model that is a faithful representation of the causal flows.\nFor the next few lessons, we will use DAGs for another purpose: to simulate data and make it easy to conduct random trials.\nOther than for teaching purposes (as in (3)), the role of DAGs in statistics and data analysis is to encode hypotheses about how elements of a system might be connected. Usually, you work with DAGs that reflect your and your colleague’s beliefs about how things are connected in the real world. Of course, believing a hypothesis does not make it true. Think of a DAG as a piece of fiction. Sometimes fiction is close to real life, and sometimes not. Both situations have their purposes for story-telling."
  },
  {
    "objectID": "posts/Takeaway-21/index.html#lesson-21",
    "href": "posts/Takeaway-21/index.html#lesson-21",
    "title": "Lesson 21 take-aways",
    "section": "Lesson 21",
    "text": "Lesson 21\n\nIt is helpful to think of any response variable as a combination of “signal” and “noise.” The signal reflects how the explanatory variables are related to the response. The noise is the unexplained part of the response variable. More precisely, the “noise” is that part that we do not care to explain in terms of relationships to other variables.\nThe idea that there is always noise in the response variable allows us to train models that do not go through every (or even any) data point. This enables us to claim that simple shapes of models can be good representations of relationships among variables.\nIn Math 300Z, with few exceptions we will work with models that have one or two explanatory variables. A nicer feature of such models is that we can draw a graphic of the model using just two or three aesthetics.\n\naes(y= ), the vertical axis: always will be assigned to the response variable. (This is a Math 300Z convention, and a good one, but not universal.)\naes(x= ), the horizontal axis: the first explanatory variable will be assigned to this.\naes(color= ) if there is a second explanatory variable, it will be assigned to color.\n\nThe model_plot() function will take care of all this assignment of variables to aesthetics.\nSince explanatory variables can be either categorical or quantitative, there are only a handful of model shapes we need to deal with. (The response variable is always quantitative.) These are enumerated in the Instructor’s notes for Lesson 21.\nAn excellent type of exam question would show you the graph of a model and ask you to identify whether there is one or two explanatory variables, and what type(s) it (they) are: quantitative or categorical.\nThere is also a role in statistics for models that have zero explanatory variables. The tilde expression for such models (letting y be the response variable) is y ~ 1. We have not yet discussed what the use is of such models.\nThe data used to build a regression model is called the training data. It is a data frame containing both the response variable and any explanatory variables. Once a model is built, we often run the rows of the training data through the model function. Doing this divides the response variable values into two components:\n\nThe signal, which is the model function output for each row of the training data. We call this the “model value.”\nThe noise, which for each row of the training data is the difference between the value of the response variable (the “response value”) and the “model value.” This difference—one for each row of the training data—is called the “residual”.\n\nRemember this simple relationship:\n\nresponse value = model value + residual\n\nOften we will need to measure how big these three things are. We use the variance as the measure of “how big.” The variance is only one of many possible ways to quantify “how big.” But it has the great advantage that\n\nvar(response value) = var(model value) + var(residual)"
  },
  {
    "objectID": "posts/Takeaway-21/index.html#new-topics",
    "href": "posts/Takeaway-21/index.html#new-topics",
    "title": "Lesson 21 take-aways",
    "section": "New topics",
    "text": "New topics\n\nAccount-for/explain. Often when we use the word “explain” we mean to suggest a causal connection. For instance, this randomized clinical trial established that a particular blood-pressure drug leads to lower blood pressure, that is, it causes the blood pressure to go down.\nWhen we say that “A causes B,” we don’t necessarily mean that A is the complete and total explanation for B. More often, we mean that “A contributes in some way to the value of B.” For instance, “high blood pressure increases mortality” does not mean that high blood pressure is the sole determinant of mortality. Instead, it means that high blood pressure contributes to an increased risk of mortality.\nA DAG (Directed acyclic graph—unnecessarily intimidating name!) is a way of encoding a hypothesis of what causes what in a system. We discussed the system involving treating a battlefield casualty with a tourniquet. (Link to in-class activity.) The system—a “system” is a collection of components—involved USE of a tourniquet, SEVERITY of injury, staying alive long enough for ADMISSION to hospital, and post-hospital SURVIVAL. Common sense suggests some causal connections:\n\nSEVERITY \\(\\longrightarrow\\) USE, since tourniquets are used for severe injuries involving bleeding from a limb.\nSEVERITY \\(\\longrightarrow\\) ADMISSION. The more severe the injury, the less likely the soldier is to reach the hospital.\nADMISSION \\(\\longrightarrow\\) SURVIVAL. There would be little point to having hospitals if they didn’t contribute to survival!\nSEVERITY \\(\\longrightarrow\\) SURVIVAL. Even if reaching the hospital, the soldier’s survival is in part determined by the SEVERITY of the injury.\n\nOther link were more hypothetical:\n\nUSE \\(\\longrightarrow\\) ADMISSION\n\n\nUSE \\(\\longrightarrow\\) SURVIVAL\n\nA DAG describes the hypothesized causal links among all the system components.\n\n\n\n\n\n\n\n\n\n\n\n\nMore detail\n\n\n\nThe clinical study looking at the relationship between tourniquet use and survival included only those patients admitted to the hospital, not the ones who died before admission. Their data did not demonstration a relationship between USE and SURVIVAL. That is to say, the hypothesized DAG as a result of the study was this:\n\n\n\n\n\nIn this second DAG, there is no causal connection between USE and SURVIVAL.\nA critique of the clinical trial is that it didn’t consider ADMISSION, that is, both those soldiers who reached the hospital and those who didn’t. Consequently, even if there were no direct link between USE and SURVIVAL, a more complete DAG includes a possible indirect path, as in this third DAG:\n\n\n\n\n\nThe path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL could not be demonstrated by the (imperfect) clinical study, even though it might well exist. Any decision about the efficacy of tourniquets would need to consider the possible path USE \\(\\longrightarrow\\) ADMISSION \\(\\longrightarrow\\) SURVIVAL.\nHere’s a paradox: Effective use of tourniquets increases hospital mortality rates. Explain why this says something positive about tourniquet use."
  },
  {
    "objectID": "posts/Takeaway-21/index.html#not-gotten-to-in-class",
    "href": "posts/Takeaway-21/index.html#not-gotten-to-in-class",
    "title": "Lesson 21 take-aways",
    "section": "NOT gotten to in class",
    "text": "NOT gotten to in class\n\nUse of sample() and dag_draw() with DAGs.\nWhat is a “random trial”\nHow (and why) to automate replication of random trials.\n\nYou can learn these things from the text and the worksheet for Lesson 20."
  },
  {
    "objectID": "posts/Takeaways-22/index.html",
    "href": "posts/Takeaways-22/index.html",
    "title": "Lesson 22 take-aways",
    "section": "",
    "text": "A DAG (directed acyclic graph) is a way of representing a hypothesis or belief or possibility to be considered of the causal connections among variables.\n\n\n\n\n\n\n\nFigure 1: A drawing of dag06, to use as an example.\n\n\n\n- DAGs consist of **nodes**---`a`, `b`, `c`, and `d` above, each representing a variable.\n- Any pair of nodes can be connected (or not!) by a **directed edge**. The edge means that the two variables are causally connected. The one-way flow of influence from one variable to the other is indicated by the direction.\n- The **A** in D**A**G stands for \"acyclic,\" meaning that there are no loops (cycles) in the flow.\n\nWe will use DAGs for three purposes:\n\nTo encapsulate hypotheses about causal connections in a variety of real-world settings.\nAs a means to reason about which covariates should and should not be included in a model. (Lesson 30)\nTo generate data from a source whose mechanism is exactly known. This will allow us to learn how and how much we can learn from data. We can then carry these lessons to real situations where the mechanism is only hypothetically known.\n\nThe software for our use of DAGs is built in to the {math300} package. This includes:\n\nAbout a dozen different DAGs providing examples of different sorts of causal connection. This includes those named dag01 through dag12, which are meant to be schematic (abstract). Other built-in DAGs encode hypotheses about causal connections in real-world systems.\nsample(dag01, size=10) generates data.\ndag_draw(dag01) draws a picture of the graph.\nprint(dag01) shows the formulas used in the generation of simulated data.\nIn Lesson 32 (about experimentation) you will see how to change a DAG using dag_intervene().\ndag_make(..formulas..) creates a DAG, but you won’t have much occasion to use it."
  },
  {
    "objectID": "posts/Takeaways-22/index.html#lesson-22",
    "href": "posts/Takeaways-22/index.html#lesson-22",
    "title": "Lesson 22 take-aways",
    "section": "Lesson 22",
    "text": "Lesson 22\n\nOften, the data we have at hand is a sample of specimens from a much larger population of objects (called a “population”). We store the sample’s observations in a data frame.\nEven when the data are a census, we often analyze them as if they are a sample from a population.\nA statistical point of view is that the data in our data frame are merely one sample collected at random from the population. Consequently, we imagine for the purpose of constructing many statistical methods, that there are infinitely many other samples that are equivalent to the one we have but which just happened not to be selected.\nSample statistics are numbers that we calculate from our sample. For us in Math 300Z, such statistics will typically be coefficients from a model (but we will also use some other sample statistics).\nOwing to the randomness involved in collecting our sample, we regard any sample statistic as a random draw from a population of sample statistics that could have been computed on other samples (as in (3)). That is, every sample statistic is a combination of “signal” and “noise.”\nSince our sample statistic includes noise, it is appropriate to quantify how much noise there is. Knowing this can, for example, enable us to decide whether two different samples come from different populations or not."
  },
  {
    "objectID": "posts/Takeaways-21/index.html",
    "href": "posts/Takeaways-21/index.html",
    "title": "Lesson 21 take-aways",
    "section": "",
    "text": "Figure 1: A drawing of dag06, to use as an example.\n\n\n\n\nDAGs (“directed acyclic graphs”) have three properties, all of which are essential for representing causality.\n\nThey are “graphs” in the technical vocabulary of mathematics. That is, they consist of nodes and edges. Each edge connects two (and only two) of the nodes. In Figure 1 there are four nodes, labeled a, b, c, and d. Coincidentally, there are also four edges, one of which is c\\(\\longrightarrow\\)d.\nEvery edge is directed, that is, it points from a source node to a target node. Think of the directed edge as if it were a pipe carrying causal “fluid” from the source to the target. All the pipes in a graph are one-way only. In Figure 1 each of the edges is an arrow. The edge a\\(\\longrightarrow\\)b means that causal “fluid” can flow from a to b, but not from b to a.\nThere are no loops in the fluid flow (that is, there are no “cycles” of flow). In the name DAG, this is the meaning of the A: “acyclic,” meaning “no cycles.”\n\n\nExercise: Figure 1 is a DAG. By reversing the direction of a particular edge, you can modify the graph so that it has a cycle of flow. Which edge will do the job.\nExercise: Figure 1 does not have an edge connecting c and a. Add in that edge, directing it so that the new graph is still acyclic.\n\nStarting in Lesson 30, we will get into the ways to use DAGs in order to select explanatory variables that produce a model that is a faithful representation of the causal flows.\nFor the next few lessons, we will use DAGs for another purpose: to simulate data and make it easy to conduct random trials.\nOther than for teaching purposes (as in (3)), the role of DAGs in statistics and data analysis is to encode hypotheses about how elements of a system might be connected. Usually, you work with DAGs that reflect your and your colleague’s beliefs about how things are connected in the real world. Of course, believing a hypothesis does not make it true. Think of a DAG as a piece of fiction. Sometimes fiction is close to real life, and sometimes not. Both situations have their purposes for story-telling."
  },
  {
    "objectID": "posts/Takeaways-21/index.html#lesson-21",
    "href": "posts/Takeaways-21/index.html#lesson-21",
    "title": "Lesson 21 take-aways",
    "section": "Lesson 21",
    "text": "Lesson 21\n\nIt is helpful to think of any response variable as a combination of “signal” and “noise.” The signal reflects how the explanatory variables are related to the response. The noise is the unexplained part of the response variable. More precisely, the “noise” is that part that we do not care to explain in terms of relationships to other variables.\nThe idea that there is always noise in the response variable allows us to train models that do not go through every (or even any) data point. This enables us to claim that simple shapes of models can be good representations of relationships among variables.\nIn Math 300Z, with few exceptions we will work with models that have one or two explanatory variables. A nicer feature of such models is that we can draw a graphic of the model using just two or three aesthetics.\n\naes(y= ), the vertical axis: always will be assigned to the response variable. (This is a Math 300Z convention, and a good one, but not universal.)\naes(x= ), the horizontal axis: the first explanatory variable will be assigned to this.\naes(color= ) if there is a second explanatory variable, it will be assigned to color.\n\nThe model_plot() function will take care of all this assignment of variables to aesthetics.\nSince explanatory variables can be either categorical or quantitative, there are only a handful of model shapes we need to deal with. (The response variable is always quantitative.) These are enumerated in the Instructor’s notes for Lesson 21.\nAn excellent type of exam question would show you the graph of a model and ask you to identify whether there is one or two explanatory variables, and what type(s) it (they) are: quantitative or categorical.\nThere is also a role in statistics for models that have zero explanatory variables. The tilde expression for such models (letting y be the response variable) is y ~ 1. We have not yet discussed what the use is of such models.\nThe data used to build a regression model is called the training data. It is a data frame containing both the response variable and any explanatory variables. Once a model is built, we often run the rows of the training data through the model function. Doing this divides the response variable values into two components:\n\nThe signal, which is the model function output for each row of the training data. We call this the “model value.”\nThe noise, which for each row of the training data is the difference between the value of the response variable (the “response value”) and the “model value.” This difference—one for each row of the training data—is called the “residual”.\n\nRemember this simple relationship:\n\nresponse value = model value + residual\n\nOften we will need to measure how big these three things are. We use the variance as the measure of “how big.” The variance is only one of many possible ways to quantify “how big.” But it has the great advantage that\n\nvar(response value) = var(model value) + var(residual)"
  },
  {
    "objectID": "posts/Takeaways-23/index.html",
    "href": "posts/Takeaways-23/index.html",
    "title": "Lesson 23 take-aways",
    "section": "",
    "text": "lm() creates a model, which we can summarize in several ways. These numerical summaries—for instance, the coefficients reported by lm()—are called sample statistics. Mathematically, the sample statistics are exact, that is, the arithmetic is done correctly and everyone will get the same sample statistics when building the same model on the same data.\nStatistically, we take another point of view. We see the sample that we are working with as just one of the many samples that might have been collected. Imagine calculating a sample statistic on each of the many samples. The sample statistic would vary from one hypothetical sample to another. We call this sampling variation: note the “ing” ending on “sampling.”\nA confidence interval indicates the amount of sampling variation. It always consists of two numbers, the lower and the upper limits of the interval. Compute them from a model using conf_interval().\nThe width of a confidence interval is proportional to \\(1/\\sqrt{n}\\); the more data you have, the narrower will be the confidence interval.\nPrecision and accuracy are two different concepts. Accuracy refers to whether the measurement is “on target” or “close to reality.” Confidence intervals have nothing at all to say about accuracy. To get an accurate measurement of a coefficient, we need to choose the model that represents reality. Usually, we have no way to do this for sure. (With DAG simulations, we can read reality from the formulas, letting us match the model to the formula.)\nPrecision refers to the reliability or repeatability of the measurement. Confidence intervals are a good way to describe the precision of your measurements."
  },
  {
    "objectID": "posts/Takeaways-24/index.html",
    "href": "posts/Takeaways-24/index.html",
    "title": "Lesson 24 take-aways",
    "section": "",
    "text": "The lm() model-training function is an entirely automatic machine for turning two inputs into model coefficients (which are stored in a “model object”).\n\nInput 1: A model specification in the form of a tilde-expression. Example: height ~ mother + sex\nInput 2: A data frame holding the variables used in the tilde espression. Example: Galton\n\n\n\nthe_model <- lm(height ~ mother + sex, data = Galton)\ncoefficients(the_model)\n\n(Intercept)      mother        sexM \n 41.4495235   0.3531371   5.1766949 \n\n\n\nModel coefficients are a convenient and historically important way to present a model. For fundamental, however, is the idea of a model function that takes as inputs the explanatory variables and returns a corresponding output to be interpreted as a value of the response variable.\nThere are other ways to represent model functions. The field of “machine learning” is largely about the variety of ways of representing model functions. At an elementary level, when there are few explanatory variables, a graph will do:\n\n\nmodel_plot(the_model, interval=\"confidence\")\n\n\n\n\nIn the above graph, a confidence interval has been added to indicate the precision that can justifiably be claimed for the model function. Any line that fits within the shaded region is a reasonable claim.\n\nOften, the interest is in measuring the size of the connection between an explanatory variable and the response variable. This is called the effect size. For instance, in the above graph there is obviously a connection between mother’s height and her children’s height. The size of the connection is the amount by which the child’s height would change if the mother’s height were magically altered. Here, the effect size is about 0.35. A two-inch gain in mother’s height would lead to a 0.70 inch gain in child’s height.\nEffect size is always “with respect to” a single, selected explanatory variable. Each explanatory variable has its own effect size. An effect size always means to change the selected variable while holding every other component of the system contant. In mathematical language, an effect size is a partial derivative (if the selected explanatory variable is quantitative) or a partial change (if the selected explanatory variable is categorical).\nWhen we talk about effect size, we are not necessarily implying any causal connection in the real world. Obviously, it’s absurd to think that changing a mother’s height (and nothing else!) would lead to any change in her children’s heights. The effect size describes how the model function output will change when the input is changed. The model function may or may not be faithful to the causal mechanisms in the world. For our mother/child height example, the model height ~ mother + sex does not capture the real-world genetics/environment determinants of child’s height.\nAlmost all the models we will construct in Math 300Z have only “linear” terms, so in every case the effect size with respect to a variable will be exactly the same as the coefficient on that variable. This is just to keep the accounting simple for us. (In class we showed a couple of models that have nonlinear terms, the most common of which are called “interactions,” but which also include curvy (rather than straight-line) functions. You won’t be responsible for this material.)"
  },
  {
    "objectID": "posts/Takeaways-25/index.html",
    "href": "posts/Takeaways-25/index.html",
    "title": "Lesson 25 take-aways",
    "section": "",
    "text": "We contrasted the very different tasks of ….\n\nEstimation, which is mainly what we have been talking about until today. Estimation has to do with coefficients and effect sizes, understanding the relationships among values. Confidence intervals are a very important part of estimation methodology. Estimation focuses on “average” or “typical” or “central” patterns.\nPrediction which focuses on outcomes for individuals, and doesn’t benefit much from averaging. Graphically, a reasonable job drawing a prediction “interval” can be done from a plot of the data: look at the range of outcomes for the points near the given input levels.\n\nThe proper form for a prediction is to list all the possible outcomes, then assign a probability to each possible outcome.\n\nWhen the outcomes are numeric over a continuous range, then “probability” should be interpreted as “probability density” (a technical term from calculus) or, in more everyday language, a “relative probability.”\nA violin plot gives a reasonable representation of the relative probability of the different outcomes. But this is suitable only when the explanatory variables are categorical. For continuous, numerical explanatory variables, we will need another technique.\n\nEstimation of an effect size or a coefficient is accompanied by a confidence interval, which has a lower and an upper bound (the “confidence bounds”). The specific interval depends on the “confidence level,” but you won’t be mislead if you always use a 95% level, which is the convention.\n\nHistorical aside: The term “confidence” rather than “probability” was used to step around philosophical debates about the nature of probability. The confidence interval is not intended to be translated into a probability. Such a translation would look like this, “There is a 95% probability that the true value falls into the range covered by the interval.” This is what almost everybody does, even though it is not exactly legitimate. The “mathematically correct” formulation for translating a confidence interval into a probability is more subtle and not satisfying. (It is, “If I build confidence intervals according to the rules, then I can expect that in 95% of the situations being studied the true value will be within the confidence interval. But I can’t know for any one situation whether this is the case.”)\n\nPredictions are often formatted into an interval, so it’s tempting to think that the same principles (e.g., use a 95% level) are applicable. But useful predictions often have to do with extreme events. So levels like 80% are often appropriate. Strictly speaking, what’s presented as an interval ought really to be presented as a probability distribution (like a violin plot). Experts learn how to reverse engineer the probability distribution from the interval.\nNotwithstanding (4), the proper form for a prediction is to assign a probability to each possible outcome, as in (2). It is this form that is useful for decision making.\nWe looked at a case study about SAT scores"
  },
  {
    "objectID": "posts/Prediction-vs-estimation/index.html",
    "href": "posts/Prediction-vs-estimation/index.html",
    "title": "Case study: Prediction vs estimation",
    "section": "",
    "text": "This case study is motivated by the common misconception that “estimation” is more appropriate than “prediction.” The setting is gender preconceptions, specifically the claim that “boys are better at math than girls.” The data we looked at are math SAT scores for males and for females.\n\n\n\nAlthough published studies use actual individual-level data about SAT scores—for instance, a data frame with 1.5 million rows and the variables sex and score—those data are not available to us. Instead, I’ve reconstructed 20,000 rows of such raw data from the summary information provided by the College Board. The data frame is called Math_scores. (It is not available from R packages.)\nHere is an estimation: What is the difference in mean SAT scores for females and males?\n\nMath_scores |>\n  lm(score ~ sex, data=_) |>\n  conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    493.26869 \n    495.44183 \n    497.61497 \n  \n  \n    sexmale \n    19.33271 \n    22.40599 \n    25.47927 \n  \n\n\n\n\nThis model summary points to males having a higher average math SAT score than females, by about 20 points. The confidence interval has a width of about 6 points. In the actual data, which had 1.5 million rather than 20 thousand rows, the confidence interval would be much narrower, about 1 point.\nThere are two major problems with interpreting such results as meaning “boys are better at math than girls.”\n\nThe calculations for the confidence interval on the sexmale effect size are factually correct, but they are incomplete and misleading. The model represents an oversimplified DAG: \\(\\mathtt{sex} \\rightarrow \\mathtt{score}\\). In reality there are many other factors at work. One that we talked about in class is that a higher proportion of women take the SAT than of men. We know from other sources (and common sense) that the smaller the fraction of a group who take the SAT, the higher the average scores will be. We can estimate the effect size of SAT with respect to “fraction taking the test,” and it accounts for the observed difference in mean scores between males and females. To summarize, female test scores are on average lower than males because a more inclusive group of women take the test than of men. This is ironic: because women are more likely to go to college than men (a sign of academic success for women), the women’s scores are on average worse.\n\nMore germane to today’s topic of prediction …\n\nA statement like “boys are better at math than girls” tends to get translated to valuations of the skills of individuals. But if it is individuals we are thinking of, we should be framing the data analysis in terms of prediction rather than estimation. In this case, since sex is a categorical explanatory variable, the predictive probability distribution is well indicated by a violin plot, as in Figure 1.\n\n\ngf_jitter(score ~ sex, data = Math_scores, size=0.2, width = 0.15, height = 0, alpha = 0.3) %>% \n  gf_violin(color = NA, fill = \"blue\", alpha = 0.5) %>%\n  gf_labs(y = \"SAT math score\", x = \"\")\n\n\n\n\nFigure 1: The violin plot shows the probability distribution associated with each sex. They are almost identical.\n\n\n\n\nA reasonable interpretation of this graph: Knowing the sex of a person tells nothing about the person’s SAT score.\nAnother way to look at the problem is whether knowing the test score gives us insight into the sex of the person. We will study the modeling technique used to answer such a question in Lesson 33. But for now, all you need to know is that it’s possible to model the probability that a person is female given the test score.\n\nMath_scores <- Math_scores |> mutate(female = zero_one(sex, one=\"female\"))\nmodel <- glm(female ~ splines::ns(score,4), data = Math_scores, family=\"binomial\")\nmodel_plot(model, interval=\"none\", show_data=FALSE)\n\n\n\n\nExcept for scores above 650, the difference between the model values and 50% probability would be statistically undetectable unless averaging over groups of 400 or larger. At the very highest scores (around 750-800), there are about 2 males for every female. Even that hardly justifies applying the claim that “boys have high math scores than girls” to individuals."
  },
  {
    "objectID": "posts/Self-driving-safety/index.html",
    "href": "posts/Self-driving-safety/index.html",
    "title": "Bayesian updating for self-driving cars",
    "section": "",
    "text": "Suppose ordinary new-ish cars have a mean distance between accidents of 200,000 miles. (This is roughly consistent with the Internet factoid that the probability of a car accident in 1000 miles is 1/366.)\nWhat might a skeptical regulator reasonably believe about newly released self-driving cars?\n“These things are crazy. Very likely to get in an accident.”\n“Perhaps a 1% chance that they are safer than regular cars.”\nIt takes some math to translate these views into a prediction about the actual mean time between accidents. We teach that in other courses. But here is a graph of such a probability function.\nThis is called a “prior” probability distribution: our starting point.\nNow data comes in. Each day we get a report from all the self-driving cars:\nBased on these data, we update our beliefs to produce a “posterior” probability distribution. The Bayesian updating rule is:\nPosterior(m) \\(\\propto\\) Likelihood(m, observations) \\(\\times\\) prior(m)\nSuppose the observation is: car crashed at 23,241 miles. A plausible **likelihood function* is based on simple probability, not the prior.\nThere is also a likelihood function for a car having driven 30,000 miles without an accident."
  },
  {
    "objectID": "posts/Self-driving-safety/index.html#now-for-the-data",
    "href": "posts/Self-driving-safety/index.html#now-for-the-data",
    "title": "Bayesian updating for self-driving cars",
    "section": "Now for the data",
    "text": "Now for the data\nThe New York Times report indicates 400 crashes out of 360,000 self-driving cars. Suppose we observe these data for Tesla\n95 cars have driven 20K miles without an accident; 5 cars had accidents respectively at 1K, 4K, 8K, 12K, 16K\n\n\nCode\nlog_likelihood_observed <- function(m) {\n  ( log10(nocrash_likelihood(m, 20000))*95) +\n    log10(crash_likelihood(m, 1000)) +\n    log10(crash_likelihood(m, 4000)) +\n    log10(crash_likelihood(m, 8000)) +\n    log10(crash_likelihood(m, 12000)) +\n    log10(crash_likelihood(m, 16000)) \n}\n\n\n\n\nCode\nlog_posterior <- function(m) log_likelihood_observed(m) + log10(belief(m))\nslice_plot(10^((log_posterior(m)+38)) ~ m, bounds(m=2000:500000), npts=1001) +\n  geom_vline(xintercept=200000, color=\"red\") +\n  ylab(\"Posterior: relative probability\") + xlab(\"Average miles per accident\")\n\n\n\n\n\nThe posterior indicates that data on 100 cars, 5 of which had accidents before 20,000 miles, places most belief that the self-driving cars are safer than regular cars: about twice as safe.\nThis is counter-intuitive, since we have no data on cars that drove farther than 20,000 miles. But would be hard to get 95 out of 100 cars to 20,000 without an accident if the mean distance betwee accidents were even 100,000 miles."
  },
  {
    "objectID": "posts/Takeaways-28/index.html",
    "href": "posts/Takeaways-28/index.html",
    "title": "Lesson 28 take-aways",
    "section": "",
    "text": "Recall that the fundamental method in the second half of Math 300Z is linear regression. That involves identifying a response variable and one or more explanatory variables.1\n\nWe are adding some more detail to the vocabulary of explanatory variables in order to be able to talk about a very common situation:\n\nThere is one explanatory variable that is of primary interest to us.\nThere are also other explanatory variables that we might choose to include in the model specification, even though they aren’t of direct interest to us.\n\nThe generic name covariates is used for the explanatory variables in (b). A covariate is an ordinary variable, but it is always one that is being considered for inclusion in a model specification. We have a choice in this matter since the covariate is not of direct interest. Using this vocabulary gives us a way to say concisely, “The variable may be important in an explanatory role, but it is not of direct interest to us.”\nAn important critical thinking skill for evaluating a claim is to ask, “What other factors might be involved?” This is exactly the role for covariates. The statistics of covariates lets us examine the consequences of incorporating such “other factors.”\nCovariates often show up implicitly in models. For instance, the model life_expectancy ~ GDP has little explanatory power. Instead, we likely want to consider “per capita GDP, which takes into account the size of the population. In our modeling framework, this is quite like adding a covariatepopulation, that islife_expectancy ~ GDP + population`.\n\n\n\n\n\n\n\nAside: Intensive, extensive, and logarithms\n\n\n\nIn physical chemistry it’s common to distinguish between intensive and extensive variables. An extensive variable refers to the size (or “extent”) of the system: e.g. mass, volume, or energy. An intensive variable is not a measure of the “extent” of the system, but of a property like temperature or density. GDP is an extensive variable, as is population or land area. GDP is one way of describing how “big” the country is. Life expectancy does not tell you about the extent of the country, it’s an intensive quantity that refers to individuals.\nAs a modeling rule of thumb, whenever you are working with an extensive quantity, think seriously about using the logarithm of that quantity.\nDoing this in the context of life-expectancy and GDP (with population size as a covariate) would involve the model specification life_expectancy ~ log(GDP) + log(population). This model incorporates the “per capita” adjustment, since $(GDP/population) = (GDP) - ln(population). In other words,life_expectancy ~ log(GDP) + log(population)` is a generalization of the per-capita adjustment.\n\n\n\nIn Lesson 30, we will consider reasons why or why not to include covariates in a model. But in this Lesson 28, we want to make a technical point about comparing the coefficients between two nested models. An example of a pair of nested models is BFat ~ Hips and BFat ~ Hips + DThigh (to use the Anthro_F example). The models are nested because they both have the same response variable and the larger model includes all the explanatory variables in the smaller models.\nComparing the Hips coefficient for the two models shows something that surprises many people. Adding the covariate PThigh leads to a change in the coefficient on the explanatory variable Hips.\n\n\nlm(BFat ~ Hips, data=Anthro_F) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -53.00 \n    -44.00 \n    -35.00 \n  \n  \n    Hips \n    0.58 \n    0.68 \n    0.78 \n  \n\n\n\nlm(BFat ~ Hips + PThigh, data=Anthro_F) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -50.000 \n    -40.00 \n    -31.00 \n  \n  \n    Hips \n    0.083 \n    0.31 \n    0.54 \n  \n  \n    PThigh \n    0.240 \n    0.56 \n    0.87 \n  \n\n\n\n\nNotice the the confidence interval on the `Hips` coefficient from the smaller model doesn't overlap at all with the confidence interval on `Hips` from the larger model.\n\nMany people fallaciously believe that a situation as in (5) indicates some kind of deficiency in statistical methods. Such a person might ask, “How can I take a coefficient or confidence interval seriously if adding in another factor changes things completely?” In fact, the situation in (5) is entirely a mathematical phenomenon that depends on how closely related the explanatory variable (Hips) is to the covariate (PThigh).\n\nWe can measure how closely two variables are related using R2\n\nlm(Hips ~ PThigh, data=Anthro_F) |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    180 \n    1 \n    0.83 \n    880 \n    0.83 \n    0 \n    1 \n    180 \n  \n\n\n\n\nWhen the covariate is not closely related to the explanatory variable, the dramatic shift in the confidence interval of the explanatory variable is not seen. To illustrate, suppose we use Calf as the covariate rather than PThigh.\n\nlm(Hips ~ Calf, data=Anthro_F) |> R2() # not so closely related\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    180 \n    1 \n    0.2 \n    47 \n    0.2 \n    0 \n    1 \n    180 \n  \n\n\n\n\nThe high R2 indicates that the two variables are closely related. But with Calf as the covariate, the confidence interval on Hips is almost identical to what’s seen in the smaller model (see above) BFat ~ Hips.\n\nlm(BFat ~ Hips + Calf, data=Anthro_F) |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -55.000 \n    -45.000 \n    -35.00 \n  \n  \n    Hips \n    0.540 \n    0.650 \n    0.76 \n  \n  \n    Calf \n    -0.091 \n    0.095 \n    0.28 \n  \n\n\n\n\nThe decision to include a covariate should rest on one’s understanding of the system being models. We’ve been using DAGs to describe such understanding. Lesson 30 will show how a DAG can be transformed into a decision about whether to include covariates.\n\n\n\n\nFootnotes\n\n\nTo be pedantic, I should say “zero or more explanatory variables.” That’s because y ~ 1 is also a regression model, even though it has no explanatory variables.↩︎"
  }
]